{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OedVdllz9JCc",
        "outputId": "fce543ec-268f-43ff-c93d-cdb105ea8b42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai) (0.4.6)\n",
            "Requirement already satisfied: openai==0.28 in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
            "Requirement already satisfied: keras in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.15.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\lib\\site-packages (3.1)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "!pip install openai\n",
        "!pip install openai==0.28\n",
        "!pip install keras\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement distutils (from versions: none)\n",
            "ERROR: No matching distribution found for distutils\n"
          ]
        }
      ],
      "source": [
        "!pip install distutils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5v_tsg2u7po7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.util import bigrams\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "import networkx as nx\n",
        "import string\n",
        "import pickle\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import openai\n",
        "\n",
        "def sentence_length(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if not sentences:  # Handle case where there are no sentences in the text\n",
        "        return 0.0  # Return 0 if there are no sentences\n",
        "    lengths = [len(sentence.split()) for sentence in sentences]\n",
        "    min_length = min(lengths)\n",
        "    max_length = max(lengths)\n",
        "    if min_length == max_length:  # Handle case where all sentences have the same length\n",
        "        return 0.0  # Return 0 if all sentences have the same length\n",
        "    avg_sentence_length = sum(lengths) / len(sentences)\n",
        "    normalized_length = (avg_sentence_length - min_length) / (max_length - min_length)\n",
        "    return normalized_length\n",
        "\n",
        "# Function to calculate punctuation frequency feature vector\n",
        "def calculate_punctuation_frequency(text):\n",
        "    punctuation_marks = set(string.punctuation)\n",
        "    punctuation_counts = Counter(char for char in text if char in punctuation_marks)\n",
        "    total_punctuation = sum(punctuation_counts.values())\n",
        "    punctuation_distribution = {punct: count / total_punctuation for punct, count in punctuation_counts.items()}\n",
        "    return [punctuation_distribution.get(mark, 0) for mark in punctuation_marks]\n",
        "\n",
        "# Function to calculate POS tag frequency feature vector\n",
        "def calculate_pos_tag_frequency(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    pos_tag_counts = Counter(tag for word, tag in pos_tags)\n",
        "    total_pos_tags = sum(pos_tag_counts.values())\n",
        "    pos_tag_distribution = {tag: count / total_pos_tags for tag, count in pos_tag_counts.items()}\n",
        "    all_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
        "    return [pos_tag_distribution.get(tag, 0) for tag in all_tags]\n",
        "\n",
        "# Function to calculate function word frequency feature vector\n",
        "def calculate_function_word_frequency(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
        "    total_function_words = len(function_words_text)\n",
        "    function_word_counts = Counter(function_words_text)\n",
        "    function_word_frequencies = {word: count / total_function_words for word, count in function_word_counts.items()}\n",
        "    all_function_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
        "    return [function_word_frequencies.get(word, 0) for word in all_function_words]\n",
        "def is_passive_voice(tagged_sentence):\n",
        "    for i in range(1, len(tagged_sentence)):\n",
        "        if (\n",
        "            tagged_sentence[i][0] == \"by\" and\n",
        "            tagged_sentence[i - 1][1].startswith(\"V\") and\n",
        "            tagged_sentence[i][1] == \"IN\"\n",
        "        ):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def calculate_passive_to_active_ratio(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_sentences = len(sentences)\n",
        "    passive_count = 0\n",
        "    active_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tagged_sentence = nltk.pos_tag(words)\n",
        "\n",
        "        if is_passive_voice(tagged_sentence):\n",
        "            passive_count += 1\n",
        "        else:\n",
        "            active_count += 1\n",
        "\n",
        "    return passive_count / active_count if active_count > 0 else 0\n",
        "\n",
        "# Function to convert is_passive_voice result to binary feature vector\n",
        "def is_passive_to_binary(passive_to_active_ratio):\n",
        "    return 1 if passive_to_active_ratio > 1 else 0\n",
        "\n",
        "def ngram_transition_graph_feature(text, n=5):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    ngrams = list(nltk.ngrams(tokens, n))\n",
        "    transition_graph = nx.DiGraph()\n",
        "    transition_graph.add_nodes_from(ngrams)\n",
        "    for i in range(len(ngrams) - 1):\n",
        "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
        "\n",
        "    # Compute graph properties\n",
        "    num_nodes = transition_graph.number_of_nodes()\n",
        "    num_edges = transition_graph.number_of_edges()\n",
        "    avg_degree = np.mean([val for (node, val) in transition_graph.degree()])\n",
        "    density = nx.density(transition_graph)\n",
        "\n",
        "    # Return computed graph properties as a feature vector\n",
        "    return np.array([num_nodes, num_edges, avg_degree, density])\n",
        "\n",
        "def type_token_ratio(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    unique_tokens = set(tokens)\n",
        "    return len(unique_tokens) / len(tokens)\n",
        "\n",
        "# Gender prediction model\n",
        "def predict_gender(text):\n",
        "    # Load tokenizer\n",
        "    with open(\"lstm_tokenizer.pickle\", \"rb\") as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # Load label encoder\n",
        "    with open(\"lstm_label_encoder.pickle\", \"rb\") as handle:\n",
        "        label_encoder = pickle.load(handle)\n",
        "\n",
        "    # Load max length\n",
        "    with open(\"max_length.pickle\", \"rb\") as handle:\n",
        "        max_length = pickle.load(handle)\n",
        "\n",
        "    # Load model\n",
        "    model = load_model(\"lstm_trained_model.h5\")\n",
        "\n",
        "    # Tokenize input text\n",
        "    new_data_sequence = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # Pad tokenized sequence\n",
        "    new_data_padded = pad_sequences(new_data_sequence, maxlen=max_length)\n",
        "\n",
        "    # Make predictions\n",
        "    prediction = model.predict(new_data_padded)\n",
        "    predicted_class = (prediction > 0.5).astype('int')[0][0]\n",
        "\n",
        "    # Return predicted gender as numeric value - (0 for male, 1 for female)\n",
        "    return predicted_class\n",
        "\n",
        "# American or British detection using gpt 3.5\n",
        "def detect_english_variant(text):\n",
        "    prompt = \"Please analyze the language and phrasing of the paragraph provided below and determine whether it aligns more closely with American English or British English.\\n\\n\" + text + \"\\n\\nLanguage variant:\"\n",
        "\n",
        "    # Set up OpenAI API\n",
        "    openai.api_key = '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'\n",
        "\n",
        "    # Use GPT-3.5 to determine English variant\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      temperature=0,\n",
        "      max_tokens=800\n",
        "    )\n",
        "\n",
        "    # Extracting the prediction from the response\n",
        "    prediction_text = response.choices[0].text.strip()\n",
        "\n",
        "    # Assign numeric values to the outcomes\n",
        "    if prediction_text == \"American English\":\n",
        "        return 1\n",
        "    elif prediction_text == \"British English\":\n",
        "        return 2\n",
        "    else:\n",
        "        return 0  # Return 0 for other cases or errors\n",
        "\n",
        "# Function to check for double spaces after a full stop\n",
        "def check_double_spaces_after_full_stop(text):\n",
        "    double_spaces_count = text.count(\".  \")\n",
        "    if double_spaces_count >= 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Function to calculate all features\n",
        "def calculate_all_features(text):\n",
        "    features = []\n",
        "\n",
        "    # Sentence Length\n",
        "    features.append(sentence_length(text))\n",
        "\n",
        "    # Punctuation Frequency\n",
        "    features.extend(calculate_punctuation_frequency(text))\n",
        "\n",
        "    # POS Tag Frequency\n",
        "    features.extend(calculate_pos_tag_frequency(text))\n",
        "\n",
        "    # Function Word Frequency\n",
        "    features.extend(calculate_function_word_frequency(text))\n",
        "\n",
        "    # N-gram Transition Graph Feature\n",
        "    features.extend(ngram_transition_graph_feature(text))\n",
        "\n",
        "    # Type-Token Ratio\n",
        "    features.append(type_token_ratio(text))\n",
        "\n",
        "    # Passive to Active Ratio\n",
        "    passive_to_active_ratio = calculate_passive_to_active_ratio(text)\n",
        "    features.append(is_passive_to_binary(passive_to_active_ratio))\n",
        "\n",
        "    # Gender prediction\n",
        "    gender_prediction = predict_gender(text)\n",
        "    features.append(gender_prediction)\n",
        "\n",
        "    # American or British detection using GPT 3.5\n",
        "    #english_variant = detect_english_variant(text)\n",
        "    #features.append(english_variant)\n",
        "\n",
        "    # Check for double spaces after a full stop\n",
        "    double_spaces = check_double_spaces_after_full_stop(text)\n",
        "    features.append(double_spaces)\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcUEN3FL7JYu",
        "outputId": "18609793-cfba-4ef6-f756-e3da18f8e1ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.0 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 530ms/step\n",
            "1/1 [==============================] - 1s 708ms/step\n",
            "1/1 [==============================] - 1s 590ms/step\n",
            "1/1 [==============================] - 1s 539ms/step\n",
            "1/1 [==============================] - 0s 456ms/step\n",
            "1/1 [==============================] - 1s 603ms/step\n",
            "1/1 [==============================] - 1s 750ms/step\n",
            "1/1 [==============================] - 1s 627ms/step\n",
            "1/1 [==============================] - 1s 695ms/step\n",
            "1/1 [==============================] - 1s 570ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "CSV file created with headers.\n",
            "Additional data appended to the CSV file.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "import csv\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Extract features for author A\n",
        "features_author_A = [calculate_all_features(text) for text in texts_author_A]\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "for i in range(len(texts_author_A)):\n",
        "    text_author_A = texts_author_A[i]\n",
        "    features_author_A_i = features_author_A[i]\n",
        "\n",
        "    # Randomly select a text from author A and one not from author A\n",
        "    text_not_author_A = random.choice(texts_author_A[:i] + texts_author_A[i+1:] + texts_not_author_A)\n",
        "    features_not_author_A = calculate_all_features(text_not_author_A)\n",
        "\n",
        "    # Compute similarity\n",
        "    similarity = compute_similarity(features_author_A_i, features_not_author_A)\n",
        "\n",
        "    # Label the data\n",
        "    if text_not_author_A in texts_author_A:\n",
        "        label = 1  # Same author\n",
        "    else:\n",
        "        label = 0  # Not same author\n",
        "\n",
        "    # Append to dataset\n",
        "    dataset.append((text_author_A, text_not_author_A, similarity, label))\n",
        "\n",
        "# Check if the CSV file exists\n",
        "if not os.path.isfile('dataset.csv1'):\n",
        "    with open('dataset.csv1', 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        # Write headers\n",
        "        similarity_keys = dataset[0][2].keys()  # Get keys from the first data instance\n",
        "        header = list(similarity_keys) + ['Label']  # Combine keys with label\n",
        "        csvwriter.writerow(header)\n",
        "        print(\"CSV file created with headers.\")\n",
        "\n",
        "# Open the CSV file in append mode\n",
        "with open('dataset.csv1', 'a', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # Write data rows\n",
        "    for data in dataset:\n",
        "        similarity_values = data[2].values()  # Extract similarity values\n",
        "        label = data[3]  # Extract label\n",
        "        row_data = list(similarity_values) + [label]  # Combine similarity values with label\n",
        "        csvwriter.writerow(row_data)\n",
        "\n",
        "print(\"Additional data appended to the CSV file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wkdAszIrI2m2"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(feature_set1, feature_set2):\n",
        "    similarities = {}\n",
        "\n",
        "    # Sentence Length\n",
        "    similarities['sentence_length'] = max(0, 1 - abs(feature_set1[0] - feature_set2[0]))\n",
        "\n",
        "    # Punctuation Frequency\n",
        "    punctuation_freq1 = feature_set1[1:33]\n",
        "    punctuation_freq2 = feature_set2[1:33]\n",
        "    for i in range(len(punctuation_freq1)):\n",
        "        similarities['punctuation_{}'.format(i+1)] = max(0, 1 - abs(punctuation_freq1[i] - punctuation_freq2[i]))\n",
        "\n",
        "    # POS Tag Frequency\n",
        "    pos_tag_freq1 = feature_set1[33:69]\n",
        "    pos_tag_freq2 = feature_set2[33:69]\n",
        "    for i, tag in enumerate(['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']):\n",
        "        similarities['pos_tag_{}'.format(tag)] = max(0, 1 - abs(pos_tag_freq1[i] - pos_tag_freq2[i]))\n",
        "\n",
        "    # Function Word Frequency\n",
        "    function_word_freq1 = feature_set1[69:196]\n",
        "    function_word_freq2 = feature_set2[69:196]\n",
        "    for i, word in enumerate(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']):\n",
        "        similarities['function_word_{}'.format(word)] = max(0, 1 - abs(function_word_freq1[i] - function_word_freq2[i]))\n",
        "\n",
        "    # N-gram Transition Graph Feature\n",
        "    ngram_transition_feature1 = feature_set1[196:200]\n",
        "    ngram_transition_feature2 = feature_set2[196:200]\n",
        "    for i, feature in enumerate(['num_nodes', 'num_edges', 'avg_degree', 'density']):\n",
        "        similarities['ngram_transition_{}'.format(feature)] = max(0, 1 - abs(ngram_transition_feature1[i] - ngram_transition_feature2[i]))\n",
        "\n",
        "    # Type-Token Ratio\n",
        "    similarities['type_token_ratio'] = max(0, 1 - abs(feature_set1[200] - feature_set2[200]))\n",
        "\n",
        "    # Passive to Active Ratio\n",
        "    similarities['passive_to_active_ratio'] = max(0, 1 - abs(feature_set1[201] - feature_set2[201]))\n",
        "\n",
        "    # Gender prediction\n",
        "    similarities['gender_prediction'] = 1 if feature_set1[202] == feature_set2[202] else 0\n",
        "\n",
        "    # English variant\n",
        "    #similarities['english_variant'] = 1 if feature_set1[203] == feature_set2[203] else 0\n",
        "\n",
        "    # Double spaces after full stop\n",
        "    similarities['double_spaces'] = 1 if feature_set1[203] == feature_set2[203] else 0\n",
        "\n",
        "    return similarities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "THIS IS THE GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "def extract_tweets(csv_file, author_name):\n",
        "    texts_author_A = []\n",
        "    texts_not_author_A = []\n",
        "\n",
        "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            if row['author'] == author_name:\n",
        "                texts_author_A.append(row['tweet'])\n",
        "            else:\n",
        "                texts_not_author_A.append(row['tweet'])\n",
        "\n",
        "    # Shuffle other tweets to ensure randomness\n",
        "    random.shuffle(texts_not_author_A)\n",
        "    random.shuffle(texts_author_A)\n",
        "\n",
        "    # Trim both lists to have exactly 50 tweets each\n",
        "    texts_author_A = texts_author_A[:100]\n",
        "    texts_not_author_A = texts_not_author_A[:100]\n",
        "\n",
        "    return texts_author_A, texts_not_author_A\n",
        "\n",
        "# Example usage:\n",
        "texts_author_A, texts_not_author_A = extract_tweets('tweet_with_authors.csv', 'Sebastian Ruder')\n",
        "print(len(texts_not_author_A))\n",
        "print(len(texts_author_A))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
