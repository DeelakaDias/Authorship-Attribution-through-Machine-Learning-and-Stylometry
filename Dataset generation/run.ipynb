{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: openai==0.28 in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Requirement already satisfied: keras in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\lib\\site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "!pip install openai\n",
    "!pip install openai==0.28\n",
    "!pip install keras\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import networkx as nx\n",
    "import string\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import openai\n",
    "\n",
    "def sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if not sentences:  # Handle case where there are no sentences in the text\n",
    "        return 0.0  # Return 0 if there are no sentences\n",
    "    lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    if min_length == max_length:  # Handle case where all sentences have the same length\n",
    "        return 0.0  # Return 0 if all sentences have the same length\n",
    "    avg_sentence_length = sum(lengths) / len(sentences)\n",
    "    normalized_length = (avg_sentence_length - min_length) / (max_length - min_length)\n",
    "    return normalized_length\n",
    "\n",
    "# Function to calculate punctuation frequency feature vector\n",
    "def calculate_punctuation_frequency(text):\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "    punctuation_counts = Counter(char for char in text if char in punctuation_marks)\n",
    "    total_punctuation = sum(punctuation_counts.values())\n",
    "    punctuation_distribution = {punct: count / total_punctuation for punct, count in punctuation_counts.items()}\n",
    "    return [punctuation_distribution.get(mark, 0) for mark in punctuation_marks]\n",
    "\n",
    "# Function to calculate POS tag frequency feature vector\n",
    "def calculate_pos_tag_frequency(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    pos_tag_counts = Counter(tag for word, tag in pos_tags)\n",
    "    total_pos_tags = sum(pos_tag_counts.values())\n",
    "    pos_tag_distribution = {tag: count / total_pos_tags for tag, count in pos_tag_counts.items()}\n",
    "    all_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    return [pos_tag_distribution.get(tag, 0) for tag in all_tags]\n",
    "\n",
    "# Function to calculate function word frequency feature vector\n",
    "def calculate_function_word_frequency(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
    "    total_function_words = len(function_words_text)\n",
    "    function_word_counts = Counter(function_words_text)\n",
    "    function_word_frequencies = {word: count / total_function_words for word, count in function_word_counts.items()}\n",
    "    all_function_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "    return [function_word_frequencies.get(word, 0) for word in all_function_words]\n",
    "def is_passive_voice(tagged_sentence):\n",
    "    for i in range(1, len(tagged_sentence)):\n",
    "        if (\n",
    "            tagged_sentence[i][0] == \"by\" and\n",
    "            tagged_sentence[i - 1][1].startswith(\"V\") and\n",
    "            tagged_sentence[i][1] == \"IN\"\n",
    "        ):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_passive_to_active_ratio(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    passive_count = 0\n",
    "    active_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_sentence = nltk.pos_tag(words)\n",
    "\n",
    "        if is_passive_voice(tagged_sentence):\n",
    "            passive_count += 1\n",
    "        else:\n",
    "            active_count += 1\n",
    "\n",
    "    return passive_count / active_count if active_count > 0 else 0\n",
    "\n",
    "# Function to convert is_passive_voice result to binary feature vector\n",
    "def is_passive_to_binary(passive_to_active_ratio):\n",
    "    return 1 if passive_to_active_ratio > 1 else 0\n",
    "\n",
    "def ngram_transition_graph_feature(text, n=5):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    transition_graph.add_nodes_from(ngrams)\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "\n",
    "    # Compute graph properties\n",
    "    num_nodes = transition_graph.number_of_nodes()\n",
    "    num_edges = transition_graph.number_of_edges()\n",
    "    avg_degree = np.mean([val for (node, val) in transition_graph.degree()])\n",
    "    density = nx.density(transition_graph)\n",
    "\n",
    "    # Return computed graph properties as a feature vector\n",
    "    return np.array([num_nodes, num_edges, avg_degree, density])\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "# Gender prediction model\n",
    "def predict_gender(text):\n",
    "    # Load tokenizer\n",
    "    with open(\"lstm_tokenizer.pickle\", \"rb\") as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # Load label encoder\n",
    "    with open(\"lstm_label_encoder.pickle\", \"rb\") as handle:\n",
    "        label_encoder = pickle.load(handle)\n",
    "\n",
    "    # Load max length\n",
    "    with open(\"max_length.pickle\", \"rb\") as handle:\n",
    "        max_length = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(\"lstm_trained_model.h5\")\n",
    "\n",
    "    # Tokenize input text\n",
    "    new_data_sequence = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "    # Pad tokenized sequence\n",
    "    new_data_padded = pad_sequences(new_data_sequence, maxlen=max_length)\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = model.predict(new_data_padded)\n",
    "    predicted_class = (prediction > 0.5).astype('int')[0][0]\n",
    "\n",
    "    # Return predicted gender as numeric value - (0 for male, 1 for female)\n",
    "    return predicted_class\n",
    "\n",
    "# American or British detection using gpt 3.5\n",
    "def detect_english_variant(text):\n",
    "    prompt = \"Please analyze the language and phrasing of the paragraph provided below and determine whether it aligns more closely with American English or British English.\\n\\n\" + text + \"\\n\\nLanguage variant:\"\n",
    "\n",
    "    # Set up OpenAI API\n",
    "    openai.api_key = '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'\n",
    "\n",
    "    # Use GPT-3.5 to determine English variant\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"gpt-3.5-turbo-instruct\",\n",
    "      prompt=prompt,\n",
    "      temperature=0,\n",
    "      max_tokens=800\n",
    "    )\n",
    "\n",
    "    # Extracting the prediction from the response\n",
    "    prediction_text = response.choices[0].text.strip()\n",
    "\n",
    "    # Assign numeric values to the outcomes\n",
    "    if prediction_text == \"American English\":\n",
    "        return 1\n",
    "    elif prediction_text == \"British English\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 0  # Return 0 for other cases or errors\n",
    "\n",
    "# Function to check for double spaces after a full stop\n",
    "def check_double_spaces_after_full_stop(text):\n",
    "    double_spaces_count = text.count(\".  \")\n",
    "    if double_spaces_count >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function to calculate all features\n",
    "def calculate_all_features(text):\n",
    "    features = []\n",
    "\n",
    "    # Sentence Length\n",
    "    features.append(sentence_length(text))\n",
    "\n",
    "    # Punctuation Frequency\n",
    "    features.extend(calculate_punctuation_frequency(text))\n",
    "\n",
    "    # POS Tag Frequency\n",
    "    features.extend(calculate_pos_tag_frequency(text))\n",
    "\n",
    "    # Function Word Frequency\n",
    "    features.extend(calculate_function_word_frequency(text))\n",
    "\n",
    "    # N-gram Transition Graph Feature\n",
    "    features.extend(ngram_transition_graph_feature(text))\n",
    "\n",
    "    # Type-Token Ratio\n",
    "    features.append(type_token_ratio(text))\n",
    "\n",
    "    # Passive to Active Ratio\n",
    "    passive_to_active_ratio = calculate_passive_to_active_ratio(text)\n",
    "    features.append(is_passive_to_binary(passive_to_active_ratio))\n",
    "\n",
    "    # Gender prediction\n",
    "    gender_prediction = predict_gender(text)\n",
    "    features.append(gender_prediction)\n",
    "\n",
    "    # American or British detection using GPT 3.5\n",
    "    #english_variant = detect_english_variant(text)\n",
    "    #features.append(english_variant)\n",
    "\n",
    "    # Check for double spaces after a full stop\n",
    "    double_spaces = check_double_spaces_after_full_stop(text)\n",
    "    features.append(double_spaces)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(feature_set1, feature_set2):\n",
    "    similarities = {}\n",
    "\n",
    "    # Sentence Length\n",
    "    similarities['sentence_length'] = max(0, 1 - abs(feature_set1[0] - feature_set2[0]))\n",
    "\n",
    "    # Punctuation Frequency\n",
    "    punctuation_freq1 = feature_set1[1:33]\n",
    "    punctuation_freq2 = feature_set2[1:33]\n",
    "    for i in range(len(punctuation_freq1)):\n",
    "        similarities['punctuation_{}'.format(i+1)] = max(0, 1 - abs(punctuation_freq1[i] - punctuation_freq2[i]))\n",
    "\n",
    "    # POS Tag Frequency\n",
    "    pos_tag_freq1 = feature_set1[33:69]\n",
    "    pos_tag_freq2 = feature_set2[33:69]\n",
    "    for i, tag in enumerate(['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']):\n",
    "        similarities['pos_tag_{}'.format(tag)] = max(0, 1 - abs(pos_tag_freq1[i] - pos_tag_freq2[i]))\n",
    "\n",
    "    # Function Word Frequency\n",
    "    function_word_freq1 = feature_set1[69:196]\n",
    "    function_word_freq2 = feature_set2[69:196]\n",
    "    for i, word in enumerate(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']):\n",
    "        similarities['function_word_{}'.format(word)] = max(0, 1 - abs(function_word_freq1[i] - function_word_freq2[i]))\n",
    "\n",
    "    # N-gram Transition Graph Feature\n",
    "    ngram_transition_feature1 = feature_set1[196:200]\n",
    "    ngram_transition_feature2 = feature_set2[196:200]\n",
    "    for i, feature in enumerate(['num_nodes', 'num_edges', 'avg_degree', 'density']):\n",
    "        similarities['ngram_transition_{}'.format(feature)] = max(0, 1 - abs(ngram_transition_feature1[i] - ngram_transition_feature2[i]))\n",
    "\n",
    "    # Type-Token Ratio\n",
    "    similarities['type_token_ratio'] = max(0, 1 - abs(feature_set1[200] - feature_set2[200]))\n",
    "\n",
    "    # Passive to Active Ratio\n",
    "    similarities['passive_to_active_ratio'] = max(0, 1 - abs(feature_set1[201] - feature_set2[201]))\n",
    "\n",
    "    # Gender prediction\n",
    "    similarities['gender_prediction'] = 1 if feature_set1[202] == feature_set2[202] else 0\n",
    "\n",
    "    # English variant\n",
    "    #similarities['english_variant'] = 1 if feature_set1[203] == feature_set2[203] else 0\n",
    "\n",
    "    # Double spaces after full stop\n",
    "    similarities['double_spaces'] = 1 if feature_set1[203] == feature_set2[203] else 0\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the similarity values to get the required key-value pairs\n",
    "def preprocess(similarity_dict):\n",
    "    fixed_indexes = [0, 3, 10, 13, 23, 29, 32, 200, 202, 203]  # Specify the indexes you want to extract\n",
    "    keys = list(similarity_dict.keys())\n",
    "    values = list(similarity_dict.values())\n",
    "    extracted_keys = [keys[i] for i in fixed_indexes]\n",
    "    extracted_values = [values[i] for i in fixed_indexes]\n",
    "    return extracted_keys, extracted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.0 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 1s 665ms/step\n",
      "[0.5, 1, 1, 1, 1, 1, 0.5, 0.875, 1, 1]\n",
      "{'sentence_length': 0.5, 'punctuation_1': 1, 'punctuation_2': 1, 'punctuation_3': 1, 'punctuation_4': 1, 'punctuation_5': 1, 'punctuation_6': 1, 'punctuation_7': 1, 'punctuation_8': 0.5, 'punctuation_9': 1, 'punctuation_10': 1, 'punctuation_11': 1, 'punctuation_12': 1, 'punctuation_13': 1, 'punctuation_14': 1, 'punctuation_15': 1, 'punctuation_16': 1, 'punctuation_17': 1, 'punctuation_18': 1, 'punctuation_19': 1, 'punctuation_20': 1, 'punctuation_21': 1, 'punctuation_22': 1, 'punctuation_23': 1, 'punctuation_24': 1, 'punctuation_25': 1, 'punctuation_26': 1, 'punctuation_27': 1, 'punctuation_28': 1, 'punctuation_29': 1, 'punctuation_30': 1, 'punctuation_31': 1, 'punctuation_32': 0.5, 'pos_tag_CC': 1, 'pos_tag_CD': 1, 'pos_tag_DT': 0.8571428571428572, 'pos_tag_EX': 1, 'pos_tag_FW': 1, 'pos_tag_IN': 1, 'pos_tag_JJ': 0.9821428571428572, 'pos_tag_JJR': 1, 'pos_tag_JJS': 1, 'pos_tag_LS': 1, 'pos_tag_MD': 0.8571428571428572, 'pos_tag_NN': 0.75, 'pos_tag_NNS': 1, 'pos_tag_NNP': 0.75, 'pos_tag_NNPS': 1, 'pos_tag_PDT': 1, 'pos_tag_POS': 1, 'pos_tag_PRP': 0.8392857142857143, 'pos_tag_PRP$': 1, 'pos_tag_RB': 1, 'pos_tag_RBR': 1, 'pos_tag_RBS': 1, 'pos_tag_RP': 1, 'pos_tag_SYM': 1, 'pos_tag_TO': 1, 'pos_tag_UH': 1, 'pos_tag_VB': 0.8571428571428572, 'pos_tag_VBD': 1, 'pos_tag_VBG': 1, 'pos_tag_VBN': 1, 'pos_tag_VBP': 0.875, 'pos_tag_VBZ': 0.8571428571428572, 'pos_tag_WDT': 1, 'pos_tag_WP': 1, 'pos_tag_WP$': 1, 'pos_tag_WRB': 1, 'function_word_i': 1, 'function_word_me': 0.75, 'function_word_my': 1, 'function_word_myself': 1, 'function_word_we': 1, 'function_word_our': 1, 'function_word_ours': 1, 'function_word_ourselves': 1, 'function_word_you': 1, 'function_word_your': 1, 'function_word_yours': 1, 'function_word_yourself': 1, 'function_word_yourselves': 1, 'function_word_he': 0.75, 'function_word_him': 1, 'function_word_his': 1, 'function_word_himself': 1, 'function_word_she': 1, 'function_word_her': 1, 'function_word_hers': 1, 'function_word_herself': 1, 'function_word_it': 1, 'function_word_its': 1, 'function_word_itself': 1, 'function_word_they': 1, 'function_word_them': 1, 'function_word_their': 1, 'function_word_theirs': 1, 'function_word_themselves': 1, 'function_word_what': 1, 'function_word_which': 1, 'function_word_who': 1, 'function_word_whom': 1, 'function_word_this': 1, 'function_word_that': 1, 'function_word_these': 1, 'function_word_those': 1, 'function_word_am': 1, 'function_word_is': 1, 'function_word_are': 1, 'function_word_was': 1, 'function_word_were': 1, 'function_word_be': 1, 'function_word_been': 1, 'function_word_being': 1, 'function_word_have': 1, 'function_word_has': 1, 'function_word_had': 1, 'function_word_having': 1, 'function_word_do': 1, 'function_word_does': 1, 'function_word_did': 1, 'function_word_doing': 1, 'function_word_a': 1, 'function_word_an': 1, 'function_word_the': 1, 'function_word_and': 1, 'function_word_but': 1, 'function_word_if': 1, 'function_word_or': 1, 'function_word_because': 1, 'function_word_as': 1, 'function_word_until': 1, 'function_word_while': 1, 'function_word_of': 1, 'function_word_at': 1, 'function_word_by': 1, 'function_word_for': 1, 'function_word_with': 1, 'function_word_about': 1, 'function_word_against': 1, 'function_word_between': 1, 'function_word_into': 1, 'function_word_through': 1, 'function_word_during': 1, 'function_word_before': 1, 'function_word_after': 1, 'function_word_above': 1, 'function_word_below': 1, 'function_word_to': 1, 'function_word_from': 1, 'function_word_up': 1, 'function_word_down': 1, 'function_word_in': 1, 'function_word_out': 1, 'function_word_on': 1, 'function_word_off': 1, 'function_word_over': 1, 'function_word_under': 1, 'function_word_again': 1, 'function_word_further': 1, 'function_word_then': 1, 'function_word_once': 1, 'function_word_here': 1, 'function_word_there': 1, 'function_word_when': 1, 'function_word_where': 1, 'function_word_why': 1, 'function_word_how': 1, 'function_word_all': 1, 'function_word_any': 1, 'function_word_both': 1, 'function_word_each': 1, 'function_word_few': 1, 'function_word_more': 1, 'function_word_most': 1, 'function_word_other': 1, 'function_word_some': 1, 'function_word_such': 1, 'function_word_no': 1, 'function_word_nor': 1, 'function_word_not': 1, 'function_word_only': 1, 'function_word_own': 1, 'function_word_same': 1, 'function_word_so': 1, 'function_word_than': 1, 'function_word_too': 1, 'function_word_very': 1, 'function_word_s': 1, 'function_word_t': 1, 'function_word_can': 0.75, 'function_word_will': 1, 'function_word_just': 1, 'function_word_don': 1, 'function_word_should': 1, 'function_word_now': 1, 'ngram_transition_num_nodes': 0, 'ngram_transition_num_edges': 0, 'ngram_transition_avg_degree': 0.8333333333333333, 'ngram_transition_density': 0.9166666666666667, 'type_token_ratio': 0.875, 'passive_to_active_ratio': 1, 'gender_prediction': 1, 'double_spaces': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>punctuation_3</th>\n",
       "      <th>punctuation_10</th>\n",
       "      <th>punctuation_13</th>\n",
       "      <th>punctuation_23</th>\n",
       "      <th>punctuation_29</th>\n",
       "      <th>punctuation_32</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>gender_prediction</th>\n",
       "      <th>double_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_length punctuation_3 punctuation_10 punctuation_13 punctuation_23  \\\n",
       "0             0.5             1              1              1              1   \n",
       "\n",
       "  punctuation_29 punctuation_32 type_token_ratio gender_prediction  \\\n",
       "0              1            0.5            0.875                 1   \n",
       "\n",
       "  double_spaces  \n",
       "0             1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Define suspect and anonymous texts\n",
    "suspect_text = \"Hi I'm weird lolll. Hi hwgohehd\"\n",
    "anonymous_text = \"This weirdo thinks he can kill me\"\n",
    "\n",
    "# Extract features for the suspect text\n",
    "suspect_features = calculate_all_features(suspect_text)\n",
    "\n",
    "# Extract features for the anonymous text\n",
    "anonymous_features = calculate_all_features(anonymous_text)\n",
    "\n",
    "# Compute similarity between suspect and anonymous text\n",
    "new_similarity_values = compute_similarity(suspect_features, anonymous_features)\n",
    "\n",
    "# Get preprocessed keys and values\n",
    "preprocessed_keys, preprocessed_values = preprocess(new_similarity_values)\n",
    "\n",
    "# Create a DataFrame using the preprocessed values\n",
    "new_data = pd.DataFrame([preprocessed_values], columns=[preprocessed_keys])\n",
    "\n",
    "print(preprocessed_values)\n",
    "print(new_similarity_values)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of class 1: [0.70909091]\n",
      "Predictions: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the pickle file\n",
    "with open('random_forest_model.pickle', 'rb') as f:\n",
    "    model_info = pickle.load(f)\n",
    "\n",
    "# Make predictions and obtain predicted probabilities\n",
    "y_pred_proba_new = model_info['model'].predict_proba(new_data)\n",
    "y_pred_new = model_info['model'].predict(new_data)\n",
    "probability_class_1 = y_pred_proba_new[:, 1]\n",
    "print(\"Probability of class 1:\", probability_class_1)\n",
    "# Display predictions and probabilities\n",
    "print(\"Predictions:\", y_pred_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
