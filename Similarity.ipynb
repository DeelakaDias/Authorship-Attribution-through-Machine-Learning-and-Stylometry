{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pc\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentence_length                              punctuation_frequency  \\\n",
      "0          0.157143  {'!': 0.0625, '\"': 0, '#': 0, '$': 0, '%': 0, ...   \n",
      "1          0.213333  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "2          0.123333  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "3          0.085000  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "4          0.120000  {'!': 0.125, '\"': 0, '#': 0, '$': 0, '%': 0, '...   \n",
      "5          0.133000  {'!': 0.041666666666666664, '\"': 0.08333333333...   \n",
      "6          0.710000  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "7          0.118462  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "8          0.111429  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "9          0.166000  {'!': 0, '\"': 0.06666666666666667, '#': 0, '$'...   \n",
      "10         0.156667  {'!': 0.058823529411764705, '\"': 0, '#': 0, '$...   \n",
      "11         0.138750  {'!': 0.0625, '\"': 0.0625, '#': 0, '$': 0, '%'...   \n",
      "12         0.190909  {'!': 0.027777777777777776, '\"': 0, '#': 0, '$...   \n",
      "13         0.147674  {'!': 0, '\"': 0.05217391304347826, '#': 0, '$'...   \n",
      "14         0.750000  {'!': 0, '\"': 0.2222222222222222, '#': 0, '$':...   \n",
      "15         0.108333  {'!': 0.08333333333333333, '\"': 0, '#': 0, '$'...   \n",
      "16         0.172683  {'!': 0.015625, '\"': 0.0625, '#': 0, '$': 0, '...   \n",
      "17         0.130390  {'!': 0.03626943005181347, '\"': 0.082901554404...   \n",
      "18         0.139687  {'!': 0, '\"': 0.16216216216216217, '#': 0, '$'...   \n",
      "19         0.152857  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "20         0.120000  {'!': 0, '\"': 0.14285714285714285, '#': 0, '$'...   \n",
      "21         0.115000  {'!': 0.5, '\"': 0, '#': 0, '$': 0, '%': 0, '&'...   \n",
      "22         0.276000  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "23         0.207778  {'!': 0.04838709677419355, '\"': 0, '#': 0, '$'...   \n",
      "24         0.157500  {'!': 0.125, '\"': 0, '#': 0, '$': 0, '%': 0, '...   \n",
      "25         0.058182  {'!': 0.07142857142857142, '\"': 0, '#': 0, '$'...   \n",
      "26         0.172143  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "27         0.118333  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n",
      "28         0.145263  {'!': 0.02666666666666667, '\"': 0.106666666666...   \n",
      "\n",
      "                                    pos_tag_frequency  \\\n",
      "0   {'CC': 0.031496062992125984, 'PRP': 0.07874015...   \n",
      "1   {'CC': 0.04, 'PRP': 0.12, 'VB': 0.013333333333...   \n",
      "2   {'CC': 0.012195121951219513, 'PRP': 0.10975609...   \n",
      "3   {'CD': 0.07692307692307693, 'VBZ': 0.025641025...   \n",
      "4   {'VBZ': 0.014925373134328358, 'CD': 0.04477611...   \n",
      "5   {'``': 0.00641025641025641, 'JJR': 0.006410256...   \n",
      "6   {'(': 0.011904761904761904, 'IN': 0.0476190476...   \n",
      "7   {'JJR': 0.02247191011235955, 'CC': 0.022471910...   \n",
      "8   {'JJR': 0.010638297872340425, 'CC': 0.02127659...   \n",
      "9   {'``': 0.005128205128205128, 'JJR': 0.00512820...   \n",
      "10  {'CC': 0.01639344262295082, 'PRP': 0.131147540...   \n",
      "11  {'JJR': 0.008064516129032258, 'CC': 0.03225806...   \n",
      "12  {'JJR': 0.012345679012345678, 'CC': 0.02880658...   \n",
      "13  {'``': 0.004043126684636119, 'JJR': 0.00673854...   \n",
      "14  {'``': 0.011627906976744186, 'JJR': 0.01162790...   \n",
      "15  {'CC': 0.012987012987012988, 'UH': 0.012987012...   \n",
      "16  {'``': 0.0048484848484848485, 'JJR': 0.0012121...   \n",
      "17  {'``': 0.006762468300929839, 'JJR': 0.00422654...   \n",
      "18  {'``': 0.01651376146788991, 'JJR': 0.001834862...   \n",
      "19  {'CC': 0.024793388429752067, 'PRP': 0.05785123...   \n",
      "20  {'``': 0.01282051282051282, 'JJR': 0.006410256...   \n",
      "21  {'CC': 0.0392156862745098, 'UH': 0.01960784313...   \n",
      "22  {'CC': 0.050955414012738856, 'PRP': 0.07324840...   \n",
      "23  {'JJR': 0.004662004662004662, 'CC': 0.05128205...   \n",
      "24  {'CC': 0.07042253521126761, 'PRP': 0.070422535...   \n",
      "25  {'PRP': 0.10256410256410256, 'VB': 0.025641025...   \n",
      "26  {'CC': 0.05204460966542751, 'PRP': 0.156133828...   \n",
      "27  {'CC': 0.011627906976744186, 'PRP': 0.11627906...   \n",
      "28  {'``': 0.011461318051575931, 'CC': 0.040114613...   \n",
      "\n",
      "                                      phrase_patterns  type_token_ratio  \\\n",
      "0                                        [(was, too)]          0.614173   \n",
      "1                                          [(I, was)]          0.746667   \n",
      "2                           [(she, was), (the, next)]          0.658537   \n",
      "3                                                  []          0.871795   \n",
      "4                                                  []          0.791045   \n",
      "5   [(have, nothing), (nothing, to), (to, do.), (I...          0.628205   \n",
      "6                                                  []          0.809524   \n",
      "7                          [(into, the), (wants, to)]          0.674157   \n",
      "8                  [(I, have), (have, a), (want, to)]          0.574468   \n",
      "9   [(the, tiles), (Of, course,), (the, two), (or,...          0.558974   \n",
      "10                                                 []          0.770492   \n",
      "11                        [(for, the), (tiles, that)]          0.750000   \n",
      "12  [(I, had), (had, a), (a, glass), (It, was), (a...          0.596708   \n",
      "13  [(was, a), (if, I), (I, was), (it, was), (more...          0.402965   \n",
      "14  [(with, addons), (addons, fused), (fused, in),...          0.732558   \n",
      "15                                                 []          0.779221   \n",
      "16  [(of, bus), (in, every), (importance, and), (a...          0.469091   \n",
      "17  [(I, am), (of, the), (to, work.), (I, get), (g...          0.437025   \n",
      "18  [(of, the), (for, the), (are, a), (kind, of), ...          0.530275   \n",
      "19                             [(to, write), (to, a)]          0.652893   \n",
      "20  [(a, Lonely), (I, could), (then, I), (Lonely, ...          0.647436   \n",
      "21                                                 []          0.784314   \n",
      "22  [(to, go), (to, work), (and, your), (is, in), ...          0.547771   \n",
      "23  [(I, have), (at, least), (I, am), (am, a), (I,...          0.466200   \n",
      "24                           [(and, you), (you, get)]          0.760563   \n",
      "25                                    [(turned, out)]          0.730769   \n",
      "26  [(and, I), (in, town), (town, and), (so, sad),...          0.472119   \n",
      "27                                                 []          0.674419   \n",
      "28  [(if, you), (to, think), (think, a), (have, to...          0.478510   \n",
      "\n",
      "    voice_detection  author_a  \n",
      "0          1.000000         1  \n",
      "1          1.000000         1  \n",
      "2          1.000000         1  \n",
      "3          1.000000         1  \n",
      "4          0.800000         1  \n",
      "5          1.000000         1  \n",
      "6          1.000000         1  \n",
      "7          0.769231         1  \n",
      "8          1.000000         1  \n",
      "9          0.800000         1  \n",
      "10         0.666667         1  \n",
      "11         0.750000         1  \n",
      "12         0.818182         1  \n",
      "13         0.976744         1  \n",
      "14         1.000000         1  \n",
      "15         0.833333         1  \n",
      "16         0.682927         0  \n",
      "17         0.857143         0  \n",
      "18         0.656250         0  \n",
      "19         0.714286         0  \n",
      "20         1.000000         0  \n",
      "21         1.000000         0  \n",
      "22         0.700000         0  \n",
      "23         0.888889         0  \n",
      "24         0.750000         0  \n",
      "25         0.909091         0  \n",
      "26         0.857143         0  \n",
      "27         0.666667         0  \n",
      "28         0.947368         0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "\n",
    "# Define functions for additional features\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams\n",
    "import string\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def phrase_patterns(text):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < 2:\n",
    "        return []  # Return empty list for texts with less than two words\n",
    "    bigram_counts = Counter(bigrams(tokens))\n",
    "    significant_collocations = [bigram for bigram, count in bigram_counts.items() if count > 1]  # Example threshold for significance\n",
    "    return significant_collocations\n",
    "\n",
    "def sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "    return avg_sentence_length / 100  # Normalize between 0 and 1\n",
    "\n",
    "def punctuation_frequency(text):\n",
    "    punctuation_counts = Counter(char for char in text if char in string.punctuation)\n",
    "    total_punctuation = sum(punctuation_counts.values())\n",
    "    punctuation_distribution = {punct: count / total_punctuation for punct, count in punctuation_counts.items()}\n",
    "    return punctuation_distribution\n",
    "\n",
    "def pos_tag_frequency(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    pos_tag_counts = Counter(tag for word, tag in pos_tags)\n",
    "    total_pos_tags = sum(pos_tag_counts.values())\n",
    "    pos_tag_distribution = {tag: count / total_pos_tags for tag, count in pos_tag_counts.items()}\n",
    "    return pos_tag_distribution\n",
    "\n",
    "def function_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
    "    return function_words_text\n",
    "\n",
    "def ngram_transition_graph(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    transition_graph.add_nodes_from(ngrams)\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "    return transition_graph\n",
    "\n",
    "def ngram_transition_graph_similarity(graph1, graph2):\n",
    "    nodes_graph1 = set(graph1.nodes)\n",
    "    nodes_graph2 = set(graph2.nodes)\n",
    "    intersection = nodes_graph1.intersection(nodes_graph2)\n",
    "    union = nodes_graph1.union(nodes_graph2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "def voice_detection(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    active_count = 0\n",
    "    passive_count = 0\n",
    "    for sentence in sentences:\n",
    "        if 'is' in sentence.split() or 'are' in sentence.split():\n",
    "            passive_count += 1\n",
    "        else:\n",
    "            active_count += 1\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences == 0:\n",
    "        return 0\n",
    "    return active_count / total_sentences  # Normalize between 0 and 1\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "texts_positive = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "    \"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "    \"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "    \"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "    \"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "    \"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "    \"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "    \"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "    \"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "    \"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "    \"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "    \"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "    \"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "    \"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "    \"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "    \"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "texts_negative = [\"\"\"\t\t\t\t\t\t\n",
    "        As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "    \"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\n",
    "    \"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "    I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "    ]\n",
    "\n",
    "# Define texts_positive, texts_negative, and other necessary variables here.\n",
    "# Create empty lists to store extracted features and class labels\n",
    "feature_dicts = []\n",
    "\n",
    "# Iterate through each text to extract features\n",
    "for text in texts_positive + texts_negative:\n",
    "    features = {}\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "    # Pad sequences to make them of equal length\n",
    "    max_sequence_length = max([len(seq) for seq in sequences])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')  # Padding sequences using pad_sequences\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    # Extract additional features\n",
    "    features['sentence_length'] = sentence_length(text)\n",
    "    punctuation_freq = punctuation_frequency(text)\n",
    "    pos_tag_freq = pos_tag_frequency(text)\n",
    "    features['punctuation_frequency'] = {key: punctuation_freq.get(key, 0) for key in string.punctuation}\n",
    "    \n",
    "    # Obtain all possible POS tags from NLTK\n",
    "    pos_tags = set([tag for _, tag in nltk.pos_tag(nltk.word_tokenize(text))])\n",
    "    features['pos_tag_frequency'] = {key: pos_tag_freq.get(key, 0) for key in pos_tags}\n",
    "    \n",
    "    features['phrase_patterns'] = phrase_patterns(text)\n",
    "    features['type_token_ratio'] = type_token_ratio(text)\n",
    "    features['voice_detection'] = voice_detection(text)\n",
    "    \n",
    "    # Add class label\n",
    "    if text in texts_positive:\n",
    "        features['author_a'] = 1  # Positive class (Author A)\n",
    "    else:\n",
    "        features['author_a'] = 0  # Negative class (other authors)\n",
    "\n",
    "    feature_dicts.append(features)\n",
    "\n",
    "# Create DataFrame from the list of feature dictionaries\n",
    "text_features_df = pd.DataFrame(feature_dicts)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(text_features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Initialize preprocessors\n",
    "scaler = MinMaxScaler()\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features = text_features_df[['sentence_length', 'type_token_ratio']]\n",
    "scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Encode categorical feature\n",
    "categorical_feature = text_features_df['voice_detection']\n",
    "encoded_categorical_feature = LabelEncoder().fit_transform(categorical_feature)\n",
    "\n",
    "# Combine preprocessed features\n",
    "preprocessed_features = np.concatenate([scaled_numerical_features, encoded_categorical_feature.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# Extract target labels\n",
    "labels = text_features_df['author_a'].values\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_features, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming you have preprocessed features and labels ready\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Define your GRU model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mvocab_size\u001b[49m, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_sequence_length))\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39madd(GRU(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(GRU(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GRU(units=32))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_4 (GRU)                 (None, 128)               51072     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59393 (232.00 KB)\n",
      "Trainable params: 59393 (232.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 1, 3), found shape=(None, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Reshape X_test to match the input shape expected by the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3npnf7gu.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 1, 3), found shape=(None, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "\n",
    "# Reshape input data to include timestep dimension\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Define the GRU model with the correct input shape\n",
    "model = Sequential([\n",
    "    GRU(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(1, X_train.shape[1])),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Reshape X_test to match the input shape expected by the model\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train before reshaping: (23, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train before reshaping:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tokenize text data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m----> 7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mtexts\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert text data to sequences\u001b[39;00m\n\u001b[0;32m     10\u001b[0m sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(texts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert text data to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 100  # Adjust as needed\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure that the input shape is compatible with the GRU layer\n",
    "input_shape = (X_train.shape[1],)  # Shape of a single input sample (sequence length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"gru_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define the GRU model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGRU\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurrent_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"gru_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 6)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and target labels\n",
    "X = text_features_df.drop(columns=['author_a'])\n",
    "y = text_features_df['author_a']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential([\n",
    "    GRU(units=128, dropout=0.2, recurrent_dropout=0.2, input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Obtaining dependency information for pyspellchecker from https://files.pythonhosted.org/packages/e1/d2/c7e3b3a61a34b9320399fa731d1f9f0c73db8a1f28c6764e9e11efa68a29/pyspellchecker-0.8.1-py3-none-any.whl.metadata\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 187.9 kB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.0/6.8 MB 187.9 kB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.1/6.8 MB 302.7 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.1/6.8 MB 302.7 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.1/6.8 MB 302.7 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.1/6.8 MB 302.7 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.1/6.8 MB 267.6 kB/s eta 0:00:25\n",
      "    --------------------------------------- 0.1/6.8 MB 267.6 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.2/6.8 MB 377.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.2/6.8 MB 377.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.2/6.8 MB 353.1 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.2/6.8 MB 369.9 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.2/6.8 MB 369.9 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/6.8 MB 370.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/6.8 MB 370.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/6.8 MB 378.3 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/6.8 MB 378.3 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.4/6.8 MB 378.0 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.4/6.8 MB 422.5 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.4/6.8 MB 422.5 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.5/6.8 MB 437.0 kB/s eta 0:00:15\n",
      "   -- ------------------------------------- 0.5/6.8 MB 437.0 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.5/6.8 MB 440.1 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.5/6.8 MB 440.1 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 0.6/6.8 MB 448.1 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.6/6.8 MB 448.1 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.6/6.8 MB 472.7 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.6/6.8 MB 472.7 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.7/6.8 MB 448.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.7/6.8 MB 448.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.7/6.8 MB 448.9 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.7/6.8 MB 463.6 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.7/6.8 MB 463.6 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 455.8 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 455.8 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 474.0 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 498.9 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 491.7 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 478.2 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 481.1 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 481.1 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.0/6.8 MB 463.9 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.0/6.8 MB 500.5 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.0/6.8 MB 500.5 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.1/6.8 MB 517.7 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.1/6.8 MB 517.7 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.1/6.8 MB 508.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.1/6.8 MB 508.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.1/6.8 MB 508.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.1/6.8 MB 508.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.2/6.8 MB 467.5 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.2/6.8 MB 467.5 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.2/6.8 MB 467.5 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 1.2/6.8 MB 479.6 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.2/6.8 MB 479.6 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.3/6.8 MB 478.1 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.3/6.8 MB 478.1 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.3/6.8 MB 470.3 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.4/6.8 MB 486.0 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.4/6.8 MB 486.0 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.4/6.8 MB 498.6 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 1.4/6.8 MB 498.6 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 1.5/6.8 MB 507.2 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 1.5/6.8 MB 507.2 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 1.6/6.8 MB 505.7 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 1.6/6.8 MB 505.7 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 1.6/6.8 MB 498.0 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 1.6/6.8 MB 498.0 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 518.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 518.1 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.8/6.8 MB 527.9 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.8/6.8 MB 527.9 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.8/6.8 MB 523.2 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 1.8/6.8 MB 523.2 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 530.1 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 530.1 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 536.0 kB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 536.0 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.1/6.8 MB 539.5 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.1/6.8 MB 539.5 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.1/6.8 MB 547.5 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.2/6.8 MB 560.7 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.2/6.8 MB 560.7 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 2.2/6.8 MB 558.1 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 2.2/6.8 MB 558.1 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 2.3/6.8 MB 552.9 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 2.3/6.8 MB 552.9 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 2.4/6.8 MB 565.2 kB/s eta 0:00:08\n",
      "   -------------- ------------------------- 2.4/6.8 MB 565.2 kB/s eta 0:00:08\n",
      "   -------------- ------------------------- 2.5/6.8 MB 569.9 kB/s eta 0:00:08\n",
      "   -------------- ------------------------- 2.5/6.8 MB 569.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.5/6.8 MB 576.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.5/6.8 MB 576.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 2.6/6.8 MB 568.9 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 561.5 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 561.5 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 559.3 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 565.2 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 565.2 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 565.2 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 565.2 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 549.7 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 549.7 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 549.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 549.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 549.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 542.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 542.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 544.7 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.1/6.8 MB 544.7 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.2/6.8 MB 548.5 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.2/6.8 MB 548.5 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.2/6.8 MB 545.0 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.3/6.8 MB 550.4 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.3/6.8 MB 550.4 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.3/6.8 MB 552.1 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.3/6.8 MB 552.1 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.4/6.8 MB 552.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.4/6.8 MB 552.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 550.5 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 550.5 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 545.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 545.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 545.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.5/6.8 MB 547.5 kB/s eta 0:00:06\n",
      "   -------------------- ------------------- 3.5/6.8 MB 547.5 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.6/6.8 MB 546.2 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.6/6.8 MB 546.2 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.7/6.8 MB 546.5 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.7/6.8 MB 546.5 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.7/6.8 MB 549.7 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.7/6.8 MB 549.7 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 555.4 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 555.4 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 548.2 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 548.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 3.9/6.8 MB 551.0 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 3.9/6.8 MB 550.0 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 551.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 551.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 551.1 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 4.1/6.8 MB 558.2 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.4/6.8 MB 566.1 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.4/6.8 MB 566.1 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.5/6.8 MB 565.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.5/6.8 MB 565.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.5/6.8 MB 566.9 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.5/6.8 MB 566.9 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.6/6.8 MB 562.8 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.6/6.8 MB 565.6 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.6/6.8 MB 565.6 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.7/6.8 MB 569.2 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.7/6.8 MB 569.2 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.7/6.8 MB 569.0 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.7/6.8 MB 569.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 571.3 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 571.3 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 573.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 573.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 569.6 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 4.9/6.8 MB 572.1 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 571.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 571.8 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 571.8 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 568.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 568.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.1/6.8 MB 567.9 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 5.1/6.8 MB 567.9 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 5.1/6.8 MB 566.8 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.1/6.8 MB 566.8 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/6.8 MB 567.7 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/6.8 MB 567.7 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/6.8 MB 564.3 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/6.8 MB 565.6 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/6.8 MB 565.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.3/6.8 MB 564.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.3/6.8 MB 564.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.3/6.8 MB 564.1 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.3/6.8 MB 564.1 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.4/6.8 MB 566.2 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.4/6.8 MB 566.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.4/6.8 MB 563.9 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 567.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 563.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 566.9 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 566.9 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 566.9 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.5/6.8 MB 558.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.6/6.8 MB 557.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 5.6/6.8 MB 557.8 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 5.6/6.8 MB 557.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 5.7/6.8 MB 560.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.7/6.8 MB 562.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 567.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 566.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.9/6.8 MB 567.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.9/6.8 MB 571.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.9/6.8 MB 571.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.9/6.8 MB 571.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 6.0/6.8 MB 574.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 6.1/6.8 MB 576.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 6.1/6.8 MB 578.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 6.2/6.8 MB 579.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 6.2/6.8 MB 581.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.8 MB 582.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.3/6.8 MB 586.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.3/6.8 MB 585.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 589.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 592.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 594.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 594.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 591.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.6/6.8 MB 595.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.8 MB 591.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.7/6.8 MB 596.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.7/6.8 MB 596.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 598.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 598.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 595.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 592.7 kB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in calculate_similarity: name 'bigrams' is not defined\n",
      "Error in calculate_similarity. Please check the error message above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_4856\\2196734618.py\", line 77, in calculate_similarity\n",
      "    graph1 = generate_ngram_transition_graph(text1, n_value)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_4856\\2196734618.py\", line 27, in generate_ngram_transition_graph\n",
      "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
      "                  ^^^^^^^\n",
      "NameError: name 'bigrams' is not defined\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_function_words(text):\n",
    "    # Define a set of common English function words\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    # Tokenize and lowercase the words in the text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out function words\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0  # To handle the case when both graphs are empty\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    try:\n",
    "        # Tokenize sentences\n",
    "        sentences1 = sent_tokenize(text1)\n",
    "        sentences2 = sent_tokenize(text2)\n",
    "\n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words1 = [pos_tag(word_tokenize(sentence)) for sentence in sentences1]\n",
    "        words2 = [pos_tag(word_tokenize(sentence)) for sentence in sentences2]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled1 = set([spell.correction(word) for sentence in words1 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "        misspelled2 = set([spell.correction(word) for sentence in words2 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # Calculate similarity based on sentence structure, spelling, and punctuation frequency\n",
    "        structure_similarity = len(sentences1) / len(sentences2)\n",
    "        spelling_similarity = len(misspelled1.intersection(misspelled2)) / len(misspelled1.union(misspelled2))\n",
    "\n",
    "        # POS Tagging Similarity\n",
    "        pos_tags_similarity = pos_tag_similarity(words1, words2)\n",
    "\n",
    "        # Sentence Length Similarity\n",
    "        length_similarity = sentence_length_similarity(sentences1, sentences2)\n",
    "\n",
    "        # Punctuation Frequency Similarity\n",
    "        punctuation_sim = punctuation_similarity(text1, text2)\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph1 = generate_ngram_transition_graph(text1, n_value)\n",
    "        graph2 = generate_ngram_transition_graph(text2, n_value)\n",
    "\n",
    "        # Compute Jaccard similarity between the n-gram word transition graphs\n",
    "        ngram_similarity = compute_jaccard_similarity(graph1, graph2)\n",
    "        \n",
    "        # Extract function words\n",
    "        function_words1 = get_function_words(text1)\n",
    "        function_words2 = get_function_words(text2)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_word_count_similarity = len(set(function_words1).intersection(function_words2)) / len(set(function_words1).union(function_words2))\n",
    "\n",
    "        return structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity,ngram_similarity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_similarity: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def sentence_length_similarity(sentences1, sentences2):\n",
    "    avg_len1 = sum(len(sentence) for sentence in sentences1) / len(sentences1)\n",
    "    avg_len2 = sum(len(sentence) for sentence in sentences2) / len(sentences2)\n",
    "\n",
    "    return min(avg_len1, avg_len2) / max(avg_len1, avg_len2)\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation1 = [char for char in text1 if char in string.punctuation]\n",
    "    punctuation2 = [char for char in text2 if char in string.punctuation]\n",
    "\n",
    "    common_punctuations = set(punctuation1).intersection(punctuation2)\n",
    "    total_punctuations = set(punctuation1).union(punctuation2)\n",
    "\n",
    "    return len(common_punctuations) / len(total_punctuations)\n",
    "\n",
    "def main():\n",
    "    text1 = \"This is a sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\"\n",
    "    text2 = \"This is another sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\"\n",
    "\n",
    "    result = calculate_similarity(text1, text2)\n",
    "\n",
    "    if result is not None:\n",
    "        structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity,ngram_similarity = result\n",
    "        print(f\"Sentence Structure Similarity: {structure_similarity}\")\n",
    "        print(f\"Spelling Similarity: {spelling_similarity}\")\n",
    "        print(f\"POS Tag Similarity: {pos_tags_similarity}\")\n",
    "        print(f\"Sentence Length Similarity: {length_similarity}\")\n",
    "        print(f\"Punctuation Similarity: {punctuation_sim}\")\n",
    "        print(f\"N-Gram Word Transition Graph Similarity: {ngram_similarity}\")\n",
    "        print(f\"Function Word Count Similarity: {function_word_count_similarity}\")\n",
    "    else:\n",
    "        print(\"Error in calculate_similarity. Please check the error message above.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\anaconda3\\lib\\site-packages (1.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy in c:\\users\\pc\\anaconda3\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Structure Similarity: 1.0\n",
      "Spelling Similarity: 0.9\n",
      "POS Tag Similarity: 0.9\n",
      "Sentence Length Similarity: 0.9444444444444444\n",
      "Punctuation Similarity: 1.0\n",
      "N-Gram Word Transition Graph Similarity: 0.8947368421052632\n",
      "Function Word Count Similarity: 0.8\n",
      "LSA Similarity: 1\n",
      "TTR for text 1: 0.9\n",
      "TTR for text 2: 0.9\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_function_words(text):\n",
    "    # Define a set of common English function words\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    # Tokenize and lowercase the words in the text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out function words\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0  # To handle the case when both graphs are empty\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Count the number of unique words (types) and total number of words (tokens)\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    # Calculate Type-Token Ratio (TTR)\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0  # Handle the case when the text is empty\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    try:\n",
    "        # Tokenize sentences\n",
    "        sentences1 = sent_tokenize(text1)\n",
    "        sentences2 = sent_tokenize(text2)\n",
    "       \n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words1 = [pos_tag(word_tokenize(sentence)) for sentence in sentences1]\n",
    "        words2 = [pos_tag(word_tokenize(sentence)) for sentence in sentences2]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled1 = set([spell.correction(word) for sentence in words1 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "        misspelled2 = set([spell.correction(word) for sentence in words2 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # Calculate similarity based on sentence structure, spelling, and punctuation frequency\n",
    "        structure_similarity = len(sentences1) / len(sentences2)\n",
    "        spelling_similarity = len(misspelled1.intersection(misspelled2)) / len(misspelled1.union(misspelled2))\n",
    "\n",
    "        # POS Tagging Similarity\n",
    "        pos_tags_similarity = pos_tag_similarity(words1, words2)\n",
    "\n",
    "        # Sentence Length Similarity\n",
    "        length_similarity = sentence_length_similarity(sentences1, sentences2)\n",
    "\n",
    "        # Punctuation Frequency Similarity\n",
    "        punctuation_sim = punctuation_similarity(text1, text2)\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph1 = generate_ngram_transition_graph(text1, n_value)\n",
    "        graph2 = generate_ngram_transition_graph(text2, n_value)\n",
    "\n",
    "        # Compute Jaccard similarity between the n-gram word transition graphs\n",
    "        ngram_similarity = compute_jaccard_similarity(graph1, graph2)\n",
    "        \n",
    "        # Extract function words\n",
    "        function_words1 = get_function_words(text1)\n",
    "        function_words2 = get_function_words(text2)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_word_count_similarity = len(set(function_words1).intersection(function_words2)) / len(set(function_words1).union(function_words2))\n",
    "\n",
    "        # Apply Latent Semantic Analysis (LSA)\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # LSA Similarity\n",
    "        lsa_similarity = 1 - spatial.distance.cosine(lsa_matrix[0], lsa_matrix[1])\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr1 = calculate_ttr(text1)\n",
    "        ttr2 = calculate_ttr(text2)\n",
    "    \n",
    "        return structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity, ngram_similarity, lsa_similarity, ttr1, ttr2\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_similarity: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def sentence_length_similarity(sentences1, sentences2):\n",
    "    avg_len1 = sum(len(sentence) for sentence in sentences1) / len(sentences1)\n",
    "    avg_len2 = sum(len(sentence) for sentence in sentences2) / len(sentences2)\n",
    "\n",
    "    return min(avg_len1, avg_len2) / max(avg_len1, avg_len2)\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation1 = [char for char in text1 if char in string.punctuation]\n",
    "    punctuation2 = [char for char in text2 if char in string.punctuation]\n",
    "\n",
    "    common_punctuations = set(punctuation1).intersection(punctuation2)\n",
    "    total_punctuations = set(punctuation1).union(punctuation2)\n",
    "\n",
    "    return len(common_punctuations) / len(total_punctuations)\n",
    "\n",
    "def main():\n",
    "    text1 = \"This is a sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\"\n",
    "    text2 = \"This is another sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\"\n",
    "\n",
    "    result = calculate_similarity(text1, text2)\n",
    "\n",
    "    if result is not None:\n",
    "        structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity, ngram_similarity, lsa_similarity, ttr1, ttr2 = result\n",
    "        print(f\"Sentence Structure Similarity: {structure_similarity}\")\n",
    "        print(f\"Spelling Similarity: {spelling_similarity}\")\n",
    "        print(f\"POS Tag Similarity: {pos_tags_similarity}\")\n",
    "        print(f\"Sentence Length Similarity: {length_similarity}\")\n",
    "        print(f\"Punctuation Similarity: {punctuation_sim}\")\n",
    "        print(f\"N-Gram Word Transition Graph Similarity: {ngram_similarity}\")\n",
    "        print(f\"Function Word Count Similarity: {function_word_count_similarity}\")\n",
    "        print(f\"LSA Similarity: {lsa_similarity}\")\n",
    "        print(f\"TTR for text 1: {ttr1}\")\n",
    "        print(f\"TTR for text 2: {ttr2}\")\n",
    "    else:\n",
    "        print(\"Error in calculate_similarity. Please check the error message above.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: 6\n",
      "Shape of similarity_scores: 6\n",
      "Mean Squared Error: 0.1184\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tree import Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_function_words(text):\n",
    "    # Define a set of common English function words\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    # Tokenize and lowercase the words in the text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out function words\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0  # To handle the case when both graphs are empty\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Count the number of unique words (types) and total number of words (tokens)\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    # Calculate Type-Token Ratio (TTR)\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0  # Handle the case when the text is empty\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    try:\n",
    "        # Tokenize sentences\n",
    "        sentences1 = sent_tokenize(text1)\n",
    "        sentences2 = sent_tokenize(text2)\n",
    "       \n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words1 = [pos_tag(word_tokenize(sentence)) for sentence in sentences1]\n",
    "        words2 = [pos_tag(word_tokenize(sentence)) for sentence in sentences2]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled1 = set([spell.correction(word) for sentence in words1 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "        misspelled2 = set([spell.correction(word) for sentence in words2 for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # Calculate similarity based on sentence structure, spelling, and punctuation frequency\n",
    "        structure_similarity = len(sentences1) / len(sentences2)\n",
    "        spelling_similarity = len(misspelled1.intersection(misspelled2)) / len(misspelled1.union(misspelled2))\n",
    "\n",
    "        # POS Tagging Similarity\n",
    "        pos_tags_similarity = pos_tag_similarity(words1, words2)\n",
    "\n",
    "        # Sentence Length Similarity\n",
    "        length_similarity = sentence_length_similarity(sentences1, sentences2)\n",
    "\n",
    "        # Punctuation Frequency Similarity\n",
    "        punctuation_sim = punctuation_similarity(text1, text2)\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # adjust the n-gram size\n",
    "        graph1 = generate_ngram_transition_graph(text1, n_value)\n",
    "        graph2 = generate_ngram_transition_graph(text2, n_value)\n",
    "\n",
    "        # Compute Jaccard similarity between the n-gram word transition graphs\n",
    "        ngram_similarity = compute_jaccard_similarity(graph1, graph2)\n",
    "        \n",
    "        # Extract function words\n",
    "        function_words1 = get_function_words(text1)\n",
    "        function_words2 = get_function_words(text2)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_word_count_similarity = len(set(function_words1).intersection(function_words2)) / len(set(function_words1).union(function_words2))\n",
    "\n",
    "        # Apply Latent Semantic Analysis (LSA)\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # LSA Similarity\n",
    "        lsa_similarity = 1 - spatial.distance.cosine(lsa_matrix[0], lsa_matrix[1])\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr1 = calculate_ttr(text1)\n",
    "        ttr2 = calculate_ttr(text2)\n",
    "    \n",
    "        return structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity, ngram_similarity, lsa_similarity, ttr1, ttr2\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_similarity: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def sentence_length_similarity(sentences1, sentences2):\n",
    "    avg_len1 = sum(len(sentence) for sentence in sentences1) / len(sentences1)\n",
    "    avg_len2 = sum(len(sentence) for sentence in sentences2) / len(sentences2)\n",
    "\n",
    "    return min(avg_len1, avg_len2) / max(avg_len1, avg_len2)\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation1 = [char for char in text1 if char in string.punctuation]\n",
    "    punctuation2 = [char for char in text2 if char in string.punctuation]\n",
    "\n",
    "    common_punctuations = set(punctuation1).intersection(punctuation2)\n",
    "    total_punctuations = set(punctuation1).union(punctuation2)\n",
    "\n",
    "    return len(common_punctuations) / len(total_punctuations)\n",
    "\n",
    "def train_random_forest_model(X_train, y_train):\n",
    "    # Instantiate Random Forest Regressor\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)  # Adjust hyperparameters as needed\n",
    "\n",
    "    # Train the model\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    return rf_regressor\n",
    "\n",
    "def main():\n",
    "    # Example data (replace with your actual data)\n",
    "    texts = [\n",
    "    \"This is a sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\",\n",
    "    \"This is another sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\",\n",
    "    \"Adding another text sample here for testing purposes.\",\n",
    "    \"One more text sample to increase the dataset size.\"\n",
    "    ]\n",
    "\n",
    "    X = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Calculate similarity features for each pair of texts\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            result = calculate_similarity(texts[i], texts[j])\n",
    "            if result is not None:\n",
    "                X.append(result)\n",
    "                # Assuming the first element of the result tuple is the similarity score\n",
    "                similarity_scores.append(result[0])\n",
    "    print(\"Shape of X:\", len(X))\n",
    "    print(\"Shape of similarity_scores:\", len(similarity_scores))\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, similarity_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = train_random_forest_model(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Text 1 author prediction: Not AuthorA\n",
      "Text 2 author prediction: Not AuthorA\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tree import Tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_function_words(text):\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def calculate_similarity(text1, text2, author1):\n",
    "    try:\n",
    "        sentences1 = sent_tokenize(text1)\n",
    "        sentences2 = sent_tokenize(text2)\n",
    "\n",
    "        structure_similarity = len(sentences1) / len(sentences2)\n",
    "\n",
    "        spell = SpellChecker()\n",
    "        misspelled1 = set([spell.correction(word) for sentence in word_tokenize(text1) for (word, tag) in pos_tag(word_tokenize(sentence)) if tag.startswith('N') or tag.startswith('V')])\n",
    "        misspelled2 = set([spell.correction(word) for sentence in word_tokenize(text2) for (word, tag) in pos_tag(word_tokenize(sentence)) if tag.startswith('N') or tag.startswith('V')])\n",
    "        spelling_similarity = len(misspelled1.intersection(misspelled2)) / len(misspelled1.union(misspelled2))\n",
    "\n",
    "        pos_tags_similarity = pos_tag_similarity([pos_tag(word_tokenize(sentence)) for sentence in sentences1], [pos_tag(word_tokenize(sentence)) for sentence in sentences2])\n",
    "        length_similarity = sentence_length_similarity(sentences1, sentences2)\n",
    "        punctuation_sim = punctuation_similarity(text1, text2)\n",
    "\n",
    "        n_value = 2\n",
    "        graph1 = generate_ngram_transition_graph(text1, n_value)\n",
    "        graph2 = generate_ngram_transition_graph(text2, n_value)\n",
    "        ngram_similarity = compute_jaccard_similarity(graph1, graph2)\n",
    "\n",
    "        function_words1 = get_function_words(text1)\n",
    "        function_words2 = get_function_words(text2)\n",
    "        function_word_count_similarity = len(set(function_words1).intersection(function_words2)) / len(set(function_words1).union(function_words2))\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "        svd = TruncatedSVD(n_components=2)\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "        lsa_similarity = 1 - spatial.distance.cosine(lsa_matrix[0], lsa_matrix[1])\n",
    "\n",
    "        ttr1 = calculate_ttr(text1)\n",
    "        ttr2 = calculate_ttr(text2)\n",
    "        \n",
    "        # Determine if the text belongs to Author A (1) or not (0)\n",
    "        if author1 in text1:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        return [structure_similarity, spelling_similarity, pos_tags_similarity, length_similarity, punctuation_sim, function_word_count_similarity, ngram_similarity, lsa_similarity, ttr1, ttr2], label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_similarity: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def sentence_length_similarity(sentences1, sentences2):\n",
    "    avg_len1 = sum(len(sentence) for sentence in sentences1) / len(sentences1)\n",
    "    avg_len2 = sum(len(sentence) for sentence in sentences2) / len(sentences2)\n",
    "\n",
    "    return min(avg_len1, avg_len2) / max(avg_len1, avg_len2)\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation1 = [char for char in text1 if char in string.punctuation]\n",
    "    punctuation2 = [char for char in text2 if char in string.punctuation]\n",
    "\n",
    "    common_punctuations = set(punctuation1).intersection(punctuation2)\n",
    "    total_punctuations = set(punctuation1).union(punctuation2)\n",
    "\n",
    "    return len(common_punctuations) / len(total_punctuations)\n",
    "\n",
    "def train_random_forest_model(X_train, y_train):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    return rf_classifier\n",
    "\n",
    "def main():\n",
    "    texts = [\n",
    "    \"This is a sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\",\n",
    "    \"This is another sample text. It checks for similarity based on sentence structure, spelling, and punctuation.\",\n",
    "    \"Adding another text sample here for testing purposes.\",\n",
    "    \"One more text sample to increase the dataset size.\"\n",
    "    ]\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    author1 = \"AuthorA\"\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            result, label = calculate_similarity(texts[i], texts[j], author1)\n",
    "            if result is not None:\n",
    "                X.append(result)\n",
    "                y.append(label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_model = train_random_forest_model(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        print(f\"Text {i+1} author prediction:\", \"AuthorA\" if pred == 1 else \"Not AuthorA\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 191: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m author_a_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m not_author_a_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot Author A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 128\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_a_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_author_a_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 99\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(author_a_folder, not_author_a_folder)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(author_a_folder, not_author_a_folder):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_a_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_author_a_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Extract features from the texts\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     X_features \u001b[38;5;241m=\u001b[39m [extract_features(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m X]\n",
      "Cell \u001b[1;32mIn[11], line 82\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(author_a_folder, not_author_a_folder)\u001b[0m\n\u001b[0;32m     80\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(author_a_folder, file_name)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 82\u001b[0m         author_a_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(not_author_a_folder):\n\u001b[0;32m     85\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(not_author_a_folder, file_name)\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 191: invalid start byte"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_features(text):\n",
    "    try:\n",
    "        # Skip empty texts\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "       \n",
    "        # Initialize structure similarity to a default value\n",
    "        structure_similarity = 0.0\n",
    "        \n",
    "        if sentences:  # Check if sentences list is not empty\n",
    "            # Calculate similarity based on sentence structure\n",
    "            structure_similarity = len(sentences) / len(sentences)\n",
    "\n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled = set([spell.correction(word) for sentence in words for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # POS Tagging Similarity\n",
    "        pos_tags_similarity = pos_tag_similarity(words, words)\n",
    "\n",
    "        # Sentence Length Similarity\n",
    "        length_similarity = 1.0\n",
    "\n",
    "        # Punctuation Frequency Similarity\n",
    "        punctuation_sim = 1.0\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph = generate_ngram_transition_graph(text, n_value)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_words = get_function_words(text)\n",
    "\n",
    "        # Apply Latent Semantic Analysis (LSA)\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=2)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr = calculate_ttr(text)\n",
    "    \n",
    "        return [structure_similarity, pos_tags_similarity, length_similarity, punctuation_sim, len(misspelled), len(set(function_words)), len(graph.nodes), ttr]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_features: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "    \n",
    "# Function to load and preprocess data\n",
    "def load_data(author_a_folder, not_author_a_folder):\n",
    "    author_a_texts = []\n",
    "    not_author_a_texts = []\n",
    "    \n",
    "    for file_name in os.listdir(author_a_folder):\n",
    "        file_path = os.path.join(author_a_folder, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            author_a_texts.append(f.read())\n",
    "\n",
    "    for file_name in os.listdir(not_author_a_folder):\n",
    "        file_path = os.path.join(not_author_a_folder, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            not_author_a_texts.append(f.read())\n",
    "\n",
    "    # Label author_a texts as 1 and not_author_a texts as 0\n",
    "    X = author_a_texts + not_author_a_texts\n",
    "    y = [1] * len(author_a_texts) + [0] * len(not_author_a_texts)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def main(author_a_folder, not_author_a_folder):\n",
    "    # Load and preprocess data\n",
    "    X, y = load_data(author_a_folder, not_author_a_folder)\n",
    "\n",
    "    # Extract features from the texts\n",
    "    X_features = [extract_features(text) for text in X]\n",
    "\n",
    "    # Remove None values\n",
    "    X_features = [x for x in X_features if x is not None]\n",
    "\n",
    "    # Convert feature list to numpy array\n",
    "    X_features = np.array(X_features)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a random forest classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    author_a_folder = \"Author A\"\n",
    "    not_author_a_folder = \"Not Author A\"\n",
    "    main(author_a_folder, not_author_a_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from nltk.tree import Tree\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def calculate_sentence_length_similarity(sentences):\n",
    "    if len(sentences) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_length = sum(len(sentence) for sentence in sentences)\n",
    "    average_length = total_length / len(sentences)\n",
    "\n",
    "    max_length = max(len(sentence) for sentence in sentences)\n",
    "    min_length = min(len(sentence) for sentence in sentences)\n",
    "\n",
    "    if max_length == min_length:\n",
    "        return 0.0\n",
    "\n",
    "    normalized_average_length = (average_length - min_length) / (max_length - min_length)\n",
    "\n",
    "    return normalized_average_length\n",
    "\n",
    "def calculate_punctuation_similarity(text):\n",
    "    # Define the set of punctuation marks\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "\n",
    "    # Count punctuation marks in the text\n",
    "    punctuation_counts = {punctuation_mark: text.count(punctuation_mark) for punctuation_mark in punctuation_marks}\n",
    "\n",
    "    # Compute punctuation similarity as the sum of squared differences between punctuation frequencies\n",
    "    total_marks = sum(punctuation_counts.values())\n",
    "    punctuation_frequencies = {mark: count / total_marks for mark, count in punctuation_counts.items()}\n",
    "\n",
    "    # Compute similarity using squared Euclidean distance between punctuation distributions\n",
    "    punctuation_sim = 0.0\n",
    "    for mark in punctuation_marks:\n",
    "        punctuation_sim += (punctuation_frequencies.get(mark, 0.0) - 1.0 / len(punctuation_marks)) ** 2\n",
    "\n",
    "    punctuation_sim = 1.0 - punctuation_sim  # Normalize to [0, 1]\n",
    "    \n",
    "    return punctuation_sim\n",
    "\n",
    "def extract_features(text):\n",
    "    try:\n",
    "        # Skip empty texts\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "       \n",
    "        # Initialize structure similarity to a default value\n",
    "        structure_similarity = 0.0\n",
    "        \n",
    "        if sentences:  # Check if sentences list is not empty\n",
    "            # Calculate similarity based on sentence structure\n",
    "            structure_similarity = len(sentences) / len(sentences)\n",
    "\n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled = set([spell.correction(word) for sentence in words for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # POS Tagging Similarity\n",
    "        pos_tags_similarity = pos_tag_similarity(words, words)\n",
    "\n",
    "        # Calculate punctuation similarity\n",
    "        punctuation_sim = calculate_punctuation_similarity(text)\n",
    "\n",
    "        # Calculate sentence length similarity\n",
    "        length_similarity = calculate_sentence_length_similarity(sentences)\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph = generate_ngram_transition_graph(text, n_value)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_words = get_function_words(text)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=4)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr = calculate_ttr(text)\n",
    "\n",
    "        return [structure_similarity, pos_tags_similarity, length_similarity, punctuation_sim, len(misspelled), len(set(function_words)), len(graph.nodes), ttr] + list(lsa_matrix.flatten())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_features: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "    \n",
    "# Function to load and preprocess data\n",
    "def load_data(author_a_texts, not_author_a_texts):\n",
    "    # Label author_a texts as 1 and not_author_a texts as 0\n",
    "    X = author_a_texts + not_author_a_texts\n",
    "    y = [1] * len(author_a_texts) + [0] * len(not_author_a_texts)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def main(author_a_texts, not_author_a_texts):\n",
    "    # Load and preprocess data\n",
    "    X, y = load_data(author_a_texts, not_author_a_texts)\n",
    "\n",
    "    # Extract features from the texts\n",
    "    X_features = [extract_features(text) for text in X]\n",
    "\n",
    "    # Remove None values\n",
    "    X_features = [x for x in X_features if x is not None]\n",
    "\n",
    "    # Convert feature list to numpy array\n",
    "    X_features = np.array(X_features)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=40)\n",
    "\n",
    "    # Train a random forest classifier (new stuff)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=40)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    author_a_texts = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "    \"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "    \"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "    \"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "    \"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "    \"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "    \"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "    \"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "    \"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "    \"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "    \"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "    \"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "    \"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "    \"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "    \"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "    \"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "    not_author_a_texts = [\"\"\"\t\t\t\t\t\t\n",
    "        As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "    \"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\n",
    "    \"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "    I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "    ]  # Provide a list of texts not from author A\n",
    "    main(author_a_texts, not_author_a_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7\n",
      "Total words: 127\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'tile', 'fired', 'float', 'guess', 'mural', 'was', 'make', 'dig', 'know', 'went', 'Live', 'learn', 'thinking', 'moved', 'picked', 'fear', 'everything', 'kiln', 'blob', 'do', 'glaze', 'smearing', 'be', 'have', 'spots', 'underglaze', 'decided', 'week', 'worked'}\n",
      "Punctuation similarity: 0.765625\n",
      "Length similarity: 0.5338983050847458\n",
      "Active voice count: 0.5714285714285714\n",
      "Passive voice count: 0.42857142857142855\n",
      "Grammar errors count: 29\n",
      "Upper case count: 11\n",
      "Lower case count: 415\n",
      "Number of function words: 31\n",
      "Number of nodes in graph: 78\n",
      "Phrase patterns: [('actually', 'picked'), ('along', 'with'), ('be', 'thinking'), ('big', 'blob'), ('dig', 'deep'), ('do', \"n't\"), ('empty', '!'), ('full', 'instead'), ('just', 'do'), ('know', 'if'), ('make', 'another'), (\"n't\", 'know'), ('ok', 'except'), ('on', 'went'), ('picked', 'up'), ('several', 'spots'), ('should', 'be'), ('smearing', 'all'), ('then', 'fired'), ('too', 'thick'), ('week', 'on'), ('went', 'into'), ('was', 'too'), (\"'ll\", 'have'), ('4', 'tile'), ('a', 'big'), ('a', 'week'), ('another', 'one'), ('as', 'half'), ('deep', 'for'), ('everything', 'else'), ('for', 'fear'), ('for', 'this'), ('half', 'empty'), ('half', 'full'), ('it', 'over'), ('kiln', 'as'), ('moved', 'it'), ('thankfully', 'everything'), ('thick', '('), ('this', 'one'), ('with', 'everything'), ('worked', 'a'), (',', 'and'), (')', 'The'), ('The', '4'), ('The', 'underglaze'), ('blob', 'in'), ('decided', 'to'), ('fear', 'of'), ('fired', 'in'), ('float', 'glaze'), ('glaze', 'actually'), ('in', 'me'), ('in', 'several'), ('instead', 'of'), ('me', 'to'), ('of', 'smearing'), ('thinking', 'of'), ('to', 'dig'), ('to', 'float'), ('to', 'make'), ('underglaze', 'was'), ('was', 'ok'), ('one', '.'), ('the', 'black'), ('the', 'kiln'), ('Live', 'and'), ('and', 'learn'), ('and', 'thankfully'), ('and', 'then'), ('have', 'it'), ('tile', 'for'), ('tile', 'mural'), (',', 'along'), (',', 'moved'), ('else', ','), ('over', ','), ('.', 'I'), ('.', ')'), ('.', 'Live'), ('learn', '.'), ('spots', '.'), ('everything', 'was'), ('have', 'to'), ('in', 'a'), ('it', 'in'), ('of', 'half'), ('(', 'I'), ('I', \"'ll\"), ('I', 'decided'), ('I', 'guess'), ('I', 'just'), ('I', 'should'), ('I', 'worked'), ('all', 'the'), ('except', 'the'), ('guess', 'I'), ('if', 'I'), ('into', 'the'), ('up', 'the'), ('The', 'glaze'), ('glaze', 'was'), ('black', ','), ('kiln', ','), ('thick', ','), ('black', '.'), ('mural', '.'), ('I', 'have'), ('mural', 'I'), ('the', 'mural'), ('the', 'tile'), ('.', 'The'), ('glaze', 'the'), ('of', 'the'), ('the', 'glaze'), ('and', 'the')]\n",
      "TF-IDF matrix shape: (1, 35)\n",
      "LSA matrix shape: (1, 1)\n",
      "Number of sentences: 3\n",
      "Total words: 75\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'were', 'kept', 'today', 'firing', 'was', 'is', 'worrying', 'anything', 'degrees', 'going', 'something', 'monoxide', 'banging', 'exploding', 'day', 'sleep', 'hearing', 'fierce', 'die', 'smelled', 'worried', 'hope', 'found', 'kiln', 'poisoning', 'have', 'carbon', 'night'}\n",
      "Punctuation similarity: 0.6924070247933884\n",
      "Length similarity: 0.4935064935064935\n",
      "Active voice count: 0.0\n",
      "Passive voice count: 1.0\n",
      "Grammar errors count: 29\n",
      "Upper case count: 8\n",
      "Lower case count: 262\n",
      "Number of function words: 17\n",
      "Number of nodes in graph: 56\n",
      "Phrase patterns: [(\"'m\", 'so'), (\"'s\", 'still'), ('600', 'degrees'), ('It', 'smelled'), ('anything', 'exploding'), ('carbon', 'monoxide'), ('die', 'of'), ('found', 'out'), ('going', 'to'), ('hearing', 'banging'), ('it', \"'s\"), ('kept', 'hearing'), ('monoxide', 'poisoning'), ('next', 'day'), ('night', 'worrying'), ('of', 'carbon'), ('our', 'sleep'), ('out', 'yet'), ('smelled', 'something'), ('so', 'tired'), ('something', 'fierce'), ('still', '600'), ('tired', 'today'), ('to', 'die'), ('we', 'were'), ('worried', 'we'), ('worrying', 'about'), ('the', 'kiln'), ('all', 'going'), ('all', 'night'), ('because', 'it'), ('exploding', 'in'), ('have', \"n't\"), ('in', 'our'), ('kiln', 'firing'), (\"n't\", 'anything'), (\"n't\", 'found'), ('poisoning', 'in'), ('today', 'because'), ('up', 'all'), ('were', 'all'), ('yet', 'because'), ('.', 'It'), ('.', 'Plus'), ('about', 'the'), ('day', '.'), ('degrees', 'the'), ('firing', '.'), ('hope', 'was'), ('sleep', '.'), ('the', 'next'), ('was', 'up'), ('was', 'worried'), (',', 'and'), (',', 'but'), (',', 'which'), ('Plus', ','), ('banging', ','), ('fierce', ','), ('I', 'was'), ('I', \"'m\"), ('I', 'have'), ('I', 'hope'), ('I', 'kept'), ('and', 'I'), ('but', 'I'), ('which', 'I'), ('in', 'the'), ('was', \"n't\"), ('kiln', ','), ('because', 'I')]\n",
      "TF-IDF matrix shape: (1, 26)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6\n",
      "Total words: 82\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'mood', 'set', 'mural', 'pas', 'obsessed', 'was', 'shocked', 'had', 'day', 'lady', 'look', 'see', 'looked', 'check', 'returned', 'apologize', 'night', 'tiles', 'called', 'nightmares', 'surprise', 'warped'}\n",
      "Punctuation similarity: 0.376929012345679\n",
      "Length similarity: 0.6439393939393939\n",
      "Active voice count: 0.16666666666666666\n",
      "Passive voice count: 0.8333333333333334\n",
      "Grammar errors count: 22\n",
      "Upper case count: 7\n",
      "Lower case count: 280\n",
      "Number of function words: 20\n",
      "Number of nodes in graph: 54\n",
      "Phrase patterns: [('Their', 'check'), ('To', 'my'), ('a', 'bad'), ('and', 'obsessed'), ('apologize', 'because'), ('bad', 'mood'), ('had', 'nightmares'), ('in', 'a'), ('look', 'when'), ('me', 'back'), ('mural', 'that'), ('my', 'suprise'), ('night', 'and'), ('shocked', 'at'), ('so', 'upset'), ('tile', 'mural'), ('tiles', 'look'), ('very', 'shocked'), (',', 'she'), ('next', 'day'), ('Luckily', ','), ('all', 'night'), ('at', 'how'), ('called', 'me'), ('great', 'it'), ('happily', 'called'), ('how', 'great'), ('how', 'warped'), ('it', 'looked'), ('just', 'set'), ('lady', 'about'), ('nightmares', 'all'), ('obsessed', 'about'), ('see', 'how'), ('suprise', ','), ('to', 'apologize'), ('to', 'see'), ('when', 'set'), ('the', 'next'), ('I', 'had'), ('I', 'happily'), ('because', 'she'), ('upset', 'I'), ('she', 'was'), ('about', 'it'), ('it', 'all'), ('set', 'to'), ('day', '.'), ('back', 'the'), ('check', 'was'), ('returned', 'the'), ('that', 'was'), ('the', 'lady'), ('the', 'tile'), ('the', 'tiles'), ('warped', 'the'), ('was', 'in'), ('was', 'just'), ('was', 'returned'), ('was', 'so'), ('was', 'very'), ('.', 'Luckily'), ('.', 'P.s'), ('.', 'Their'), ('.', 'To'), ('P.s', '.'), ('looked', '.'), ('mood', '.'), ('all', 'day'), ('day', 'to'), ('she', 'called'), ('about', 'the'), ('called', 'the'), ('set', '.'), ('I', 'was'), ('.', 'I')]\n",
      "TF-IDF matrix shape: (1, 25)\n",
      "LSA matrix shape: (1, 1)\n",
      "Number of sentences: 4\n",
      "Total words: 39\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'Note', 'get', 'customs', 'today', 'tiles', 'supplier', 'years', 'has', 'weeks', 'Mexico', 'self', 'order', 'Murphy', 'assume', 'anything', 'Today', 'Remember', 'delayed'}\n",
      "Punctuation similarity: 0.35124999999999995\n",
      "Length similarity: 0.3922413793103448\n",
      "Active voice count: 1.0\n",
      "Passive voice count: 0.0\n",
      "Grammar errors count: 18\n",
      "Upper case count: 8\n",
      "Lower case count: 130\n",
      "Number of function words: 3\n",
      "Number of nodes in graph: 34\n",
      "Phrase patterns: [('100', 'bisque'), ('4', 'weeks'), ('7', 'years'), (':', 'Remember'), ('I', 'must'), ('My', 'supplier'), ('Never', 'assume'), ('Note', 'to'), ('Remember', 'Murphy'), ('Today', 'I'), ('ago', 'delayed'), ('assume', 'anything'), ('bisque', 'white'), ('get', '100'), ('has', 'my'), ('must', 'get'), ('my', 'order'), ('self', ':'), ('tiles', 'today'), ('to', 'self'), ('weeks', 'ago'), ('white', 'tiles'), ('years', 'has'), ('Mexico', 'in'), ('delayed', 'in'), ('in', 'Mexico'), ('in', 'customs'), ('of', '4'), ('of', '7'), ('order', 'of'), ('supplier', 'of'), ('.', 'My'), ('.', 'Never'), ('.', 'Note'), ('Murphy', '.'), ('anything', '.'), ('customs', '.'), ('today', '.')]\n",
      "TF-IDF matrix shape: (1, 18)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 5\n",
      "Total words: 67\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'get', 'is', 'Yesterday', 'MUST', 'test', None, 'cure', 'see', 'PAY', 'TODAY', 'cone', 'response', 'deadline', 'SALES', 'ad', 'butt', 'glazes', 'be', 'heck', 'ELSE', 'TAXES', 'placed', 'week', 'Cost', 'paper', 'OR', 'Something'}\n",
      "Punctuation similarity: 0.71875\n",
      "Length similarity: 0.49803921568627446\n",
      "Active voice count: 0.6\n",
      "Passive voice count: 0.4\n",
      "Grammar errors count: 27\n",
      "Upper case count: 38\n",
      "Lower case count: 200\n",
      "Number of function words: 12\n",
      "Number of nodes in graph: 53\n",
      "Phrase patterns: [('!', 'Something'), (\"'s\", 'butt'), (',', 'and'), ('5', 'glazes'), ('ELSE', '!'), ('MUST', 'PAY'), ('OR', 'ELSE'), ('PAY', 'SALES'), ('SALES', 'TAXES'), ('Something', 'about'), ('TAXES', 'TODAY'), ('TODAY', 'OR'), ('This', 'week'), ('VERY', 'interesting'), ('Yesterday', 'placed'), ('ad', 'in'), ('be', 'VERY'), ('cone', '5'), ('deadline', 'is'), ('glazes', 'just'), ('heck', 'of'), ('local', 'paper'), ('of', 'it'), ('some', 'cone'), ('sure', 'cure'), ('test', 'some'), ('week', 'I'), ('I', 'should'), ('cure', 'to'), ('get', 'one'), ('interesting', 'to'), ('off', 'one'), ('one', \"'s\"), ('one', 'off'), ('should', 'be'), ('should', 'test'), ('stuff-This', 'should'), ('to', 'get'), ('to', 'see'), ('for', 'free'), ('Cost', 'for'), ('a', 'deadline'), ('a', 'sure'), ('about', 'a'), ('and', 'for'), ('free', ','), ('free', 'ad'), ('free', 'stuff-This'), ('in', 'the'), ('is', 'a'), ('just', 'for'), ('placed', 'a'), ('see', 'the'), ('the', 'heck'), ('the', 'local'), ('the', 'response'), ('.', 'Cost'), ('.', 'This'), ('.', 'Yesterday'), ('butt', '.'), ('it', '.'), ('paper', '.'), ('response', '.'), ('a', 'free'), ('for', 'the')]\n",
      "TF-IDF matrix shape: (1, 24)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 10\n",
      "Total words: 156\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'fired', 'orders', 'mural', 'plan', 'couple', 'was', 'is', 'customer', 'getting', 'test', 'telling', 'day', 'work', 'buying', 'note', 'nothing', 'regretting', 'kind', 'months', 'did', 'cone', 'wedding', 'planning', 'been', 'summer', 'opportunity', 'kiln', 'do', 'lost', 'be', 'have', 'camp', 'fail', 'tiles', 'times', 'one', 'fall', 'year', 'calls', 'phone'}\n",
      "Punctuation similarity: 0.8055555555555556\n",
      "Length similarity: 0.35530303030303034\n",
      "Active voice count: 0.6\n",
      "Passive voice count: 0.4\n",
      "Grammar errors count: 41\n",
      "Upper case count: 15\n",
      "Lower case count: 485\n",
      "Number of function words: 38\n",
      "Number of nodes in graph: 98\n",
      "Phrase patterns: [('!', \"''\"), (\"''\", 'On'), (\"'s\", 'kind'), ('(', 'work'), ('Not', 'that'), ('On', 'another'), ('``', 'If'), ('all', 'year'), ('am', 'regretting'), ('an', 'opportunity'), ('another', 'note'), ('been', 'better'), ('better', 'prepared'), ('cone', '10'), ('day', 'after'), ('did', \"n't\"), ('fall', 'into'), ('into', ';'), ('it', \"'s\"), ('just', 'no'), ('last', 'mural'), ('like', 'planning'), ('me', 'what'), ('months', 'ago'), ('not', 'buying'), ('opportunity', 'because'), ('orders', ')'), ('phone', 'calls'), ('prepared', 'with'), ('regretting', 'not'), ('summer', 'camp'), ('telling', 'me'), ('three', 'times'), ('tiles', 'in'), ('too', 'sick'), ('was', 'too'), ('wedding', 'all'), ('will', 'be'), ('work', 'orders'), (\"'m\", 'certainly'), (\"'m\", 'depressed'), ('and', 'then'), ('be', 'getting'), ('certainly', 'getting'), ('couple', 'of'), ('fail', '!'), ('getting', 'phone'), ('kind', 'of'), ('no', 'one'), ('now', 'three'), ('of', 'like'), ('of', 'months'), ('one', 'soon'), ('one', 'telling'), ('small', 'test'), ('smaller', 'kiln'), ('test', 'tiles'), ('year', 'and'), ('to', 'do'), ('If', 'you'), ('after', 'you'), ('buying', 'the'), ('customer', 'or'), ('fired', 'the'), ('in', 'or'), ('or', 'an'), ('or', 'cone'), ('or', 'whatever'), ('the', 'day'), ('the', 'last'), ('the', 'small'), ('then', 'the'), ('have', 'nothing'), ('and', 'now'), ('getting', 'one'), ('test', 'kiln'), ('nothing', 'to'), (',', 'just'), ('10', ','), ('Twice', ','), ('a', 'couple'), ('a', 'customer'), ('a', 'smaller'), ('a', 'summer'), ('a', 'wedding'), ('lost', 'a'), ('note', ','), ('planning', 'a'), ('with', 'a'), ('I', \"'m\"), ('could', 'have'), ('have', 'been'), ('have', 'lost'), (\"n't\", 'have'), ('you', 'fail'), ('to', 'plan'), ('I', 'have'), ('camp', 'to'), ('sick', 'to'), ('to', 'fall'), ('do', 'test'), ('.', '('), ('.', 'But'), ('.', 'Not'), ('.', 'Twice'), ('.', '``'), ('.', 'it'), ('ago', '.'), ('calls', '.'), ('depressed', '.'), ('mural', '.'), ('soon', '.'), ('whatever', '.'), ('you', 'plan'), ('do', '.'), (')', 'I'), (';', 'I'), ('But', 'I'), ('I', 'am'), ('I', 'could'), ('I', 'did'), ('I', 'was'), ('I', 'will'), ('because', 'I'), ('that', 'I'), ('times', 'I'), ('what', 'I'), (',', 'and'), (',', 'or'), (',', 'you'), ('plan', ','), ('plan', 'a'), ('fail', 'to'), ('kiln', 'to'), ('to', 'fail'), ('.', 'now'), ('kiln', '.'), ('you', 'have'), ('now', 'I'), ('do', ','), ('plan', 'to'), ('have', 'a')]\n",
      "TF-IDF matrix shape: (1, 45)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1\n",
      "Total words: 84\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'stuff', 'ornament', 'make', 'hump', 'fix', 'vase', 'molds', None, 'babies', 'Smart', 'stars', 'schedule', 'footprints', 'containers', 'flyers', 'train', 'scout', 'certificates', 'footprint', 'Final', 'women', 'marketing', 'star', 'mix', 'summer', 'colored', 'tools', 'plaque', 'letters', 'glazes', 'maps', 'lie', 'shapes', 'baby', 'w', 'camp', 'dipping', 'tiles', 'gift', 'peacock', 'needed', 'garden', 'send', 'party', 'postcards'}\n",
      "Punctuation similarity: 0.7118055555555556\n",
      "Length similarity: 0.0\n",
      "Active voice count: 1.0\n",
      "Passive voice count: 0.0\n",
      "Grammar errors count: 45\n",
      "Upper case count: 3\n",
      "Lower case count: 405\n",
      "Number of function words: 4\n",
      "Number of nodes in graph: 68\n",
      "Phrase patterns: [('(', 'also'), (')', 'mix'), ('Final', 'marketing'), ('Liz', ')'), ('Smart', 'and'), ('also', 'to'), ('and', 'Final'), ('babyfootprint', 'gift'), ('camp', 'schedule'), ('colored', 'dipping'), ('containers', 'from'), ('daniel-frame', 'w'), ('dipping', 'glazes'), ('fix', 'website'), ('footprint', 'postcards'), ('footprints', '('), ('from', 'Smart'), ('gift', 'certificates'), ('glazes', 'in'), ('hump', 'molds'), ('in', 'quart'), ('large', 'star'), ('marketing', 'tools'), ('mix', 'colored'), ('multi-level', 'vase'), ('new', 'maps'), ('peacock', 'tray'), ('postcards', 'fix'), ('quart', 'containers'), ('slab', 'shapes'), ('small', 'stars'), ('star', 'windchimes'), ('summer', 'camp'), ('to', 'Liz'), ('tools', 'needed'), ('train', 'plaque'), ('vase', 'garden'), ('w', 'letters'), ('website', 'babyfootprint'), (':', 'babies'), (':', 'scout'), ('baby', 'footprint'), ('baby', 'stuff'), ('for', 'daniel-frame'), ('for', 'footprints'), ('maps', 'or'), ('needed', ':'), ('or', 'general'), ('or', 'initial'), ('shapes', ':'), ('stuff', 'for'), ('tray', 'baby'), ('garden', 'tiles'), ('initial', 'tiles'), ('make', 'hump'), ('make', 'multi-level'), ('make', 'slab'), ('molds', 'make'), ('send', 'tiles'), ('stars', 'make'), ('tiles', 'peacock'), ('flyers', 'new'), ('flyers', 'party'), ('flyers', 'summer'), ('general', 'flyers'), ('party', 'flyers'), ('schedule', 'flyers'), ('scout', 'flyers'), (',', 'large'), (',', 'ornament'), (',', 'send'), (',', 'small'), (',', 'train'), (',', 'women'), ('babies', ','), ('letters', ','), ('ornament', ','), ('plaque', ','), ('windchimes', ','), ('women', ','), ('tiles', 'for'), ('tiles', 'or'), ('flyers', 'baby')]\n",
      "TF-IDF matrix shape: (1, 58)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 13\n",
      "Total words: 178\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'anticipated', 'batch', 'go', 'doing', 'teach', 'mural', 'class', 'business', 'mixing', 'warping', 'is', 'wants', 'Thought', 'getting', 'going', 'had', 'broken', 'are', 'love', 'plaster', 'handmade', 'versus', 'favorites', 'look', 'molds', 'see', 'tilting', 'crafting', 'fun', 'pour', 'break', 'learn', 'Cordillera', 'children', 'clay', 'layers', 'pouring', 'chance', 'fact', 'kiln', 'colored', 'do', 'carving', 'trepidation', 'cracked', 'be', 'discovered', 'have', 'painting', 'week', 'tiles', 'Had', 'worked', 'Got', 'teacher', 'relationship', 'robins', 'student', 'gave', 'Hope'}\n",
      "Punctuation similarity: 0.6466346153846154\n",
      "Length similarity: 0.36898061288305195\n",
      "Active voice count: 0.6923076923076923\n",
      "Passive voice count: 0.3076923076923077\n",
      "Grammar errors count: 61\n",
      "Upper case count: 18\n",
      "Lower case count: 662\n",
      "Number of function words: 34\n",
      "Number of nodes in graph: 120\n",
      "Phrase patterns: [('75', 'more'), (';', 'one'), ('Double', 'stilting'), ('Hope', 'they'), ('She', 'however'), ('The', 'Cordillera'), ('They', 'look'), ('This', 'week'), ('after', 'mixing'), ('all', 're-painted'), ('are', 'doing'), ('as', 'such'), ('be', 'easier'), ('beautiful', 'going'), ('bigger', 'everyday'), ('break', 'open'), ('business', 'as'), ('children', ';'), ('clay', 'crafting'), ('could', 'see'), ('cracked', 'or'), ('do', 'love'), ('easier', 'if'), ('fact', 'that'), ('getting', 'bigger'), ('learn', 'clay'), ('less', 'warping'), ('little', 'trepidation'), ('look', 'beautiful'), ('love', 'pouring'), ('love-hate', 'relationship'), ('might', 'be'), ('now', '75'), ('one', 'of'), ('or', 'broken'), ('pouring', 'molds'), ('relationship', 'with'), ('second', 'student'), ('see', 'what'), ('self-portrait', 'class'), ('than', 'anticipated'), ('they', 'like'), ('three', 'after'), ('versus', 'painting'), ('week', 'had'), ('worked', 'out'), ('Cordillera', 'mural'), ('Robinson', 'mural'), ('Thought', 'it'), ('however', 'wants'), ('in', 'layers'), ('in', 'three'), ('is', 'fun'), ('is', 'getting'), ('it', 'might'), ('like', 'them'), ('mural', 'worked'), ('pour', 'it'), ('stilting', 'them'), ('this', 'is'), ('what', 'you'), ('who', 'wants'), ('you', 'are'), ('you', 'could'), ('wants', 'to'), ('4', 'tiles'), ('and', 'discovered'), ('and', 'pour'), ('batch', 'and'), ('class', 'for'), ('for', 'children'), ('for', 'handmade'), ('for', 'less'), ('gave', 'into'), ('go', 'into'), ('going', 'into'), ('had', 'my'), ('handmade', 'tiles'), ('into', 'business'), ('more', 'tiles'), ('my', 'favorites'), ('my', 'second'), ('of', 'my'), ('open', 'my'), ('plaster', 'carving'), ('so', 'this'), ('student', 'for'), ('tiles', 'cracked'), ('tiles', 'than'), ('tiles', 'who'), ('with', 'plaster'), ('into', 'the'), ('Got', 'to'), ('chance', 'to'), ('to', 'break'), ('to', 'go'), ('to', 'learn'), ('it', 'in'), ('mural', 'is'), ('After', 'a'), ('Had', 'a'), ('a', 'chance'), ('a', 'little'), ('a', 'love-hate'), ('a', 'self-portrait'), ('a', 'teacher'), ('am', 'a'), ('colored', 'the'), ('have', 'a'), ('mixing', 'the'), ('the', 'Robinson'), ('the', 'batch'), ('the', 'fact'), ('the', 'kiln'), ('I', 'am'), ('I', 'colored'), ('I', 'do'), ('I', 'gave'), ('I', 'have'), ('discovered', 'I'), ('if', 'I'), ('that', 'I'), ('plaster', 'in'), ('so', 'you'), ('them', 'for'), (',', 'and'), (',', 'so'), ('to', 'teach'), (',', 'all'), (',', 'now'), (',', 'versus'), ('broken', ','), ('crafting', ','), ('everyday', ','), ('layers', ','), ('painting', ','), ('teacher', ','), ('trepidation', ','), ('well', ','), ('and', 'so'), ('my', 'plaster'), ('teach', 'a'), ('I', 'teach'), ('.', '4'), ('.', 'After'), ('.', 'Double'), ('.', 'Got'), ('.', 'Had'), ('.', 'Hope'), ('.', 'She'), ('.', 'The'), ('.', 'They'), ('.', 'This'), ('.', 'Thought'), ('anticipated', '.'), ('carving', '.'), ('doing', '.'), ('favorites', '.'), ('fun', '.'), ('kiln', '.'), ('molds', '.'), ('out', '.'), ('re-painted', '.'), ('such', '.'), ('warping', '.'), ('the', 'plaster'), ('so', 'I'), ('teach', '.'), ('them', '.'), ('plaster', ',')]\n",
      "TF-IDF matrix shape: (1, 65)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7\n",
      "Total words: 94\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'hundred', 'get', 'want', 'use', 'bottles', 'fill', 'ones', 'Debating', 'reason', 'money', 'do', 'glazes', 'school', 'colors', 'have', 'painting', 'block', 'choice', 'spend'}\n",
      "Punctuation similarity: 0.703125\n",
      "Length similarity: 0.5925925925925926\n",
      "Active voice count: 1.0\n",
      "Passive voice count: 0.0\n",
      "Grammar errors count: 19\n",
      "Upper case count: 11\n",
      "Lower case count: 306\n",
      "Number of function words: 18\n",
      "Number of nodes in graph: 54\n",
      "Phrase patterns: [('Debating', 'which'), ('For', 'some'), ('block', 'about'), ('but', 'not'), ('different', 'colors'), ('enough', 'of'), ('fill', '12'), ('hundred', 'different'), ('mental', 'block'), ('more', 'money'), ('nice', 'expensive'), ('no', 'choice'), ('not', 'enough'), ('school', 'painting'), ('some', 'reason'), ('total', 'mental'), ('yucky', 'cheap'), ('have', 'a'), ('do', \"n't\"), ('I', 'have'), ('12', 'bottles'), ('any', 'more'), ('any', 'one'), ('just', 'do'), ('money', 'on'), ('of', 'any'), ('on', 'Monday'), ('on', 'half-used'), ('painting', 'on'), ('spend', 'any'), ('to', 'use'), (\"n't\", 'want'), ('Could', \"n't\"), ('a', 'hundred'), ('a', 'school'), ('a', 'total'), ('cheap', 'ones'), ('expensive', 'ones'), ('half-used', 'glazes'), (\"n't\", 'get'), ('old', 'ones'), ('which', 'glazes'), ('want', 'to'), ('have', 'no'), ('one', 'to'), ('to', 'fill'), ('to', 'spend'), (',', 'but'), (',', 'or'), ('Usually', ','), ('about', 'the'), ('colors', ','), ('get', 'the'), ('or', 'the'), ('the', 'nice'), ('the', 'old'), ('the', 'yucky'), ('glazes', '.'), ('ones', '.'), ('I', 'just'), ('reason', 'I'), ('.', 'Could'), ('.', 'Debating'), ('.', 'For'), ('.', 'Usually'), ('Monday', '.'), ('choice', '.'), (',', 'do'), ('the', 'bottles'), ('use', ','), ('use', 'the'), ('bottles', 'I'), ('glazes', 'to'), ('.', 'I'), ('bottles', '.'), ('ones', ','), ('the', 'glazes'), ('want', ','), ('I', 'want')]\n",
      "TF-IDF matrix shape: (1, 30)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 10\n",
      "Total words: 195\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'tile', 'organizer', 'managed', 'required', 'rear', 'time', 'is', 'want', 'had', 'doctor', 'painted', 'paint', 'cancelled', 'wanted', 'call', 'got', 'paw', 'line', 'accents', 'view', 'accent', 'fess', 'asked', 'bother', 'color', 'fear', 'beg', 'do', 'school', 'be', 'failure', 'done', 'course', 'appointment', 'tomorrow', 'center', 'become', 'finish', 'dilemma', 'tiles', 'one', 'kids', 'leave'}\n",
      "Punctuation similarity: 0.7334722222222222\n",
      "Length similarity: 0.4309523809523809\n",
      "Active voice count: 0.6\n",
      "Passive voice count: 0.4\n",
      "Grammar errors count: 43\n",
      "Upper case count: 19\n",
      "Lower case count: 607\n",
      "Number of function words: 45\n",
      "Number of nodes in graph: 109\n",
      "Phrase patterns: [(\"''\", 'accents'), ('Not', 'perfect'), ('Of', 'course'), ('``', 'dog'), ('amazingly', 'inspired'), ('at', 'least'), ('become', 'amazingly'), ('bother', 'with'), ('call', 'from'), ('dilemma', 'of'), ('dog', 'paw'), ('even', 're-fired'), ('fear', 'total'), ('fess', 'up'), ('just', 'fear'), ('lazy', 'do'), ('mural', 'organizer'), (\"n't\", 'even'), ('organizer', 'who'), ('passable', 'as'), ('paw', \"''\"), ('there', 'at'), ('total', 'failure'), ('view', 'tomorrow'), ('who', 'asked'), (\"'s\", 'appointment'), (\"'s\", 'there'), ('4', 'tile'), ('a', 'call'), ('a', 'doctor'), ('asked', 'if'), ('beautiful', 'like'), ('beg', 'for'), ('cancelled', 'a'), ('doctor', \"'s\"), ('failure', 'like'), ('for', 'more'), ('for', 'school'), ('had', \"n't\"), ('how', 'lazy'), ('how', 'much'), ('kids', 'got'), ('more', 'time'), ('my', 'kids'), ('my', 'rear'), ('no', 'one'), ('now', 'that'), ('of', 'how'), ('one', 'wanted'), ('stupid', '4'), ('that', 'no'), ('two', '``'), ('two', 'accent'), ('while', 'my'), ('would', 'be'), ('?', 'Mostly'), ('?', 'Or'), ('?', 'Should'), ('Or', 'is'), ('accent', 'tiles'), ('color', 'should'), ('is', 'on'), ('much', 'color'), ('rear', 'is'), ('tiles', 'while'), ('tiles', 'would'), ('tiles', 'yet'), ('with', '?'), ('it', 'in'), ('.', 'Of'), ('center', '.'), ('time', '.'), (',', 'or'), ('or', 'done'), ('be', 'ready'), ('got', 'a'), ('got', 'ready'), ('last', 'one'), ('last', 'time'), ('ready', 'for'), ('stupid', 'center'), ('that', 'my'), ('tile', 'center'), ('tile', 'that'), ('color', 'it'), ('managed', 'to'), ('to', 'become'), ('to', 'fess'), ('to', 'paint'), ('to', 'view'), ('want', 'to'), ('wanted', 'to'), ('I', 'had'), ('and', 'beautiful'), ('and', 'beg'), ('and', 'finish'), ('and', 'painted'), ('and', 'white'), ('as', 'it'), ('black', 'and'), ('but', 'it'), ('inspired', 'and'), ('it', 'black'), ('it', 'passable'), ('leave', 'it'), ('perfect', 'and'), ('be', '?'), ('like', 'the'), ('the', 'last'), ('the', 'stupid'), ('the', 'two'), ('.', 'Not'), ('.', 'Now'), ('.', 'So'), ('least', '.'), ('school', '.'), ('tomorrow', '.'), ('course', ','), ('in', ','), ('is', '?'), ('the', 'tiles'), ('I', 'bother'), ('I', 'cancelled'), ('I', 'just'), ('I', 'leave'), ('I', 'managed'), ('I', 'want'), ('Should', 'I'), ('So', 'I'), ('do', 'I'), ('should', 'I'), ('had', 'to'), ('ready', 'to'), ('to', 'be'), (',', 'I'), (',', 'and'), ('Now', 'the'), ('done', 'the'), ('finish', 'the'), ('from', 'the'), ('on', 'the'), ('painted', 'the'), ('re-fired', 'the'), ('the', 'dilemma'), ('the', 'line'), ('the', 'mural'), ('and', 'if'), ('it', \"'s\"), ('or', 'color'), ('or', 'the'), (',', 'but'), (',', 'now'), ('Mostly', ','), ('Well', ','), ('accents', ','), ('appointment', ','), ('line', ','), ('paint', ','), ('up', ','), ('white', ','), ('yet', ','), ('is', 'it'), ('it', 'is'), ('I', 'got'), ('if', 'I'), ('if', 'the'), ('the', 'tile'), (',', 'how'), ('one', ','), ('I', 'color')]\n",
      "TF-IDF matrix shape: (1, 51)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3\n",
      "Total words: 61\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'get', 'was', 'waited', 'is', 'going', 'i', 'felt', 'started', 'lady', 'give', 'asked', 'said', 'Yippee', 'asking', 'working', 'news', 'rate', 'year', 'eve'}\n",
      "Punctuation similarity: 0.8409385813148789\n",
      "Length similarity: 0.5217391304347826\n",
      "Active voice count: 0.3333333333333333\n",
      "Passive voice count: 0.6666666666666666\n",
      "Grammar errors count: 19\n",
      "Upper case count: 8\n",
      "Lower case count: 167\n",
      "Number of function words: 11\n",
      "Number of nodes in graph: 47\n",
      "Phrase patterns: [(\"'ve\", 'waited'), ('..', 'Yippee'), ('10/tile', '..'), ('At', 'that'), ('The', 'great'), ('Yippee', '!'), ('asked', 'for'), ('get', 'anywhere'), ('give', 'me'), ('going', 'to'), ('great', 'news'), ('lady', 'felt'), (\"n't\", 'going'), ('news', 'is'), ('on', 'last'), ('said', 'they'), ('she', 'said'), ('started', 'working'), ('that', 'rate'), ('they', 'would'), ('to', 'get'), ('waited', 'a'), ('working', 'on'), ('would', 'give'), ('.', 'At'), ('6/tile', '.'), ('8/tile', '.'), ('a', 'year'), ('and', 'the'), ('felt', 'so'), ('is', 'the'), ('last', 'year'), ('so', 'bad'), ('the', 'lady'), ('the', 'mural'), ('was', 'asking'), ('was', \"n't\"), ('year', ','), ('I', 'was'), ('$', '10/tile'), ('$', '6/tile'), ('$', '8/tile'), ('asking', '$'), ('for', '$'), ('me', '$'), (',', 'and'), (',', 'she'), ('anywhere', ','), ('bad', ','), ('I', \"'ve\"), ('I', 'asked'), ('I', 'started'), ('mural', 'I'), ('rate', 'I'), (',', 'so'), ('.', 'I'), ('so', 'I')]\n",
      "TF-IDF matrix shape: (1, 20)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 8\n",
      "Total words: 124\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'tile', 'crap', 'get', 'go', 'difference', 'flatten', 'mixing', 'is', 'setting', 'are', 'retiring', 'bottles', 'Guess', 'know', 'part', 'has', 'Ugh', 'did', 'fridge', 'delayed', 'need', 'weekend', 'shipped', 'been', 'broke', 'filled', 'seems', 'glaze', 'school', 'glazes', 'be', 'way', 'have', 'painting', 'course', 'done', 'compared', 'tiles', 'year', 'Today', 'start', 'question', 'tried'}\n",
      "Punctuation similarity: 0.796875\n",
      "Length similarity: 0.5721544715447154\n",
      "Active voice count: 0.75\n",
      "Passive voice count: 0.25\n",
      "Grammar errors count: 43\n",
      "Upper case count: 15\n",
      "Lower case count: 455\n",
      "Number of function words: 21\n",
      "Number of nodes in graph: 93\n",
      "Phrase patterns: [('4', 'part'), ('6', \"''\"), ('?', 'Or'), ('Friday', 'shipped'), ('Must', 'be'), ('Or', 'should'), ('Re-painting', '2'), ('The', 'question'), ('Ugh', '!'), ('already', 'been'), ('are', 'setting'), ('asap', 'because'), ('be', 'done'), ('because', 'they'), ('been', 'delayed'), ('by', 'Friday'), ('course', 'will'), ('crap', 'compared'), ('delayed', 'once'), ('did', \"n't\"), ('done', 'asap'), ('filled', '?'), ('get', '200'), ('glaze', 'did'), ('go', 'with'), ('has', 'already'), ('is', 'can'), ('last', 'tile'), ('like', 'crap'), ('mixing', 'glazes'), (\"n't\", 'flatten'), ('need', 'to'), ('never', 'know'), ('new', 'nozzle'), ('now', 'seems'), ('nozzle', 'bottles'), ('part', 'mural'), ('question', 'is'), ('re-painting', 'those'), ('school', 'year'), ('seems', 'like'), ('stupid', '4'), ('they', 'are'), ('this', 'weekend'), ('those', 'stupid'), ('tile', 'painting'), ('two', 'broken'), ('which', 'now'), ('will', 'never'), ('with', 'what'), ('200', 'of'), ('They', 'of'), ('better', 'start'), ('flatten', 'out'), ('glazes', 'for'), ('of', 'course'), ('out', 'all'), ('painting', 'for'), ('refiring', '6'), ('refiring', 'two'), ('setting', 'this'), ('start', 'mixing'), ('start', 're-painting'), ('that', 'broke'), ('to', 'start'), ('tried', 'out'), ('weekend', 'by'), ('tiles', 'that'), ('and', 'filled'), ('and', 'has'), ('shipped', 'and'), ('for', 'the'), (',', 'and'), (\"''\", 'tiles'), (',', 'which'), ('2', 'tiles'), ('broke', ','), ('broken', 'tiles'), ('have', ','), ('mural', 'tiles'), ('out', 'this'), ('.', 'Guess'), ('.', 'Must'), ('.', 'Re-painting'), ('.', 'The'), ('.', 'They'), ('.', 'Ugh'), ('Guess', 'I'), ('I', 'better'), ('I', 'get'), ('I', 'go'), ('I', 'have'), ('I', 'need'), ('I', 'tried'), ('Today', 'I'), ('all', 'the'), ('bottles', 'I'), ('can', 'I'), ('compared', '.'), ('difference', '.'), ('know', 'the'), ('once', '.'), ('should', 'I'), ('the', 'difference'), ('the', 'glaze'), ('the', 'last'), ('the', 'new'), ('the', 'school'), ('the', 'way'), ('way', '.'), ('what', 'I'), ('year', '.'), ('and', 'refiring'), (',', 'refiring'), ('weekend', ','), ('of', 'the'), ('that', 'the'), ('tiles', ','), ('tiles', '.')]\n",
      "TF-IDF matrix shape: (1, 41)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 11\n",
      "Total words: 243\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'glass', 'sifter', 'baking', 'gum', 'custom', 'was', 'sharpie', 'frit', 'intensity', 'works', 'paint', 'stick', 'inclusions', 'temperature', 'material', 'artist', 'be', 'have', 'sells', 'paper', 'frames', 'fire', 'excuse', 'bubbles', 'use', 'slumping', 'temp', 'keep', 'see', 'sons', 'sugar', 'attempt', 'cutting', 'clay', 'flaming', 'dishes', 'fused', 'research', 'breakage', 'makes', 'interesting', 'open', 'slump', 'using', 'is', 'had', 'painted', 'elders', 'learn', 'woman', 'life', 'kiln', 'do', 'enamels', 'talking', 'shards', 'Cracked', 'Today', 'stuff', 'firing', 'time', 'test', 'glue', 'molds', 'know', 'write', 'drop', 'bisque', 'Venezuela', 'does', 'Learned', 'turned', 'wait', 'fine', 'soda', 'slumped', 'applying'}\n",
      "Punctuation similarity: 0.7750771604938271\n",
      "Length similarity: 0.29623824451410663\n",
      "Active voice count: 0.5454545454545454\n",
      "Passive voice count: 0.45454545454545453\n",
      "Grammar errors count: 77\n",
      "Upper case count: 16\n",
      "Lower case count: 888\n",
      "Number of function words: 45\n",
      "Number of nodes in graph: 145\n",
      "Phrase patterns: [('It', 'was'), ('My', 'past'), ('applying', 'enamels'), ('arabic', 'through'), ('baking', 'soda'), ('bubbles', 'between'), ('ca', \"n't\"), ('can', 'open'), ('ceramic', 'frames'), ('color.Use', 'ceramic'), ('custom', 'programmed'), ('drop', 'molds'), ('elmers', 'glue'), ('fiber', 'paper'), ('fire', 'paint'), ('good', 'excuse'), ('gum', 'arabic'), ('higher', 'temperature'), ('hot', 'flaming'), ('keep', 'intensity'), ('know', 'that'), ('lower', 'temp'), ('makes', 'bubbles'), ('material', 'already'), ('metallic', 'sharpie'), (\"n't\", 'wait'), ('no', 'breakage'), ('older', 'woman'), ('out', 'so-so'), ('paper', 'vs.'), ('past', 'attempt'), ('programmed', 'my'), ('red', 'hot'), ('saggar', '!'), ('see', 'what'), ('soda', 'makes'), ('stick', 'shards'), ('test', 'frit'), ('that', 'you'), ('time', 'talking'), ('turned', 'out'), ('usually', 'does'), ('wash.', 'slump'), ('works', 'fine'), ('would', 'be'), ('you', 'can'), (';', 'how'), ('Cracked', 'bisque'), ('She', 'is'), ('an', 'older'), ('artist', 'over'), ('be', 'nice'), ('bisque', 'works'), ('do', 'some'), ('fine', 'as'), ('firing', 'inclusions'), ('for', 'drop'), ('frames', 'for'), ('have', 'some'), ('her', ';'), ('her', 'stuff'), ('in', 'Venezuela'), ('learn', 'how'), ('life', 'as'), ('nice', 'time'), ('over', 'for'), ('sells', 'her'), ('she', 'sells'), ('she', 'usually'), ('slumping', 'in'), ('some', 'material'), ('some', 'research'), ('use', 'sparingly'), ('using', 'elmers'), ('was', 'interesting'), ('what', 'is'), ('I', 'had'), ('.Use', 'of'), ('Learned', 'about'), ('about', 'cutting'), ('about', 'life'), ('at', 'painted'), ('attempt', 'at'), ('does', 'it'), ('flaming', 'kiln'), ('frit', 'on'), ('intensity', 'of'), ('it', 'manually'), ('it', 'properly'), ('it', 'would'), ('just', 'about'), ('kiln', 'wash.'), ('my', 'kiln'), ('of', 'color.Use'), ('of', 'fiber'), ('on', 'clay'), ('paint', 'at'), ('research', 'on'), ('shards', 'of'), ('slump', 'at'), ('so', 'it'), ('talking', 'about'), ('vs.', 'kiln'), ('(', 'dishes'), (')', '.Use'), (')', 'Cracked'), ('Venezuela', ')'), ('dishes', ')'), ('manually', ')'), ('molds', '('), ('sparingly', ')'), ('stuff', '('), ('with', 'no'), ('write', 'with'), ('an', 'artist'), ('as', 'an'), ('how', 'she'), ('in', 'bisque'), ('is', 'an'), ('is', 'inside'), (',', 'and'), ('I', 'ca'), ('I', 'custom'), ('I', 'have'), ('Today', 'I'), (',', 'using'), ('.', 'It'), ('had', 'a'), ('.', 'I'), ('on', 'fused'), ('use', 'it'), ('and', 'dichroic'), ('and', 'fire'), ('and', 'just'), ('and', 'slumped'), ('clay', 'and'), ('inclusions', 'and'), ('temperature', 'and'), ('at', 'a'), ('(', 'in'), ('(', 'she'), ('(', 'use'), ('inside', 'with'), ('with', 'her'), (',', 'applying'), (',', 'slumping'), (',', 'so'), ('.', 'Learned'), ('.', 'Like'), ('.', 'My'), ('.', 'She'), ('.', 'baking'), ('Like', 'to'), ('a', 'good'), ('a', 'higher'), ('a', 'lower'), ('a', 'metallic'), ('a', 'red'), ('a', 'sifter'), ('already', ','), ('breakage', '.'), ('enamels', 'to'), ('excuse', 'to'), ('glue', 'to'), ('interesting', 'to'), ('open', 'a'), ('painted', ','), ('properly', '.'), ('sharpie', 'to'), ('sifter', ','), ('so-so', '.'), ('temp', 'to'), ('through', 'a'), ('to', 'do'), ('to', 'gum'), ('to', 'keep'), ('to', 'know'), ('to', 'learn'), ('to', 'see'), ('to', 'stick'), ('to', 'test'), ('to', 'write'), ('together', ','), ('wait', 'to'), ('woman', ','), ('between', 'glass'), ('cutting', 'glass'), ('dichroic', 'glass'), ('glass', 'saggar'), ('glass', 'together'), ('glass', 'turned'), ('slumped', 'glass'), ('kiln', '('), ('kiln', 'with'), ('with', 'on'), ('and', 'firing'), ('glass', '.'), (',', 'fused'), ('a', 'firing'), ('a', 'nice'), ('artist', '.'), ('as', 'a'), ('bisque', ','), ('firing', '.'), ('for', 'a'), ('fused', ','), ('how', 'to'), ('inside', '.'), ('nice', 'to'), ('to', 'use'), ('using', 'a'), ('was', 'a'), ('fused', 'glass'), ('glass', 'artist'), ('glass', 'inside'), ('and', 'I'), ('about', 'glass'), ('of', 'glass'), ('on', 'glass'), ('a', 'glass'), ('glass', ','), (')', ','), (')', '.'), ('glass', '('), ('glass', 'with'), ('with', 'glass')]\n",
      "TF-IDF matrix shape: (1, 91)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 43\n",
      "Total words: 742\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'wanting', 'find', 'set', 'breaking', 'tired', 'pennies', 'majored', 'was', 'creating', 'experimented', 'silent', 'confidence', 'demos', 'felt', 'day', 'realized', 'live', 'work', 'enter', 'contest', 'cone', 'did', 'come', 'asked', 'companies', 'muse', 'businesswoman', 'hobby', 'accept', 'been', 'opportunity', 'grail', 'saw', 'taking', 'basement', 'living', 'be', 'artist', 'have', 'avenues', 'working', 'Paper', 'ventures', 'office', 'purpose', 'energy', 'create', 'paper', 'season', 'grown', 'came', 'Was', 'samples', 'selling', 'period', 'became', 'get', 'doing', 'engulfed', 'agenda', 'absorbed', 'projects', 'paid', 'make', 'whole', 'passed', 'like', None, 'got', 'see', 'production', 'demonstrations', 'lease', 'beckoning', 'clay', 'years', 'sat', 'everything', 'house', 'degree', 'closed', 'jumped', 'renegotiated', 'way', 'supporting', 'given', 'par', 'ceramics', 'entries', 'support', 'Tools', 'people', 'am', 'existed', 'death', 'walked', 'world', 'is', 'going', 'had', 'catapult', 'are', 'made', 'knew', 'questions', 'studio', 'look', 'Attending', 'courtyard', 'teachers', 'home', 'learn', 'being', 'ferocity', 'conversations', 'throwing', 'driveway', 'life', 'planning', 'wandered', 'caseloads', 'asking', 'lost', 'do', 'lies', 'space', 'garage', 'pondering', 'schools', 'spying', 'left', 'box', 'mug', 'cut', 'spend', 'everyone', 'housewife', 'hours', 'mother', 'study', 'interest', 'stuff', 'taught', 'dingo', 'go', 'arm', 'business', 'sale', 'trying', 'sanity', 'imagined', 'experience', 'lot', 'time', 'San', 'store', 'love', 'apprentice', 'grow', 'know', 'ways', 'exploring', 'went', 'thought', 'sacrifices', 'knowing', 'sake', 'ceca', 'entrepreneur', 'watch', 'storefront', 'children', 'moved', 'vowed', 'hated', 'lifetime', 'First', 'point', 'area', 'potters', 'story', 'exhibits', 'touching', 'kids', 'lectures', 'bedroom', 'music', 'year'}\n",
      "Punctuation similarity: 0.6896999054820416\n",
      "Length similarity: 0.29347184511864005\n",
      "Active voice count: 0.5581395348837209\n",
      "Passive voice count: 0.4418604651162791\n",
      "Grammar errors count: 188\n",
      "Upper case count: 81\n",
      "Lower case count: 2501\n",
      "Number of function words: 148\n",
      "Number of nodes in graph: 299\n",
      "Phrase patterns: [('$', '65'), ('......', 'First'), ('Attending', 'NCECA'), ('San', 'Diego'), ('absorbed', 'conversations'), ('am', 'lucky'), ('are', 'grown'), ('area', 'where'), ('basement', 'area'), ('be', 'competitive'), ('been', 'different'), ('box', 'contest'), ('come', 'later'), ('companies', 'throwing'), ('cone', 'box'), ('grow', 'literally'), ('hated', 'being'), ('holy', 'grail'), ('kids', 'are'), ('many', 'ways'), ('mug', 'sale'), ('music', 'on'), ('passing', 'interest'), ('purpose', 'without'), ('questions', '......'), ('sanity', 'lies'), ('schools', 'beckoning'), ('so', 'great'), ('sometimes', 'breaking'), ('spend', 'hours'), ('stuff', 'up'), ('taking', 'over'), ('this', 'season'), ('throwing', 'samples'), ('turning', 'point'), ('went', 'reluctantly'), ('world', 'experience'), ('felt', 'like'), (\"'s\", 'entries'), ('The', 'driveway'), ('almost', 'could'), ('another', 'one'), ('another', 'story'), ('asked', 'if'), ('beckoning', '('), ('being', 'just'), ('breaking', 'even'), ('but', 'curious'), ('by', 'production'), ('cut', 'off'), ('degree', '?'), ('different', 'if'), ('engulfed', 'by'), ('everything', 'else'), ('exploring', 'new'), ('extra', 'time'), ('k-12', 'children'), ('living', 'doing'), ('most', 'interested'), ('new', 'avenues'), ('new', 'confidence'), ('no', 'extra'), ('only', 'really'), ('or', 'energy'), ('or', 'working'), ('real', 'world'), ('really', 'excited'), ('really', 'wanting'), ('sat', 'through'), ('selling', 'everything'), ('set', 'off'), ('short', 'time'), ('story', ')'), ('support', '?'), ('teachers', 'who'), ('through', 'exhibits'), ('time', 'period'), ('where', 'everyone'), ('who', 'live'), ('an', 'artist'), ('from', 'now'), ('got', 'back'), ('more', 'than'), (\"''\", 'teachers'), ('20', 'years'), ('40', 'years'), ('70', 'years'), ('It', 'felt'), ('``', 'agenda'), ('``', 'create'), ('agenda', \"''\"), ('came', 'back'), ('create', \"''\"), ('enter', 'next'), ('have', 'imagined'), ('have', 'patient'), ('into', 'blindly'), ('jumped', 'into'), ('life', '20'), ('next', 'day'), ('next', 'projects'), ('next', 'year'), ('potters', 'do'), ('supporting', 'myself'), ('walked', 'into'), ('wandered', 'aimlessly'), ('when', 'several'), ('years', 'old'), ('years', 'from'), ('did', \"n't\"), ('They', 'know'), ('an', 'apprentice'), ('an', 'entrepeneur'), ('arm', 'for'), ('as', 'possible'), ('catapault', 'me'), ('for', 'pennies'), ('from', 'sometimes'), ('given', 'me'), ('know', 'existed'), ('learn', 'as'), ('lease', 'for'), ('made', 'as'), ('not', 'married'), ('opportunity', 'than'), ('patient', 'people'), ('people', 'asked'), ('people', 'get'), ('people', 'look'), ('point', 'for'), ('several', 'people'), ('tired', 'from'), ('ventures', 'as'), ('watched', 'people'), (\"'s\", 'another'), ('(', 'asking'), ('children', \"'s\"), ('children', ')'), ('everyone', 'thought'), ('time', 'or'), ('way', 'by'), ('.', 'I'), ('my', 'life'), ('like', 'death'), ('Little', 'did'), ('else', 'would'), ('experimented', 'more'), ('how', 'would'), ('more', 'opportunity'), ('studio', 'became'), ('studio', 'taking'), ('would', 'catapault'), ('would', 'come'), ('would', 'enter'), ('would', 'spend'), ('The', 'next'), ('``', 'real'), ('asking', 'myself'), ('back', 'home'), ('business', 'lectures'), ('could', 'have'), ('going', 'into'), ('got', 'there'), ('knew', 'much'), ('no', '``'), ('real', \"''\"), ('saw', 'myself'), ('through', 'lectures'), ('wandered', 'through'), ('hours', 'with'), ('live', 'with'), ('par', 'with'), ('planning', 'what'), ('see', 'what'), ('taught', 'with'), ('unknown', 'with'), ('with', 'ferocity'), ('working', 'with'), ('avenues', 'of'), ('lifetime', 'of'), ('lot', 'of'), ('of', 'creating'), ('of', 'potters'), ('of', 'study'), ('out', 'of'), ('sake', 'of'), ('season', 'of'), ('silent', 'of'), ('whole', 'of'), ('as', 'much'), ('for', 'another'), ('from', 'work'), ('most', 'people'), ('much', 'as'), ('not', 'knowing'), ('people', 'who'), ('than', 'most'), ('do', 'when'), ('to', 'find'), ('could', \"n't\"), (',', 'and'), ('.', 'When'), ('life', 'had'), ('grail', 'that'), ('housewife', 'that'), ('lucky', 'that'), ('that', 'accept'), ('that', 'yes'), ('I', 'was'), ('the', 'studio'), ('more', 'paper'), ('much', 'more'), (\"''\", 'for'), ('do', 'it'), ('know', 'when'), ('Diego', 'in'), ('NCECA', 'in'), ('get', 'in'), ('in', '2003'), ('in', 'San'), ('in', 'ceramics'), ('in', 'many'), ('interest', 'in'), ('interested', 'in'), ('lies', 'in'), ('majored', 'in'), ('possible', 'in'), ('Was', 'my'), ('blindly', 'had'), ('had', 'been'), ('had', 'given'), ('had', 'jumped'), ('had', 'made'), ('had', 'majored'), ('had', 'passed'), ('had', 'renegotiated'), ('had', 'walked'), ('my', '$'), ('my', 'arm'), ('my', 'lease'), ('my', 'purpose'), ('my', 'sanity'), ('my', 'space'), ('my', 'stuff'), ('my', 'ventures'), ('paid', 'my'), ('renegotiated', 'my'), ('samples', 'my'), ('doing', 'what'), ('doing', 'with'), ('knowing', 'what'), ('with', 'no'), ('lectures', 'and'), ('I', 'had'), (\"n't\", 'do'), (\"n't\", 'have'), ('When', 'I'), ('of', 'business'), ('as', 'an'), ('for', 'me'), ('me', 'from'), ('not', 'know'), ('2003', 'was'), ('Where', 'was'), ('a', 'businesswoman'), ('a', 'degree'), ('a', 'hobby'), ('a', 'housewife'), ('a', 'lifetime'), ('a', 'living'), ('a', 'lot'), ('a', 'mother'), ('a', 'muse'), ('a', 'passing'), ('a', 'short'), ('a', 'store'), ('a', 'storefront'), ('a', 'turning'), ('became', 'a'), ('day', 'was'), ('make', 'a'), ('was', 'better'), ('was', 'engulfed'), ('was', 'lost'), ('was', 'selling'), ('was', 'so'), ('without', 'a'), ('it', 'was'), ('what', 'I'), ('and', '40'), ('and', 'demonstrations'), ('and', 'demos'), ('and', 'exploring'), ('and', 'out'), ('and', 'set'), ('and', 'slowly'), ('and', 'this'), ('and', 'watch'), ('and', 'watched'), ('clay', 'and'), ('conversations', 'and'), ('energy', 'to'), ('excited', 'to'), ('go', 'to'), ('grown', 'and'), ('look', 'and'), ('married', 'and'), ('one', 'to'), ('production', 'and'), ('store', 'to'), ('to', 'be'), ('to', 'cut'), ('to', 'go'), ('to', 'grow'), ('to', 'learn'), ('to', 'make'), ('to', 'par'), ('to', 'see'), ('to', 'support'), ('to', 'supporting'), ('trying', 'to'), ('up', 'to'), ('wanting', 'to'), ('like', 'an'), (\"n't\", 'know'), ('I', 'got'), ('I', 'knew'), ('I', 'realized'), ('.', 'The'), ('knew', 'what'), ('but', 'that'), ('knowing', 'that'), ('that', \"'s\"), ('that', 'everyone'), (',', 'but'), ('now', ','), ('work', ','), ('of', 'lectures'), ('did', 'not'), ('me', 'more'), ('I', 'did'), ('even', 'in'), ('in', 'a'), ('had', 'children'), ('had', 'no'), ('my', 'home'), ('my', 'way'), ('off', 'my'), ('saw', 'my'), ('who', 'had'), ('know', 'what'), ('what', 'it'), ('with', 'me'), ('accept', 'the'), ('closed', 'the'), ('moved', 'the'), ('over', 'the'), ('pondering', 'the'), ('slowly', 'the'), ('spying', 'the'), ('the', 'basement'), ('the', 'bedroom'), ('the', 'caseloads'), ('the', 'clay'), ('the', 'cone'), ('the', 'courtyard'), ('the', 'garage'), ('the', 'holy'), ('the', 'house'), ('the', 'k-12'), ('the', 'kids'), ('the', 'livingroom'), ('the', 'mug'), ('the', 'music'), ('the', 'office'), ('the', 'questions'), ('the', 'sacrifices'), ('the', 'sake'), ('the', 'same'), ('the', 'silent'), ('the', 'unknown'), ('the', 'whole'), ('touching', 'the'), ('?', 'I'), ('I', 'saw'), ('if', 'I'), ('of', 'my'), ('would', 'like'), ('realized', 'that'), ('that', 'felt'), ('a', 'new'), ('a', 'way'), ('everyone', 'was'), ('find', 'a'), ('just', 'a'), ('thought', 'was'), ('was', 'doing'), ('was', 'going'), ('was', 'most'), ('was', 'really'), ('was', 'there'), ('myself', ','), ('and', 'asking'), ('and', 'everything'), ('even', 'to'), ('home', 'to'), ('just', 'to'), ('off', 'to'), ('there', 'to'), ('to', 'work'), ('way', 'to'), ('wandered', 'in'), ('.', 'But'), ('.', 'It'), ('.', 'Little'), ('.', 'Paper'), ('.', 'They'), ('.', 'Tools'), ('.', 'Was'), ('.', 'Where'), ('bedroom', '.'), ('businesswoman', '.'), ('caseloads', '.'), ('competitive', '.'), ('curious', '.'), ('death', '.'), ('demonstrations', '.'), ('demos', '.'), ('entries', '.'), ('ferocity', '.'), ('great', '.'), ('hobby', '.'), ('house', '.'), ('imagined', '.'), ('later', '.'), ('literally', '.'), ('love', '.'), ('mother', '.'), ('muse', '.'), ('passed', '.'), ('pennies', '.'), ('period', '.'), ('same', '.'), ('storefront', '.'), ('study', '.'), ('watch', '.'), ('ways', '.'), ('me', 'that'), ('that', 'it'), ('I', 'would'), (',', '70'), (',', 'companies'), (',', 'how'), (',', 'only'), (',', 'planning'), (',', 'pondering'), (',', 'schools'), (',', 'spying'), (',', 'tired'), (',', 'touching'), (',', 'trying'), ('65', ','), ('First', ','), ('Paper', ','), ('aimlessly', ','), ('apprentice', ','), ('better', ','), ('ceramics', ','), ('confidence', ','), ('contest', ','), ('courtyard', ','), ('creating', ','), ('driveway', ','), ('entrepeneur', ','), ('exhibits', ','), ('existed', ','), ('garage', ','), ('left', ','), ('livingroom', ','), ('lost', ','), ('office', ','), ('old', ','), ('on', ','), ('projects', ','), ('reluctantly', ','), ('sale', ','), ('space', ','), ('year', ','), ('yes', ','), ('I', 'wandered'), ('with', 'what'), (',', 'not'), ('in', 'it'), ('me', 'in'), ('in', 'the'), ('But', 'I'), ('I', 'absorbed'), ('I', 'almost'), ('I', 'am'), ('I', 'came'), ('I', 'closed'), ('I', 'experimented'), ('I', 'hated'), ('I', 'left'), ('I', 'love'), ('I', 'moved'), ('I', 'paid'), ('I', 'sat'), ('I', 'taught'), ('I', 'vowed'), ('I', 'went'), ('Tools', 'I'), ('experience', 'I'), ('sacrifices', 'I'), ('vowed', 'I'), ('had', 'an'), ('know', 'my'), ('and', 'wandered'), ('back', 'to'), ('have', 'to'), ('to', '``'), ('to', 'do'), ('with', 'the'), ('asking', 'the'), ('by', 'the'), ('find', 'the'), ('the', 'business'), ('the', 'real'), ('than', 'I'), ('that', 'would'), ('as', 'a'), ('not', 'a'), ('than', 'a'), ('was', 'an'), ('and', 'not'), ('what', 'that'), ('to', 'my'), (')', '.'), ('.', '('), ('artist', '.'), ('business', '.'), ('everything', '.'), ('paper', '.'), ('thought', '.'), ('studio', ','), ('had', 'more'), ('my', 'studio'), ('would', 'my'), ('like', 'a'), ('was', \"n't\"), ('was', 'a'), ('into', 'the'), ('realized', 'the'), ('the', '``'), ('the', 'next'), ('when', 'the'), (')', ','), (',', 'even'), (',', 'going'), (',', 'just'), (',', 'knowing'), (',', 'or'), (',', 'paper'), ('artist', ','), ('going', ','), ('home', ','), ('paper', ','), ('like', 'to'), (',', 'the'), ('a', 'studio'), ('was', 'more'), ('(', 'I'), ('I', 'could'), ('I', 'thought'), ('there', 'I'), ('and', 'did'), ('and', 'more'), ('studio', 'to'), ('for', 'the'), ('with', 'a'), (\"''\", '.'), ('do', '.'), ('into', '.'), ('myself', '.'), (',', 'when'), ('back', ','), ('lectures', ','), (\"n't\", 'the'), ('that', 'had')]\n",
      "TF-IDF matrix shape: (1, 205)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1\n",
      "Total words: 86\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'tile', 'faceplates', 'BOTTLES', 'backblocks', 'FROM', 'horse', 'PICKUP', 'GET', None, 'bank', 'GO', 'themes', 'fence', 'wall', 'erase', 'pour', 'adding', 'frame', 'whorehouse', 'family', 'do', 'gifts', 'tree', 'dance', 'plates', 'sgraffito', 'fused', 'C', 'additions', 'AND', 'handbill', \"rand's\", 'c'}\n",
      "Punctuation similarity: 0.7966820987654322\n",
      "Length similarity: 0.0\n",
      "Active voice count: 1.0\n",
      "Passive voice count: 0.0\n",
      "Grammar errors count: 33\n",
      "Upper case count: 46\n",
      "Lower case count: 359\n",
      "Number of function words: 14\n",
      "Number of nodes in graph: 63\n",
      "Phrase patterns: [(\"''\", 'or'), ('+C', 'WHEREHOUSE'), ('200', 'BOTTLES'), ('BOTTLES', 'FROM'), ('C', '+C'), ('FROM', 'C'), ('GET', '200'), ('GO', 'GET'), ('``', 'wall'), ('a', '``'), ('addons', 'fused'), ('babyblocks', 'frame'), ('babybottle', 'bank'), ('dry', 'erase'), ('family', 'tree'), ('flower', 'ect-blank'), ('frame', 'family'), ('fused', 'in'), ('gifts', 'to'), ('handbuild', 'over'), ('handbuilt', 'additions'), ('in', 'themes'), ('it', 'GO'), ('large', 'tile'), ('lightswich', 'plates'), ('lightswitch', 'faceplates'), ('or', 'fence'), ('plates', 'then'), ('pour', 'lightswich'), ('then', 'handbuild'), ('to', 'do'), ('tree', 'large'), ('wall', \"''\"), ('with', 'addons'), (':', 'horse'), (':', 'scriffito'), ('AND', 'PICKUP'), ('AND', 'RANDIS'), ('PICKUP', 'AND'), ('WHEREHOUSE', 'AND'), ('also', 'pour'), ('around', 'it'), ('do', ':'), ('fence', 'around'), ('natalie', 'daniel'), ('over', 'them'), ('scriffito', 'doorhangers'), ('them', 'gifts'), ('daniel', 'and'), (',', 'emily'), (',', 'flower'), ('bank', 'for'), ('ceramic', 'babybottle'), ('ceramic', 'lightswitch'), ('ect-blank', 'for'), ('emily', ','), ('erase', 'ceramic'), ('faceplates', 'with'), ('for', 'dry'), ('for', 'stefani'), ('horse', ','), ('stefani', ','), ('tile', 'with'), ('with', 'handbuilt'), ('around', 'them'), ('themes', ':'), ('themes', 'also'), ('additions', 'and'), ('and', 'a'), ('and', 'babyblocks'), ('and', 'natalie'), ('also', 'ceramic'), ('ceramic', 'doorhangers'), ('doorhangers', 'for'), ('doorhangers', 'with'), ('for', 'daniel'), ('and', 'also'), ('and', 'around'), ('them', 'and'), (',', 'and')]\n",
      "TF-IDF matrix shape: (1, 40)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6\n",
      "Total words: 77\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'mistake', 'dinnerware', 'leeching', 'was', 'lead', 'is', 'return', 'use', 'took', 'beating', 'wanted', 'thing', 'write', 'god', 'warning', 'Wo', 'did', 'sell', 'discontinued', 'label', 'dipping', 'factory', 'Wonder', 'tried'}\n",
      "Punctuation similarity: 0.7256944444444444\n",
      "Length similarity: 0.6229166666666667\n",
      "Active voice count: 0.6666666666666666\n",
      "Passive voice count: 0.3333333333333333\n",
      "Grammar errors count: 24\n",
      "Upper case count: 6\n",
      "Lower case count: 294\n",
      "Number of function words: 21\n",
      "Number of nodes in graph: 60\n",
      "Phrase patterns: [(\"'ll\", 'still'), ('Good', 'thing'), ('They', 'should'), ('Wonder', 'if'), ('an', 'uneccessary'), ('clear', 'that'), ('different', 'label'), ('dipping', 'clear'), ('discontinued', 'due'), ('factory', 'like'), ('is', 'beautiful'), ('label', 'warning'), ('lead', 'leeching'), ('my', 'god'), ('new', 'dipping'), ('oh', 'my'), ('on', 'dinnerware'), ('recently', 'tried'), ('should', '!'), ('still', 'sell'), ('that', 'unfortunately'), ('took', 'an'), ('uneccessary', 'beating'), ('unfortunately', 'was'), ('was', 'discontinued'), ('will', 'write'), ('write', 'them'), ('!', 'I'), ('I', 'did'), ('I', 'will'), ('Wo', \"n't\"), ('a', 'different'), ('a', 'new'), ('beating', 'because'), ('because', 'of'), ('did', \"n't\"), ('due', 'to'), (\"n't\", 'return'), (\"n't\", 'use'), ('of', 'the'), ('the', 'factory'), ('the', 'mistake'), ('them', 'because'), ('thing', 'I'), ('to', 'lead'), ('tried', 'a'), ('with', 'a'), (',', 'but'), (',', 'oh'), ('but', ','), ('dinnerware', ','), ('god', ','), ('if', 'they'), ('like', 'they'), ('they', \"'ll\"), ('they', 'took'), ('they', 'wanted'), ('it', 'is'), ('it', 'on'), ('it', 'with'), ('return', 'it'), ('sell', 'it'), ('use', 'it'), ('to', 'the'), ('.', 'Good'), ('.', 'They'), ('.', 'Wo'), ('.', 'Wonder'), ('beautiful', '.'), ('leeching', '.'), ('mistake', '.'), ('wanted', '.'), ('warning', '.'), ('because', 'they'), ('it', 'to'), (',', 'it')]\n",
      "TF-IDF matrix shape: (1, 33)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 41\n",
      "Total words: 825\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'arms', 'instalment', 'form', 'defaced', 'requirement', 'handbags', 'Bus', 'represent', 'flailing', 'was', 'enjoy', 'act', 'anything', 'stick', 'band', 'reasons', 'head', 'turning', 'raisin', 'favor', 'force', 'part', 'elbows', 'assume', 'buses', 'looking', 'enter', 'd', 'did', 'b', 'estate', 'cow', 'accept', 'varying', 'found', 'been', 'dialect', 'beg', 'songs', 'c', 'be', 'have', 'place', 'stops', 'chats', 'promised', 'looks', 'protracted', 'choice', 'rarer', 'driver', 'et', 'hello', 'sympathy', 'detailed', 'feel', 'imagine', 'bossing', 'doing', 'met', 'friends', 'use', 'best', 'drenched', 'state', 'status', 'gets', 'talisman', 'battlefield', 'volume', 'guardians', 'Wedged', 'thing', 'wonder', 'position', None, 'top', 'see', 'bus', 'humans', 'whilst', 'nomination', 'stretches', 'thrust', 'e', 'ignorance', 'foot', 'convey', 'years', 'elected', 'marvel', 'take', 'side', 'disgust', 'Ooh', 'none', 'association', 'way', 'karma', 'shouting', 'exist', 'prattle', 'Monitors', 'become', 'stinking', 'mean', 'n', 'makes', 'assumed', 'voids', 'Let', 'grace', 'air', 'dossiers', 'spirit', 'am', 'people', 'expecting', 'members', 'passengers', 'today', 'exercise', 'roll', 'allows', 'is', 'submission', 'awareness', 'degrees', 'getting', 'directions', 'had', 'understand', 'are', 'gives', 'involves', 'hierarchies', 'individuals', 'information', 'Getting', 'occasions', 'look', 'claim', 'tickets', 'protocol', 'solace', 'activity', 'Violets', 'rock', 'kind', 'importance', 'justify', 'monitors', 'profession', 'Frank', 'conversations', 'positions', 'women', 'life', 'flurry', 'lives', 'illustrating', 'helping', 'blitz', 'crave', 'irritations', 'nerves', 'listening', 'read', 'everyone', 'disapproving', 'queue', 'start', 'eve', 'think', 'smell', 'sample', 'yawning', 'want', 'disregard', 'bet', 'lords', 'memorial', 'know', 'walk', 'has', 'breach', 'war', 'mindless', 'shout', 'monitor', 'country', 'hope', 'rain', 'Man', 'offer', 'service', 'mix', 'putting', 'Sitting', 'fingers', 'wait', 'seats', 'deter', 'style', 'dinner', 'usage', 'shortcomings', 'saying', 'music', 'cause'}\n",
      "Punctuation similarity: 0.787109375\n",
      "Length similarity: 0.34538498326159733\n",
      "Active voice count: 0.4878048780487805\n",
      "Passive voice count: 0.5121951219512195\n",
      "Grammar errors count: 210\n",
      "Upper case count: 63\n",
      "Lower case count: 3222\n",
      "Number of function words: 159\n",
      "Number of nodes in graph: 387\n",
      "Phrase patterns: [(\"'\", 'roll'), (\"'ello\", 'Frank'), ('...', '2'), ('Any', 'breach'), ('It', 'really'), ('Monitors', 'Now'), ('Not', 'content'), ('Ooh', \"'ello\"), ('Parma', 'Violets'), ('Sitting', 'right'), ('Two', 'elderly'), ('Wrong', 'way'), ('act', 'like'), ('actually', 'quite'), ('also', 'offer'), ('always', 'feel'), ('an', 'exercise'), ('anything', 'else'), ('aural', 'submission'), ('back', 'outside'), ('basically', 'involves'), ('blatantly', 'disregard'), ('but', 'wonder'), ('by', 'association'), ('clearly', 'enjoy'), ('convey', 'exactly'), ('cow', 'everyone'), ('disapproving', 'looks'), ('down', 'today'), ('elderly', 'women'), ('even', 'notice'), ('expecting', 'his'), ('feel', 'great'), ('fingers', 'up'), ('foot', 'down'), ('formal', 'nomination'), ('found', 'solace'), ('great', 'sympathy'), ('had', 'defaced'), ('handbags', 'flailing'), ('hapless', 'footsoldiers'), ('heightened', 'awareness'), ('his', 'dinner'), ('insiduous', 'prattle'), ('large', 'queue'), ('legs', 'etc'), ('n', \"'\"), ('next', 'instalment'), ('nominally', 'claim'), ('normally', 'gets'), ('often', 'unnecessary'), ('old-woman', 'paraphanelia'), ('one', 'thing'), ('putting', 'yer'), ('queue', 'stretches'), ('quite', 'sad'), ('raison', \"d'etre\"), ('rarely', 'look'), ('rarer', 'still'), ('real', 'estate'), ('right', 'behind'), ('rock', 'n'), ('roll', 'band'), ('sad', 'individuals'), ('self-appointed', 'positions'), ('small', 'sample'), ('somehow', 'helping'), ('stretches', 'back'), ('style', 'dialect'), ('those', 'stops'), ('three', 'reasons'), ('thrust', 'naively'), ('too', 'relaxed'), ('unnecessary', 'directions'), ('varying', 'degrees'), ('war', 'memorial'), ('will', 'also'), ('wrong', 'turning'), ('yawning', 'when'), ('yer', 'foot'), ('your', 'arms'), ('occasions', 'where'), ('(', 'again'), ('As', 'far'), ('As', 'promised'), ('Bus', 'Monitors'), ('Eg', ':'), ('In', 'two'), ('The', 'Man'), ('The', 'volume'), ('Violets', 'enough'), ('accepted', 'protocol'), ('am', 'not'), ('assume', 'some'), ('become', 'slightly'), ('bus-monitor-priveleged', 'information'), ('careful', 'here'), ('clear', ';'), ('clear', 'disgust'), ('country', 'before'), ('crave', 'by'), ('dinner', '!'), ('dossiers', 'less'), ('exactly', 'what'), ('far', 'as'), ('humans', 'exist'), ('involves', ':'), ('it', 'all'), ('less', 'detailed'), ('less', 'yawning'), ('lives', 'must'), ('looking', 'forward'), ('naively', 'onto'), ('offer', 'information'), ('only', 'imagine'), ('only', 'marvel'), ('or', 'even'), ('protracted', 'chats'), ('quiet', 'chats'), ('rare', 'occasions'), ('seats', '-'), ('shortcomings', 'must'), ('shouting', 'often'), ('slightly', 'less'), ('so', 'assume'), ('status', 'without'), ('stick', 'two'), ('still', 'occasions'), ('then', 'position'), ('tickets', 'whilst'), ('two', 'fingers'), ('two', 'years'), ('was', 'never'), ('was', 'too'), ('way', '!'), ('who', 'normally'), ('!', \"''\"), (\"'ll\", 'be'), ('If', 'getting'), ('They', 'act'), ('They', 'force'), ('They', 'rarely'), ('This', 'basically'), ('Wedged', 'into'), ('any', 'formal'), ('any', 'kind'), ('any', 'requirement'), ('around', 'incredulously'), ('be', 'careful'), ('be', 'putting'), ('conversations', 'take'), ('directions', 'into'), ('doing', 'me'), ('every', 'profession'), ('every', 'walk'), ('everyone', 'around'), ('fellow', 'passengers'), ('getting', 'drenched'), ('helping', 'me'), ('into', 'aural'), ('just', 'saying'), ('look', 'around'), ('never', 'any'), ('outside', 'getting'), ('place', 'cow'), ('take', 'place'), ('where', 'humans'), ('without', 'any'), ('can', 'only'), ('other', 'passengers'), (')', 'On'), ('gives', 'them'), ('in', 'every'), (\"''\", 'style'), ('MI5', 'themselves'), ('Man', \"''\"), ('``', 'Ooh'), ('``', 'Wrong'), ('``', 'spirit'), ('blitz', \"''\"), ('ca', \"n't\"), ('did', \"n't\"), ('disregard', 'other'), ('elected', 'themselves'), ('first', 'choice'), ('force', 'themselves'), ('monitors', 'enter'), ('my', 'Bert'), ('my', 'nerves'), ('my', 'shortcomings'), (\"n't\", 'smell'), (\"n't\", 'think'), ('notice', 'other'), ('other', 'classified'), ('themselves', 'would'), ('or', 'not'), ('whether', 'or'), ('whilst', 'looking'), ('who', 'assume'), ('chats', 'with'), ('is', 'met'), ('met', 'with'), (\"'ope\", 'you'), (\"'s\", 'expecting'), (\"'s\", 'head'), (\"'ve\", 'been'), ('Bert', \"'s\"), ('Let', \"'s\"), ('at', 'fellow'), ('at', 'him'), ('at', 'those'), ('at', 'which'), ('bet', 'you'), ('ever', 'been'), ('has', 'been'), ('illustrating', 'that'), ('incredulously', 'at'), ('irritations', 'that'), ('marvel', 'at'), ('saying', 'that'), ('that', 'because'), ('thing', 'that'), ('volume', 'at'), ('you', \"'ll\"), ('you', \"'ve\"), ('you', '...'), ('must', 'have'), (';', 'these'), ('No-one', 'can'), ('We', 'can'), ('accept', 'these'), ('bossing', 'people'), ('can', 'blatantly'), ('can', 'but'), ('can', 'read'), ('can', 'then'), ('enjoy', 'this'), ('justify', 'their'), ('people', 'about'), ('people', 'cause'), ('their', 'lives'), ('their', 'old-woman'), ('their', 'self-appointed'), ('their', 'tickets'), ('these', 'hapless'), ('this', 'allows'), ('this', 'country'), ('which', 'these'), ('around', 'or'), ('as', 'if'), ('assume', 'importance'), ('be', 'clear'), ('looking', 'around'), ('makes', 'me'), ('me', 'want'), ('not', 'just'), ('place', '('), ('shouting', 'conversations'), ('they', 'crave'), ('a', 'small'), ('This', 'is'), ('is', 'just'), (')', 'Getting'), (')', 'Sitting'), ('again', ')'), ('allows', 'them'), ('are', 'actually'), ('are', 'doing'), ('are', 'verboten'), ('b', ')'), ('battlefield', ')'), ('c', ')'), ('d', ')'), ('driver', 'had'), ('driver', 'has'), ('e', ')'), ('estate', 'for'), ('for', 'loud'), ('for', 'once'), ('for', 'three'), ('fumble', 'for'), ('loud', 'driver'), ('new', 'driver'), ('requirement', 'for'), ('see', 'them'), ('sympathy', 'for'), ('them', 'justify'), ('wait', 'for'), ('and', 'status'), ('bus', 'monitors'), (':', '``'), ('Bus', 'monitors'), ('``', 'The'), ('buses', 'first'), ('exist', 'there'), ('first', 'gives'), ('monitors', 'become'), ('position', 'themselves'), ('there', 'was'), ('what', 'my'), ('getting', 'on'), ('the', 'driver'), ('content', 'with'), ('etc', 'is'), ('is', 'an'), ('is', 'impossible'), ('is', 'one'), ('is', 'somehow'), ('is', 'truly'), ('music', 'is'), ('really', 'is'), ('start', 'with'), ('with', 'disapproving'), ('with', 'varying'), ('every', 'place'), ('place', 'where'), ('these', 'people'), ('they', 'are'), ('just', 'a'), ('at', 'what'), ('been', 'looking'), ('here', \"'s\"), ('it', \"'s\"), ('not', 'you'), ('status', 'that'), ('that', 'makes'), ('want', 'you'), ('importance', 'and'), ('Getting', 'on'), ('about', 'on'), ('detailed', 'on'), ('drenched', 'in'), ('exercise', 'in'), ('gets', 'on'), ('have', 'assumed'), ('have', 'dossiers'), ('have', 'elected'), ('have', 'ever'), ('have', 'protracted'), ('have', 'yet'), ('him', 'in'), ('ignorance', 'on'), ('in', 'bossing'), ('in', 'illustrating'), ('in', 'prime'), ('on', 'members'), ('on', 'top'), ('service', 'have'), ('solace', 'in'), ('voids', 'in'), ('women', 'on'), ('would', 'have'), ('on', 'my'), ('the', 'bus'), ('-', 'they'), ('Although', 'they'), ('like', 'they'), ('they', 'clearly'), ('they', 'found'), ('they', 'nominally'), ('they', 'shout'), ('they', 'will'), ('when', 'they'), ('people', 'who'), ('their', 'clear'), ('their', 'seats'), ('these', 'heirachies'), ('this', '('), ('if', 'there'), ('of', 'Parma'), ('for', 'them'), ('a', 'favour'), ('a', 'flurry'), ('a', 'large'), ('a', 'new'), ('a', 'rock'), ('a', 'state'), ('a', 'war'), ('a', 'wrong'), ('claim', 'to'), ('defaced', 'a'), ('enter', 'a'), ('form', 'a'), ('forward', 'to'), ('listening', 'to'), ('to', 'convey'), ('to', 'music'), ('to', 'represent'), ('to', 'see'), ('to', 'start'), ('to', 'stick'), ('to', 'understand'), ('to', 'wait'), ('up', 'to'), ('yet', 'to'), ('.', 'They'), ('.', 'This'), ('with', 'their'), ('I', \"'ope\"), ('I', 'accept'), ('I', 'always'), ('I', 'am'), ('I', 'bet'), ('I', 'ca'), ('I', 'know'), ('I', 'mean'), ('Perhaps', 'I'), ('and', 'form'), ('and', 'fumble'), ('and', 'handbags'), ('and', 'legs'), ('and', 'quiet'), ('and', 'so'), ('and', 'stinking'), ('arms', 'and'), ('because', 'I'), ('elbows', 'and'), ('nerves', 'and'), ('relaxed', 'and'), (')', 'Bus'), (')', 'gives'), ('are', 'friends'), ('are', 'heirachies'), ('friends', 'are'), ('been', 'in'), (\"'s\", 'be'), ('conversations', 'at'), ('On', 'the'), ('onto', 'the'), ('that', 'they'), (\"n't\", 'there'), ('bus', 'did'), ('bus', 'lords'), ('bus', 'mongs'), ('bus', 'monitor'), ('bus', 'real'), ('bus', 'service'), ('bus', 'usage'), ('prime', 'bus'), ('bus', 'first'), ('Violets', 'is'), ('friends', 'with'), ('heirachies', 'with'), ('seats', 'with'), ('with', 'friends'), ('with', 'shouting'), ('into', 'their'), ('these', 'conversations'), ('they', 'can'), (\"n't\", 'you'), ('.', 'Essentially'), ('around', 'them'), ('driver', 'conversations'), ('them', 'if'), ('them', 'into'), (',', 'whether'), ('Essentially', ','), ('before', ','), ('I', 'can'), ('have', 'become'), ('information', 'on'), ('on', 'buses'), ('on', 'who'), ('air', 'of'), ('breach', 'of'), ('choice', 'of'), (\"d'etre\", 'of'), ('degrees', 'of'), ('flurry', 'of'), ('guardians', 'of'), ('instalment', 'of'), ('kind', 'of'), ('members', 'of'), ('of', 'accepted'), ('of', 'activity'), ('of', 'anything'), ('of', 'grace'), ('of', 'heightened'), ('of', 'ignorance'), ('of', 'life'), ('of', 'your'), ('part', 'of'), ('side', 'of'), ('smell', 'of'), ('some', 'of'), ('spirit', 'of'), ('state', 'of'), ('stinking', 'of'), ('think', 'of'), ('top', 'of'), ('use', 'of'), ('walk', 'of'), ('years', 'of'), ('them', 'to'), ('of', 'the'), (':', 'a'), ('become', 'a'), ('here', 'to'), ('makes', 'a'), ('to', 'exist'), ('want', 'to'), ('whether', 'to'), ('whilst', 'a'), ('.', 'I'), ('bus', 'people'), ('passengers', 'is'), ('(', 'I'), ('I', 'want'), ('I', 'was'), ('and', 'shouting'), ('as', 'I'), ('what', 'I'), ('are', \"n't\"), ('first', 'for'), ('monitors', 'are'), ('them', 'first'), ('there', 'are'), (',', 'and'), ('assumed', 'the'), ('behind', 'the'), ('imagine', 'the'), ('the', 'Taliban'), ('the', 'air'), ('the', 'battlefield'), ('the', 'blitz'), ('the', 'guardians'), ('the', 'irritations'), ('the', 'next'), ('the', 'part'), ('the', 'rain'), ('the', 'raison'), ('the', 'rare'), ('the', 'rarer'), ('the', 'side'), ('the', 'use'), ('the', 'voids'), ('truly', 'the'), ('of', 'bus'), (\"'s\", 'people'), ('that', 'these'), (',', 'this'), ('passengers', ','), ('in', 'place'), ('bus', 'before'), ('bus', 'makes'), ('bus', 'protocol'), ('driver', \"'s\"), ('if', 'they'), ('importance', 'they'), ('there', 'is'), ('.', 'Although'), ('.', 'Any'), ('.', 'But'), ('.', 'Eg'), ('.', 'Firstly'), ('.', 'If'), ('.', 'In'), ('.', 'It'), ('.', 'Let'), ('.', 'MI5'), ('.', 'No-one'), ('.', 'Not'), ('.', 'Perhaps'), ('.', 'Secondly'), ('.', 'Thirdly'), ('.', 'Two'), ('.', 'We'), ('.', 'Wedged'), ('.', 'b'), ('.', 'c'), ('.', 'd'), ('.', 'e'), ('2', '.'), ('Taliban', '.'), ('activity', '.'), ('association', '.'), ('awareness', '.'), ('dialect', '.'), ('disgust', '.'), ('else', '.'), ('enough', '.'), ('favour', '.'), ('grace', '.'), ('individuals', '.'), ('looks', '.'), ('lords', '.'), ('mean', '.'), ('memorial', '.'), ('mongs', '.'), ('monitor', '.'), ('nomination', '.'), ('prattle', '.'), ('rain', '.'), ('reasons', '.'), ('represent', '.'), ('submission', '.'), ('understand', '.'), ('verboten', '.'), ('wonder', '.'), ('me', 'a'), ('to', 'be'), ('where', 'a'), (',', 'bus-monitor-priveleged'), (',', 'elbows'), (',', 'insiduous'), (',', 'listening'), (',', 'mindless'), (',', 'thrust'), ('But', ','), ('Firstly', ','), ('Frank', ','), ('Now', ','), ('Secondly', ','), ('Thirdly', ','), ('all', ','), ('band', ','), ('cause', ','), ('classified', ','), ('flailing', ','), ('footsoldiers', ','), ('head', ','), ('impossible', ','), ('know', ','), ('life', ','), ('mindless', ','), ('once', ','), ('paraphanelia', ','), ('positions', ','), ('profession', ','), ('promised', ','), ('read', ','), ('shout', ','), ('stops', ','), ('today', ','), ('turning', ','), ('usage', ','), ('I', 'have'), ('for', 'their'), ('for', 'these'), ('these', 'are'), ('on', 'the'), ('have', \"n't\"), ('themselves', 'in'), (',', 'I'), ('of', 'it'), ('of', 'seats'), ('position', 'of'), ('sample', 'of'), (\"''\", 'they'), ('a', '``'), ('to', '``'), ('this', 'is'), ('been', 'on'), ('have', 'been'), ('on', 'at'), ('you', 'have'), (\"''\", 'and'), ('and', 'other'), ('there', 'and'), ('the', 'buses'), ('the', 'position'), (',', 'in'), ('been', 'to'), ('you', 'to'), ('in', 'their'), ('in', 'this'), ('people', 'have'), ('my', 'bus'), ('other', 'bus'), ('themselves', 'bus'), ('of', 'importance'), ('people', 'they'), ('with', 'the'), ('to', 'this'), ('.', 'As'), ('.', 'Bus'), ('.', 'The'), ('buses', '.'), ('crave', '.'), ('exist', '.'), ('heirachies', '.'), ('information', '.'), ('protocol', '.'), ('sample', '.'), (',', 'as'), (',', 'here'), (',', 'it'), (',', 'onto'), (',', 'whilst'), ('protocol', ','), ('driver', ','), ('if', 'the'), ('into', 'the'), ('the', 'importance'), ('where', 'the'), ('a', ')'), ('of', 'other'), ('and', 'for'), ('driver', 'and'), ('conversations', '.'), (',', 'any'), (',', 'getting'), (',', 'if'), ('me', ','), ('have', 'to'), ('in', 'a'), ('on', 'a'), ('to', 'have'), (',', 'have'), (\"'s\", 'the'), ('and', 'on'), (\"''\", '.'), ('.', '``'), ('first', '.'), ('monitors', '.'), (',', 'they'), (',', 'my'), (',', 'there'), ('themselves', ',')]\n",
      "TF-IDF matrix shape: (1, 245)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 77\n",
      "Total words: 1183\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'were', 'humiliated', 'tested', 'sitting', 'worry', 'cries', 'watching', 'positioning', 'was', 'anything', 'cocoon', 'pin', 'crimes', 'felt', 'impact', 'day', 'user', 'speeding', 'brought', 'work', 'part', 'relaxing', 'takes', 'temperature', 'buses', 'hill', 'community', 'focus', 'months', 'hair', 'did', 'come', 'transport', 'having', 'shelter', 'grooves', 'demeanor', 'tongue', 'yin', 'need', 'eyes', 'hurrying', 'fates', 'been', 'fact', 'seat', 'formed', 'shopping', 'girl', 'be', 'discovered', 'have', 'stop', 'hate', 'doubtless', 'course', 'thanks', 'place', 'idyll', 'peace', 'handbag', 'ears', 'can', 'hour', 'souls', 'boarded', 'driver', 'tried', 'feel', 'period', 'momentarily', 'fare', 'destination', 'clever', 'get', 'doing', 'starting', 'individual', 'leaving', 'engulfed', 'embarrassing', 'let', 'drew', 'chat', 'bit', 'paid', 'swear', 'bond', 'say', 'something', 'hurtle', 'Invaders', 'gets', 'passed', 'history', 'started', 'like', 'thing', 'principle', 'contact', 'position', None, 'town', 'weeks', 'City', 'person', 'bus', 'abandoned', 'nothing', 'got', 'arranging', 'Sex', 'avoid', 'Looking', 'arranged', 'vehicle', 'sparked', 'scenery', 'convey', 'Space', 'sentenced', 'journey', 'suggests', 'misjudged', 'wrestle', 'everything', 'sat', 'costs', 'smiling', 'precursor', 'ruin', 'morning', 'way', 'alright', 'done', 'gesture', 'risk', 'hurl', 'term', 'week', 'won', 'dresses', 'missed', 'hit', 'phone', 'am', 'gypsy', 'people', 'window', 'death', 'harlot', 'schoolchildren', 'mind', 'increased', 'world', 'today', 'presence', 'campaign', 'is', 'colossus', 'Yesterday', 'feels', 'controlled', 'court', 'had', 'forsake', 'superb', 'are', 'i', 'youngster', 'arrived', 'runs', 'law', 'conversation', 'idiocy', 'character', 'bound', 'running', 'run', 'look', 'appearance', 'shut', 'noticed', 'coming', 'whirligig', 'glared', 'kind', 'home', 'benefits', 'waiting', 'impression', 'comeuppance', 'someone', 'sucked', 'timeless', 'invaders', 'mystery', 'woman', 'destroy', 'ticket', 'care', 'irritation', 'neglected', 'forced', 'escalate', 'listening', 'dive', 'times', 'obliged', 'solo', 'followed', 'stormed', 'metal', 'eve', 'think', 'thinks', 'luck', 'trying', 'case', 'assuming', 'time', 'down', 'heed', 'ice', 'commute', 'define', 'ignoring', 'dyed', 'shoes', 'struggle', 'series', 'mouth', 'alarm', 'know', 'situation', 'wheels', 'existence', 'approached', 'countryside', 'jabbing', 'has', 'reading', 'behavior', 'seek', 'identify', 'somehow', 'watch', 'minute', 'pop', 'silence', 'categories', 'theory', 'legs', 'lift', 'victories', 'said', 'offer', 'turned', 'service', 'confusion', 'happened', 'days', 'absence', 'contempt', 'tube', 'delight', 'afterwards', 'attempted', 'wait', 'Snapping', 'raced', 'dance', 'pulled', 'rescue', 'friendship', 'evidence', 'lock', 'names', 'ensued', 'users', 'order', 'needed', 'music', 'forces'}\n",
      "Punctuation similarity: 0.7304902480603506\n",
      "Length similarity: 0.2540999146838563\n",
      "Active voice count: 0.6233766233766234\n",
      "Passive voice count: 0.37662337662337664\n",
      "Grammar errors count: 284\n",
      "Upper case count: 114\n",
      "Lower case count: 4253\n",
      "Number of function words: 229\n",
      "Number of nodes in graph: 517\n",
      "Phrase patterns: [('18', 'months'), ('30-45', 'minute'), ('A', 'few'), ('Karmically', 'arranged'), ('My', 'mind'), ('Of', 'course'), ('Their', 'cries'), ('Then', 'there'), ('actually', 'resemble'), ('adequately', 'comfortable'), ('arrived', 'ridiculously'), ('bus-waiting', 'position'), ('can', 'pop'), ('carefully', 'temperature'), ('case', 'any'), ('clever', 'kerbside'), ('commupance', 'today'), ('controlled', 'cocoon'), ('cosmic', 'forces'), ('crimes', 'against'), ('desperate', 'harlot'), ('di', '....'), ('diabolical', 'dance'), ('disastrous', 'impact'), ('distinct', 'history'), ('dive', 'out'), ('done', 'hit'), ('drew', 'near'), ('either', 'suggests'), ('embarrassing', 'social'), ('exponentially', 'increased'), ('favourite', 'bus-waiting'), ('feel', 'obliged'), ('few', 'weeks'), ('focus', 'why'), ('frequently', 'tested'), ('full', 'disastrous'), ('gyppo', 'behaviour'), ('gypsy-like', 'existence'), ('hardy', 'souls'), ('hate', 'other'), ('heroically', 'akimbo'), ('home', 'town'), ('hurl', 'everything'), ('ignoring', 'eachother'), ('immediately', 'afterwards'), ('insidious', 'campaign'), ('instead', 'assuming'), ('international', 'court'), ('invisible', 'bond'), ('irritation', 'followed'), ('kerbside', 'positioning'), ('latest', 'grooves'), ('legs', 'heroically'), ('lock', 'eyes'), ('long', 'period'), ('many', 'names'), ('metal', 'tube'), ('mind', 'raced'), ('minute', 'journey'), ('mobile', 'phone'), ('momentarily', 'lock'), ('months', 'ago'), (\"n't\", 'risk'), ('new', 'series'), ('nice', 'gesture'), ('now', 'define'), ('possible', 'friendship'), ('private', 'vehicle'), ('public', 'transport'), ('pulled', 'away'), ('regular', 'user'), ('resemble', 'buses'), ('respectful', 'silence'), ('ridiculously', 'early'), ('roughly', 'perpendicular'), ('run', 'onto'), ('self-important', 'co-busee'), ('shall', 'now'), ('sharp', 'focus'), ('silence', 'between'), ('slack-jawed', 'windowlickers'), ('smiling', 'smugly'), ('so', 'much'), ('somehow', 'Karmically'), ('somewhat', 'surprised'), ('speeding', 'countryside'), ('starting', 'below'), ('systematically', 'humiliated'), ('temperature', 'controlled'), ('terrible', 'thing'), ('thing', 'happened'), ('timeless', 'struggle'), ('today', 'alright'), ('tongue', 'forsake'), ('unbelievably', 'relaxing'), ('uncomfortably', 'long'), ('undoubtedly', 'arranging'), ('unfortunate', 'enough'), ('usual', 'slack-jawed'), ('waiting', 'ensued'), ('week', 'alone'), ('young', 'girl'), ('£2', 'handbag'), ('(', 'ICBL'), ('(', 'although'), ('...', 'something'), ('An', 'insidious'), ('An', 'uncomfortably'), ('And', 'though'), ('But', 'such'), ('But', 'then'), ('ICBL', ')'), ('Space', 'invaders'), ('The', 'feckless'), ('am', 'one'), ('am', 'trying'), ('arranging', '('), ('between', 'us'), ('brought', 'into'), ('by', 'putrid'), ('categorise', 'them'), ('could', \"n't\"), ('could', 'wrestle'), ('directly', 'behind'), ('doubtless', 'destroy'), ('doubtless', 'know'), ('everything', 'into'), ('feckless', 'young'), ('feckless', 'youngster'), ('forced', 'by'), ('frantically', 'jabbing'), ('good', 'will'), ('identify', 'them'), ('individual', 'started'), ('into', 'sharp'), ('law', '('), ('let', 'off'), ('merely', 'sat'), ('merely', 'sitting'), ('most', 'needed'), ('principle', ')'), ('putrid', 'invaders'), ('rudely', 'turned'), ('runs', 'directly'), ('same', 'destination'), ('schoolchildren', 'passed'), ('some', 'schoolchildren'), ('someone', 'else'), ('stormed', 'off'), ('tested', 'by'), ('their', 'luck'), ('their', 'presence'), ('they', 'systematically'), ('though', 'they'), ('transport', 'will'), ('weeks', 'passed'), ('were', 'delight'), ('will', 'identify'), ('worst', 'kind'), ('No', 'more'), ('both', 'missed'), (\"'ve\", 'missed'), ('?', 'Why'), ('All', 'users'), ('City', 'down'), ('Either', 'way'), ('Looking', 'little'), ('Oh', 'cruelest'), ('Other', 'times'), ('Why', '?'), ('about', '18'), ('about', 'anything'), ('anything', 'for'), ('avoid', 'confusion'), ('both', 'bound'), ('bound', 'for'), ('character', 'from'), ('comfortable', 'seat'), ('contact', 'or'), ('contempt', 'for'), ('destroy', 'what'), ('doing', 'nothing'), ('for', 'half'), ('from', 'Sex'), ('hurtle', 'up'), ('little', 'heed'), ('little', 'peace'), ('look', 'up'), ('not', 'having'), ('or', 'possible'), ('or', 'worse'), ('paid', 'little'), ('relaxing', 'way'), ('should', 'avoid'), ('sparked', 'up'), ('surprised', 'when'), ('window', 'seat'), ('worry', 'about'), ('wrestle', 'from'), ('you', 'can'), ('Her', 'hair'), ('In', 'case'), ('In', 'fact'), ('This', 'week'), ('are', 'busable'), ('are', 'myriad'), ('be', 'coming'), ('be', 'sucked'), ('be', 'tried'), ('be', 'won'), ('discovered', 'that'), ('hair', 'chat'), ('has', 'neglected'), ('has', 'pulled'), ('more', 'beautiful'), ('more', 'contact'), ('more', 'evidence'), ('more', 'louche'), ('must', 'be'), ('noticed', 'that'), ('only', 'di'), ('only', 'exponentially'), ('only', 'gyppo'), ('only', 'part'), ('probably', 'be'), ('risk', 'more'), ('social', 'situation'), ('such', 'are'), ('that', 'no'), ('victories', 'are'), ('who', 'actually'), ('who', 'commute'), ('who', 'dresses'), ('whole', 'situation'), ('world', 'who'), ('yan', 'that'), ('your', 'hair'), ('frantically', 'runs'), ('passed', 'us'), ('runs', 'behind'), ('some', 'kind'), ('in', 'order'), ('would', 'doubtless'), ('5', 'days'), ('She', 'gets'), ('She', 'rudely'), ('akimbo', 'like'), ('beautiful', 'days'), ('days', 'reading'), ('days', 'watching'), ('dresses', 'like'), ('eyes', 'with'), ('felt', 'we'), ('friendship', 'with'), ('glared', 'at'), ('jabbing', 'at'), ('like', 'your'), ('louche', 'days'), ('think', 'we'), ('tube', 'with'), ('vehicle', 'which'), ('wait', 'at'), ('we', \"'ve\"), ('which', 'must'), ('which', 'takes'), ('with', 'respectful'), ('with', 'wheels'), ('won', 'at'), ('something', 'was'), ('down', 'on'), ('all', 'costs'), ('all', 'except'), ('all', 'fates'), ('leaving', 'all'), ('3', 'times'), ('All', '3'), ('One', 'day'), ('different', 'about'), ('different', 'from'), ('if', 'you'), ('little', 'different'), ('someone', 'doing'), ('started', 'when'), ('were', 'both'), ('It', \"'s\"), ('!', 'Her'), ('course', '!'), ('dance', '!'), ('ears', 'as'), ('fates', '!'), ('gets', 'on'), ('handbag', '!'), ('music', 'on'), ('on', 'immediately'), ('on', 'principle'), ('open', 'as'), ('people', 'care'), ('people', 'so'), ('person', '!'), ('ticket', 'on'), ('turned', 'her'), ('people', 'who'), ('at', 'all'), ('the', 'bus'), (\"''\", 'By'), (\"'s\", 'diabolical'), (\"'s\", 'difficult'), (\"'s\", 'whirlygig'), ('....', \"''\"), ('By', 'this'), ('We', 'have'), ('``', 'let'), ('actual', '``'), ('also', 'an'), ('an', 'abandoned'), ('an', 'adequately'), ('an', 'embarrassing'), ('an', 'hour'), ('an', 'impression'), ('an', 'international'), ('an', 'invisible'), ('an', 'unbelievably'), ('define', '``'), ('formed', 'an'), ('half', 'an'), ('harlot', \"'s\"), ('have', 'attempted'), ('have', 'many'), ('non-commital', '``'), ('term', '``'), ('there', \"'s\"), ('this', 'desperate'), ('this', 'either'), ('this', 'individual'), ('this', 'metal'), ('this', 'time'), ('this', 'woman'), ('this', 'world'), ('but', 'more'), ('clear', 'that'), ('has', 'been'), ('that', 'something'), ('More', 'would'), ('against', 'me'), ('except', 'me'), ('forsake', 'me'), ('gesture', 'would'), ('had', 'discovered'), ('had', 'done'), ('had', 'dyed'), ('had', 'formed'), ('had', 'misjudged'), ('had', 'run'), ('had', 'somehow'), ('had', 'sparked'), ('hit', 'me'), ('hurrying', 'me'), ('me', 'along'), ('takes', 'me'), ('would', 'come'), ('would', 'escalate'), ('would', 'probably'), ('Why', 'did'), ('not', 'what'), ('what', 'little'), ('would', 'be'), ('I', 'had'), ('missed', 'it'), ('is', 'also'), ('is', 'frequently'), ('is', 'unmistakable'), ('morning', 'is'), ('mouth', 'was'), ('presence', 'is'), ('was', 'amiss'), ('was', 'forced'), ('was', 'open'), ('was', 'somewhat'), ('was', 'sure'), ('was', 'undoubtedly'), ('was', 'unfortunate'), ('wheels', 'is'), ('3', 'days'), ('As', 'we'), ('One', 'which'), ('conversation', 'with'), ('like', 'someone'), ('passed', 'with'), ('we', 'were'), ('offer', 'a'), ('This', 'is'), ('order', 'to'), ('did', 'my'), ('we', 'had'), ('it', \"'s\"), ('all', 'started'), ('not', 'only'), ('!', '!'), ('Instead', 'she'), ('Sex', 'in'), ('Snapping', 'it'), ('Well', 'she'), ('engulfed', 'in'), ('existence', 'in'), ('idyll', 'in'), ('in', 'theory'), ('indeed', 'it'), ('it', 'shut'), ('journey', 'in'), ('lift', 'she'), ('needed', 'it'), ('pin', 'it'), ('place', 'in'), ('reading', 'in'), ('she', 'approached'), ('she', 'boarded'), ('she', 'feels'), ('she', 'glared'), ('she', 'thinks'), ('shelter', 'in'), ('sucked', 'in'), ('swear', 'she'), ('thinks', 'she'), ('she', 'has'), ('assuming', 'my'), ('despite', 'her'), ('dyed', 'her'), ('her', '...'), ('her', 'appearance'), ('her', 'back'), ('her', 'commupance'), ('her', 'crimes'), ('her', 'idiocy'), ('her', 'mobile'), ('humiliated', 'my'), ('increased', 'my'), ('misjudged', 'her'), ('my', 'alarm'), ('my', 'best'), ('my', 'contempt'), ('my', 'ears'), ('my', 'favourite'), ('my', 'home'), ('my', 'idyll'), ('my', 'mouth'), ('my', 'self-important'), ('my', 'tongue'), ('my', 'victories'), ('ruin', 'her'), ('!', 'No'), ('as', 'if'), ('as', 'most'), ('as', 'they'), ('as', 'well'), ('most', 'people'), ('on', '3'), ('on', 'their'), ('to', 'work'), ('that', 'my'), ('people', \"''\"), ('and', 'demeanour'), ('and', 'dive'), ('and', 'indeed'), ('and', 'sentenced'), ('and', 'serious'), ('and', 'stormed'), ('and', 'watch'), ('and', 'yan'), ('appearance', 'and'), ('back', 'and'), ('myriad', 'and'), ('tried', 'and'), ('yin', 'and'), ('avoid', 'at'), ('we', 'both'), ('you', 'like'), ('to', 'offer'), (\"''\", 'were'), (\"'s\", 'clear'), (\"'s\", 'got'), ('been', '``'), ('have', 'been'), ('have', 'merely'), ('have', 'turned'), ('off', \"''\"), ('they', 'have'), ('will', 'have'), ('are', 'only'), ('has', 'only'), ('who', 'get'), ('bus', 'people'), ('To', 'me'), ('behind', 'me'), ('It', 'all'), ('all', 'times'), ('I', 'am'), ('I', 'could'), ('Perhaps', 'I'), (',', 'and'), ('.', 'This'), ('work', '.'), ('would', 'have'), ('and', 'we'), ('Yesterday', 'a'), ('a', '30-45'), ('a', 'bit'), ('a', 'carefully'), ('a', 'character'), ('a', 'distinct'), ('a', 'gypsy-like'), ('a', 'mystery'), ('a', 'new'), ('a', 'nice'), ('a', 'non-commital'), ('a', 'place'), ('a', 'pre-cursor'), ('a', 'regular'), ('a', 'terrible'), ('a', 'ticket'), ('a', 'timeless'), ('a', '£2'), ('abandoned', 'bus'), ('bus', 'arrived'), ('bus', 'community'), ('bus', 'drew'), ('bus', 'first'), ('bus', 'law'), ('bus', 'person'), ('bus', 'service'), ('no', 'bus'), ('other', 'bus'), ('pop', 'a'), ('suggests', 'a'), ('but', 'is'), ('is', 'but'), ('was', 'clear'), ('was', 'different'), ('was', 'well'), ('which', 'are'), ('which', 'has'), ('!', 'Why'), ('or', 'people'), ('you', 'people'), ('the', 'same'), ('.', 'She'), ('to', 'avoid'), (\"'s\", 'not'), ('``', 'Oh'), ('``', 'doing'), ('doing', \"''\"), ('doing', 'an'), ('.', 'All'), ('.', 'It'), ('I', 'was'), ('days', 'which'), (')', 'she'), ('before', 'it'), ('behind', 'it'), ('clear', 'she'), ('she', 'frantically'), ('she', 'got'), ('she', 'runs'), ('them', 'in'), ('in', 'this'), ('had', 'both'), ('what', 'I'), ('To', 'my'), ('got', 'her'), ('get', 'on'), ('Collossus', 'of'), ('Invaders', 'of'), ('J-Lo', 'of'), ('absence', 'of'), ('any', 'of'), ('benefits', 'of'), ('bit', 'of'), ('bond', 'of'), ('campaign', 'of'), ('come', 'of'), ('court', 'of'), ('cries', 'of'), ('cruelest', 'of'), ('evidence', 'of'), ('forces', 'of'), ('impact', 'of'), ('impression', 'of'), ('of', 'Rhodes'), ('of', 'ignoring'), ('of', 'irritation'), ('of', 'music'), ('of', 'public'), ('of', 'roughly'), ('of', 'waiting'), ('of', 'yin'), ('one', 'of'), ('out', 'of'), ('part', 'of'), ('period', 'of'), ('position', 'of'), ('souls', 'of'), ('user', 'of'), ('users', 'of'), ('whirlygig', 'of'), ('windowlickers', 'of'), ('All', 'was'), ('is', 'not'), ('attempted', 'to'), ('best', 'to'), ('commute', 'to'), ('delight', 'to'), ('difficult', 'to'), ('due', 'to'), ('enough', 'to'), ('escalate', 'to'), ('having', 'to'), ('listening', 'to'), ('mystery', 'to'), ('need', 'to'), ('neglected', 'to'), ('obliged', 'to'), ('perpendicular', 'to'), ('pre-cursor', 'to'), ('raced', 'to'), ('seek', 'to'), ('sentenced', 'to'), ('to', 'categorise'), ('to', 'clever'), ('to', 'convey'), ('to', 'death'), ('to', 'hurl'), ('to', 'hurtle'), ('to', 'look'), ('to', 'momentarily'), ('to', 'pin'), ('to', 'rescue'), ('to', 'ruin'), ('to', 'say'), ('to', 'shopping'), ('to', 'wait'), ('to', 'worry'), ('trying', 'to'), ('In', 'an'), ('hair', \"''\"), ('up', 'the'), ('.', 'In'), (',', 'before'), ('I', 'feel'), ('I', 'felt'), ('I', 'hate'), ('I', 'noticed'), ('I', 'paid'), ('I', 'said'), ('I', 'seek'), ('I', 'shall'), ('I', 'should'), ('I', 'swear'), ('I', 'think'), ('although', 'I'), ('peace', 'I'), ('time', 'I'), ('why', 'I'), ('.', 'An'), ('.', 'And'), ('.', 'As'), ('.', 'But'), ('.', 'One'), ('.', 'Perhaps'), ('.', 'To'), ('us', '.'), ('me', 'are'), ('situation', 'would'), ('I', 'get'), ('Why', 'she'), ('it', 'did'), ('it', 'down'), ('stop', 'it'), ('when', 'she'), ('and', 'I'), ('a', 'conversation'), ('a', 'feckless'), ('got', 'a'), ('same', 'bus'), (\"'s\", 'days'), ('She', \"'s\"), ('``', 'She'), ('with', 'this'), ('in', 'the'), ('about', 'her'), ('for', 'her'), ('from', 'my'), ('her', 'day'), ('her', 'down'), ('her', 'way'), ('my', 'day'), ('my', 'stop'), ('Maybe', 'the'), ('Namely', 'the'), ('approached', 'the'), ('arranged', 'the'), ('boarded', 'the'), ('convey', 'the'), ('feels', 'the'), ('onto', 'the'), ('rescue', 'the'), ('the', '5'), ('the', 'City'), ('the', 'Collossus'), ('the', 'J-Lo'), ('the', 'absence'), ('the', 'actual'), ('the', 'benefits'), ('the', 'cosmic'), ('the', 'driver'), ('the', 'fare'), ('the', 'full'), ('the', 'hardy'), ('the', 'hill'), ('the', 'latest'), ('the', 'lift'), ('the', 'morning'), ('the', 'need'), ('the', 'private'), ('the', 'running'), ('the', 'scenery'), ('the', 'shelter'), ('the', 'speeding'), ('the', 'term'), ('the', 'usual'), ('the', 'whole'), ('the', 'window'), ('the', 'worst'), ('watch', 'the'), ('watching', 'the'), ('``', 'bus'), ('have', 'a'), ('and', 'not'), ('seat', 'and'), ('get', 'the'), ('the', 'situation'), ('She', 'had'), ('She', 'would'), ('at', 'me'), ('.', 'I'), ('I', 'like'), ('as', 'people'), (',', 'or'), ('Oh', ','), ('seat', ','), ('is', 'a'), ('.', 'Oh'), ('day', '.'), ('her', 'hair'), ('!', \"''\"), ('!', '``'), (\"''\", 'as'), ('as', 'this'), ('on', 'an'), ('kind', 'of'), ('of', 'conversation'), ('of', 'merely'), (',', 'I'), ('bus', 'stop'), ('up', 'a'), (',', 'as'), ('been', 'to'), ('started', 'to'), ('to', 'some'), ('to', 'someone'), ('of', 'this'), ('me', 'as'), ('people', 'would'), ('like', 'it'), (',', 'brought'), (',', 'despite'), (',', 'due'), (',', 'engulfed'), (',', 'good'), (',', 'hurrying'), (',', 'instead'), (',', 'leaving'), (',', 'legs'), (',', 'listening'), (',', 'smiling'), (',', 'starting'), (',', 'superb'), (',', 'thanks'), ('Anyway', ','), ('Basically', ','), ('However', ','), ('Once', ','), ('Yes', ','), ('afterwards', ','), ('ago', ','), ('alarm', ','), ('alone', ','), ('away', ','), ('behaviour', ','), ('busable', ','), ('care', ','), ('co-busee', ','), ('cocoon', ','), ('destination', ','), ('early', ','), ('ensued', ','), ('fact', ','), ('fare', ','), ('first', ','), ('girl', ','), ('heed', ','), ('hill', ','), ('know', ','), ('names', ','), ('near', ','), ('nothing', ','), ('said', ','), ('sat', ','), ('scenery', ','), ('series', ','), ('service', ','), ('shopping', ','), ('shut', ','), ('sitting', ','), ('sure', ','), ('thanks', ','), ('then', ','), ('theory', ','), ('town', ','), ('worse', ','), ('of', 'the'), ('.', '1'), ('.', 'A'), ('.', 'Anyway'), ('.', 'Basically'), ('.', 'Either'), ('.', 'However'), ('.', 'Instead'), ('.', 'Invaders'), ('.', 'Looking'), ('.', 'Maybe'), ('.', 'More'), ('.', 'My'), ('.', 'Namely'), ('.', 'Of'), ('.', 'Once'), ('.', 'Other'), ('.', 'Snapping'), ('.', 'Space'), ('.', 'The'), ('.', 'Their'), ('.', 'Then'), ('.', 'We'), ('.', 'Well'), ('.', 'Yes'), ('.', 'Yesterday'), ('1', '.'), ('Rhodes', '.'), ('along', '.'), ('alright', '.'), ('amiss', '.'), ('below', '.'), ('buses', '.'), ('chat', '.'), ('coming', '.'), ('community', '.'), ('confusion', '.'), ('costs', '.'), ('countryside', '.'), ('death', '.'), ('demeanour', '.'), ('driver', '.'), ('eachother', '.'), ('else', '.'), ('followed', '.'), ('grooves', '.'), ('happened', '.'), ('history', '.'), ('hour', '.'), ('idiocy', '.'), ('luck', '.'), ('much', '.'), ('phone', '.'), ('positioning', '.'), ('running', '.'), ('say', '.'), ('serious', '.'), ('smugly', '.'), ('struggle', '.'), ('superb', '.'), ('unmistakable', '.'), ('woman', '.'), ('youngster', '.'), ('at', 'her'), ('at', 'my'), ('with', 'her'), ('And', 'I'), ('I', 'most'), ('if', 'I'), ('hair', '.'), ('I', 'have'), ('``', 'I'), ('In', 'a'), ('bus', 'has'), ('is', 'an'), ('me', ','), ('As', 'the'), ('before', 'the'), ('by', 'the'), ('into', 'the'), ('of', 'what'), ('of', 'you'), ('I', 'would'), ('as', 'she'), ('to', 'stop'), ('way', 'to'), (',', 'which'), ('days', '.'), ('with', 'a'), ('have', 'it'), ('in', 'an'), ('it', \"''\"), ('on', 'and'), ('I', 'did'), ('when', 'I'), ('of', 'hair'), ('she', 'would'), ('of', 'her'), ('of', 'my'), ('get', 'to'), ('situation', 'to'), ('to', 'get'), (',', 'a'), ('avoid', 'the'), ('for', 'the'), ('from', 'the'), ('missed', 'the'), ('the', 'seat'), ('times', 'the'), ('it', 'is'), ('it', 'was'), ('she', 'is'), ('she', 'was'), (',', 'she'), (',', 'but'), (',', 'frantically'), (',', 'if'), (',', 'some'), (',', 'their'), ('invaders', ','), ('off', ','), ('well', ','), (')', '.'), ('.', 'No'), ('conversation', '.'), ('invaders', '.'), ('kind', '.'), ('them', '.'), ('well', '.'), ('I', 'only'), ('are', 'the'), ('of', 'all'), ('which', 'I'), ('of', 'a'), ('bus', 'would'), ('to', 'all'), ('me', '.'), (',', 'about'), (',', 'doing'), (',', 'when'), (',', 'you'), ('day', ','), ('stop', ','), ('times', ','), ('way', ','), ('when', ','), ('bus', 'is'), ('.', 'Why'), ('did', '.'), ('down', '.'), ('stop', '.'), ('times', '.'), ('way', '.'), ('like', 'the'), ('of', '``'), ('bus', ','), (\"''\", 'to'), ('to', 'have')]\n",
      "TF-IDF matrix shape: (1, 361)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 32\n",
      "Total words: 545\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'were', 'outfit', 'German', 'Rhyming', 'holidaying', 'crass', 'pin', 'Time', 'surface', 'brilliance', 'work', 'dancing', 'call', 'singer', 'sound', 'refreshing', 'smörgåsbord', 'Opinion', 'Well', 'type', 'established', 'flashes', 'having', 'parts', 'cinemas', 'reason', 'spectacles', 'releasing', 'heard', 'extols', 'indie', 'instance', 'be', 'baroque', 'have', 'debut', 'lyrics', 'absurdity', 'bass', 'wear', 'snatches', 'driver', 'erectile', 'let', 'benefit', 'something', 'make', 'Indie', None, 'craftwork', 'influences', 'justice', 'artists', 'vervain', 'approach', 'stand', 'toffee', 'christened', 'instances', 'take', 'corner', 'way', 'Difficult', 'density', 'decide', 'Idiot', 'mean', 'makes', 'called', 'comparisons', 'Let', 'Pixies', 'sopranos', 'Music', 'undercurrent', 'mind', 'tracks', 'assassinating', 'is', 'debt', 'album', 'example', 'frank', 'are', 'painted', 'polished', 'Stroke', 'run', 'look', 'tapped', 'well', 'kind', 'lines', 'Start', 'section', 'funkiness', 'do', 'men', 'zenith', 'moments', 'taken', 'funky', 'barnstorming', 'cut', 'read', 'recall', 'bothered', 'romp', 'chorus', 'eve', 'Reviews', 'tell', 'owe', 'go', 'tells', 'spells', 'words', 'in', 'rounded', 'Television', 'want', 'ale', 'time', 'Pop', 'sounds', 'and', 'has', 'themes', 'pumping', 'joy', 'friend', 'blonde', 'hammer', 'virtues', 'does', 'Tom', 'said', 'adrenaline', 'displays', 'reader', 'Them', 'mix', 'ordinand', 'Archdukes', 'Wars', 'meaning', 'used', 'Any', 'page', 'cutis', 'piggy', 'guitar', 'music', 'opener', 'conjure', 'darkened', 'fight'}\n",
      "Punctuation similarity: 0.8375157251846441\n",
      "Length similarity: 0.3785112359550562\n",
      "Active voice count: 0.59375\n",
      "Passive voice count: 0.40625\n",
      "Grammar errors count: 157\n",
      "Upper case count: 81\n",
      "Lower case count: 1947\n",
      "Number of function words: 111\n",
      "Number of nodes in graph: 289\n",
      "Phrase patterns: [(\"'re\", 'Good'), ('A', 'well'), ('Alex', 'Kapranos'), ('Any', 'Wars'), ('But', 'what'), ('Either', 'way'), ('Franz', 'Fedinand'), ('Ian', 'Curtis'), ('Iggy', 'Pop'), ('In', 'this'), ('Kapranos', 'sounds'), ('Michael', 'conjure'), ('My', 'Opinion'), ('Nand', 'stand'), ('Not', 'Start'), ('Over', 'Them'), ('Parallel-Lines', 'zenith'), ('Scottish', 'indie'), ('So', 'we'), ('Start', 'Any'), ('Tom', 'Verlaine'), ('Wars', 'Over'), ('Weimar', 'atmosphere'), ('adreneline', 'pumping'), ('also', 'recall'), ('an', 'undercurrent'), ('barnstorming', 'chorus'), ('bass', 'lines'), ('be', 'bothered'), ('big', 'debt'), ('by', 'releasing'), ('called', 'Michael'), ('coquetteish', 'Ian'), ('corner', 'by'), ('cut', 'through'), ('darkened', 'cinemas'), ('decide', 'whether'), ('far', 'too'), ('from', 'here'), ('full-on', 'absurdity'), ('funkiness', 'cut'), ('hard', 'enough'), ('just', 'painted'), ('lead', 'singer'), ('left-field', 'artists'), ('less', 'normal'), ('loosely', 'nowadays'), ('main', 'reason'), ('me', 'after'), ('melodious', 'guitar'), ('men', 'called'), ('more', 'coquetteish'), ('multi-layered', 'romp'), ('my', 'mind'), ('new', 'section'), ('norm', 'hard'), ('not', 'full-on'), ('other', 'instances'), ('painted', 'themselves'), ('particularly', 'Weimar'), ('peculiarly', 'teutonic'), ('pin', 'down'), ('pop-artful', 'approach'), ('pretty', 'crass'), ('pumping', 'brilliance'), ('real', 'meaning'), ('really', 'fast'), ('recall', 'Blondie'), ('releasing', 'something'), ('rounded', 'debut'), ('run', 'really'), ('singer', 'Alex'), ('such', 'example'), ('teutonic', 'baroque'), ('themselves', 'into'), ('this', 'instance'), ('time', 'tells'), ('toffee', 'hammer'), ('too', 'loosely'), ('used', 'far'), ('well', 'rounded'), (\"'ll\", 'tell'), (\"'ve\", 'established'), (\"'ve\", 'read'), ('(', 'Kraftwerk'), ('Fedinand', '('), ('Kraftwerk', ')'), ('Time', 'will'), ('You', 'work'), ('album', 'also'), ('christened', 'them'), ('fight', 'or'), ('first', 'album'), ('friend', 'said'), ('guitar', 'work'), ('has', 'kind'), ('having', 'said'), ('heard', 'comparisons'), ('indie', 'type'), ('mix', 'spells'), ('one', 'such'), ('or', 'run'), ('polished', 'so'), ('self-consciously', 'skewed'), ('skewed', 'funkiness'), ('slightly', 'non-conformist'), ('slightly', 'self-consciously'), ('so', 'polished'), ('so', 'soon'), ('something', 'so'), ('spells', 'funky'), ('them', 'justice'), ('type', 'outfit'), ('we', \"'ve\"), ('will', 'call'), ('Blondie', 'at'), ('German', 'on'), ('Opinion', 'on'), ('Them', 'Well'), ('They', \"'re\"), ('They', 'might'), ('They', 'owe'), ('Where', 'do'), ('any', 'Archdukes'), ('any', 'real'), ('assassinating', 'any'), ('at', 'least'), ('best', 'music'), ('crass', 'on'), ('dancing', 'with'), ('for', 'words'), ('go', 'assassinating'), ('go', 'from'), ('here', '?'), ('joy', 'for'), ('look', 'at'), ('makes', 'you'), ('mind', 'at'), ('on', 'Music'), ('reason', 'for'), ('stand', 'out'), ('their', 'Parallel-Lines'), ('their', 'influences'), ('through', 'with'), ('with', 'men'), ('you', 'want'), ('are', 'moments'), ('are', 'progressive'), ('ca', \"n't\"), ('have', 'just'), ('influences', 'are'), ('justice', '-'), ('like', 'darkened'), ('might', 'have'), (\"n't\", 'be'), (\"n't\", 'christened'), ('romp', 'which'), ('some', 'melodious'), ('some', 'parts'), ('sounds', 'like'), ('surface', '-'), ('themes', 'like'), ('which', 'displays'), ('which', 'extols'), ('(', 'or'), ('That', 'does'), ('That', 'kind'), ('spells', 'good'), ('them', ')'), ('will', 'tell'), ('but', 'Let'), ('but', 'has'), ('but', 'themes'), ('but', 'until'), (',', 'but'), ('I', \"'ll\"), ('I', 'ca'), ('I', 'look'), ('Idiot', 'in'), ('Pixies', 'in'), ('call', 'it'), ('in', 'my'), ('in', 'other'), ('it', 'less'), ('take', 'it'), ('whether', 'it'), ('any', 'good'), ('do', 'them'), ('do', 'they'), ('music', 'does'), ('music', 'spells'), ('tell', 'you'), ('their', 'sound'), ('they', 'go'), ('driver', 'that'), ('established', 'that'), ('mean', 'that'), ('that', 'makes'), ('that', 'time'), ('until', 'that'), ('flashes', 'of'), ('kind', 'of'), ('The', \"'Nand\"), ('The', \"'Werk\"), ('The', 'Idiot'), ('The', 'Nand'), ('The', 'Pixies'), ('The', 'barnstorming'), ('The', 'opener'), (')', 'are'), ('album', '-'), ('does', \"n't\"), ('skewed', '-'), ('some', 'tracks'), ('sound', 'like'), ('sound', 'which'), ('they', 'are'), (\"'Nand\", \"''\"), (\"'Werk\", \"''\"), ('Difficult', 'to'), ('Indie', \"''\"), ('Jacqueline', \"''\"), ('Music', \"''\"), ('Or', '``'), ('Reviews', \"''\"), ('Rhyming', '``'), ('This', 'is'), ('``', 'Indie'), ('``', 'Jacqueline'), ('``', 'My'), ('``', 'Reviews'), ('``', 'Where'), ('``', 'erecticles'), ('``', 'spectacles'), ('approach', 'to'), ('debt', 'to'), ('enough', 'to'), ('erecticles', \"''\"), ('holidaying', 'is'), ('is', 'an'), ('is', 'another'), ('is', 'pretty'), ('is', 'probably'), ('is', 'refreshing'), ('is', 'used'), ('nowadays', 'to'), ('opener', '``'), ('spectacles', \"''\"), ('to', 'Tom'), ('to', 'lyrics'), ('to', 'me'), ('to', 'mean'), ('to', 'pin'), ('want', 'to'), ('what', 'is'), ('?', 'Well'), ('Well', 'there'), ('at', 'music'), ('at', 'their'), ('go', 'out'), ('out', '?'), ('-', '``'), ('which', 'is'), (\"''\", 'is'), (\"'s\", 'Not'), (\"'s\", 'first'), (\"'s\", 'not'), (\"'s\", 'snatches'), (\"'s\", 'take'), ('It', \"'s\"), ('Let', \"'s\"), ('Pop', \"'s\"), ('Stroke', \"'s\"), ('There', \"'s\"), ('Verlaine', 'and'), ('What', \"'s\"), ('and', 'Television'), ('and', 'bass'), ('and', 'dancing'), ('and', 'decide'), ('and', 'heard'), ('and', 'tapped'), ('cinemas', 'and'), ('funky', 'and'), ('hammer', 'and'), ('let', \"'s\"), ('progressive', 'and'), ('read', 'and'), ('words', 'and'), ('but', 'as'), ('I', \"'ve\"), ('I', 'will'), ('as', 'I'), ('make', 'it'), ('work', 'it'), ('-', 'there'), ('do', \"n't\"), ('have', 'any'), ('like', '?'), ('like', 'all'), (\"n't\", 'do'), (\"n't\", 'go'), ('on', 'some'), ('there', 'are'), ('with', 'some'), ('tracks', ','), ('a', 'Scottish'), ('a', 'big'), ('a', 'corner'), ('a', 'fight'), ('a', 'friend'), ('a', 'joy'), ('a', 'more'), ('a', 'multi-layered'), ('a', 'new'), ('a', 'particularly'), ('a', 'peculiarly'), ('a', 'smorgasbord'), ('a', 'toffee'), ('conjure', 'a'), ('displays', 'a'), ('into', 'a'), ('owe', 'a'), ('taken', 'a'), ('for', 'the'), ('are', 'a'), ('benefit', 'of'), ('density', 'of'), ('moments', 'of'), ('of', 'German'), ('of', 'Iggy'), ('of', 'adreneline'), ('of', 'holidaying'), ('of', 'left-field'), ('of', 'taken'), ('parts', 'of'), ('section', 'of'), ('smorgasbord', 'of'), ('snatches', 'of'), ('undercurrent', 'of'), ('virtues', 'of'), ('said', 'that'), ('that', 'make'), ('that', 'one'), ('that', 'they'), ('type', 'that'), ('.', 'That'), ('it', \"'s\"), ('The', 'type'), ('make', 'The'), ('have', \"n't\"), ('after', 'the'), ('extols', 'the'), ('probably', 'the'), ('tapped', 'the'), ('the', 'Stroke'), ('the', 'benefit'), ('the', 'best'), ('the', 'density'), ('the', 'driver'), ('the', 'main'), ('the', 'mix'), ('the', 'norm'), ('the', 'page'), ('the', 'pop-artful'), ('the', 'reader'), ('the', 'surface'), ('the', 'virtues'), (\"''\", '('), (\"''\", 'as'), ('comparisons', 'to'), ('is', 'one'), ('or', '``'), ('said', 'to'), ('to', 'make'), ('?', 'I'), ('all', 'in'), ('in', 'all'), ('it', 'out'), (',', 'in'), ('``', 'The'), ('Well', ','), (\"'s\", 'flashes'), (\"'s\", 'slightly'), ('and', 'slightly'), ('and', 'sound'), ('work', 'and'), ('that', 'their'), ('.', 'I'), ('.', 'They'), ('-', 'it'), ('I', 'have'), (',', 'Franz'), (',', 'having'), (',', 'lead'), (',', 'let'), ('Bizzarely', ','), ('Good', ','), ('However', ','), ('Then', ','), ('Yeah', ','), ('absurdity', ','), ('atmosphere', ','), ('chorus', ','), ('debut', ','), ('down', ','), ('fast', ','), ('instance', ','), ('instances', ','), ('page', ','), ('tells', ','), ('way', ','), ('as', 'a'), (',', 'which'), (\"''\", 'with'), ('?', \"''\"), ('there', 'is'), ('to', 'go'), ('with', '``'), ('in', 'the'), ('of', 'skewed'), ('.', 'A'), ('.', 'Bizzarely'), ('.', 'But'), ('.', 'Difficult'), ('.', 'Either'), ('.', 'However'), ('.', 'In'), ('.', 'It'), ('.', 'Or'), ('.', 'Rhyming'), ('.', 'So'), ('.', 'Then'), ('.', 'There'), ('.', 'This'), ('.', 'Time'), ('.', 'What'), ('.', 'Yeah'), ('.', 'You'), ('Archdukes', '.'), ('Curtis', '.'), ('Television', '.'), ('another', '.'), ('artists', '.'), ('baroque', '.'), ('bothered', '.'), ('brilliance', '.'), ('example', '.'), ('least', '.'), ('lines', '.'), ('lyrics', '.'), ('meaning', '.'), ('non-conformist', '.'), ('normal', '.'), ('outfit', '.'), ('reader', '.'), ('refreshing', '.'), ('soon', '.'), ('zenith', '.'), (\"'s\", 'any'), (\"'s\", 'their'), ('music', 'and'), ('out', 'and'), ('there', \"'s\"), ('.', 'The'), ('of', 'the'), ('the', 'album'), ('the', 'comparisons'), ('the', 'tracks'), (\"''\", '.'), ('to', 'have'), ('and', 'have'), ('of', 'you'), (',', 'flashes'), ('good', ','), ('tell', ','), ('all', 'the'), ('on', 'the'), ('the', 'music'), ('have', 'a'), ('like', 'a'), ('it', '``'), ('it', 'to'), ('of', 'some'), ('some', 'of'), (')', '.'), ('comparisons', '.'), ('does', '.'), ('good', '.'), ('one', '.'), (',', 'all'), (',', 'do'), (',', 'for'), (',', 'there'), ('all', ','), ('you', ','), ('in', 'a'), (\"'s\", 'The'), ('to', '``'), ('.', 'Well'), ('out', '.'), ('you', '.'), (',', 'like'), (',', 'some'), ('of', 'The')]\n",
      "TF-IDF matrix shape: (1, 186)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7\n",
      "Total words: 121\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'think', 'sets', 'go', 'today', 'is', 'missives', 'anything', 'going', 'make', 'missive', 'statements', 'hinges', 'write', 'ways', 'know', 'being', 'suppose', 'halt', 'agency', 'sentence', 'grind', 'continue', 'confusion', 'do', 'tracts', 'length', 'teetering', 'be', 'turn', 'have', 'ground', 'axis', 'times'}\n",
      "Punctuation similarity: 0.6801388888888888\n",
      "Length similarity: 0.3913894324853229\n",
      "Active voice count: 0.42857142857142855\n",
      "Passive voice count: 0.5714285714285714\n",
      "Grammar errors count: 33\n",
      "Upper case count: 9\n",
      "Lower case count: 433\n",
      "Number of function words: 21\n",
      "Number of nodes in graph: 79\n",
      "Phrase patterns: [('It', 'would'), ('Nearly', 'ground'), ('after', 'that'), ('agency', 'which'), ('an', 'entertaining'), ('and', 'statements'), ('being', 'able'), ('central', 'agency'), ('continue', 'for'), ('go', 'one'), ('hinges', 'now'), ('how', 'long'), ('into', 'an'), ('just', 'grind'), ('keep/delete', 'axis'), ('know', 'if'), ('least', 'prevent'), ('or', 'it'), ('prevent', 'confusion'), ('should', 'be'), ('teetering', 'precariously'), ('that', 'sentence'), ('times', 'like'), ('tracts', 'and'), ('two', 'ways'), ('which', 'sets'), ('If', 'not'), ('at', 'least'), ('at', 'times'), ('ca', \"n't\"), ('confusion', 'at'), ('do', \"n't\"), ('entertaining', 'missive'), ('halt', 'after'), ('if', 'there'), ('is', 'going'), ('it', 'will'), (\"n't\", 'know'), (\"n't\", 'think'), ('not', 'being'), ('sets', 'the'), ('the', 'keep/delete'), ('the', 'length'), ('there', 'should'), ('will', 'just'), ('will', 'turn'), ('would', 'at'), ('write', 'today'), ('a', 'halt'), ('length', 'of'), ('now', 'on'), ('of', 'missives'), ('of', 'two'), ('on', 'how'), ('one', 'of'), ('precariously', 'on'), ('think', 'of'), ('to', 'write'), ('a', 'central'), ('there', 'is'), ('write', 'anything'), (',', 'or'), (',', 'so'), (',', 'teetering'), (',', 'tracts'), ('Either', 'I'), ('I', 'ca'), ('I', 'do'), ('I', 'have'), ('I', 'suppose'), ('like', 'this'), ('long', 'I'), ('make', 'this'), ('missives', ','), ('so', 'this'), ('suppose', 'this'), ('this', 'hinges'), ('this', 'into'), ('today', ','), ('turn', 'this'), ('missive', 'on'), ('of', 'anything'), ('on', 'not'), ('on', 'the'), ('.', 'Either'), ('.', 'If'), ('.', 'It'), ('.', 'Nearly'), ('able', 'to'), ('axis', '.'), ('be', '.'), ('for', 'to'), ('going', 'to'), ('grind', 'to'), ('ground', 'to'), ('have', 'to'), ('sentence', '.'), ('statements', '.'), ('to', 'continue'), ('to', 'go'), ('to', 'make'), ('ways', '.'), ('to', 'a'), ('a', 'missive'), ('is', 'a'), ('.', 'I'), (',', 'there'), ('I', 'will'), ('anything', ','), ('halt', ','), ('not', ','), ('this', 'is'), ('anything', 'to'), ('missive', '.'), ('this', 'a')]\n",
      "TF-IDF matrix shape: (1, 37)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 11\n",
      "Total words: 156\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'people', 'leaflets', 'think', 'things', 'get', 'today', 'atmosphere', 'lot', 'time', 'anything', 'say', 'franchise', 'organizing', 'spring', 'network', 'thing', 'organizers', 'loneliness', 'work', 'know', 'thought', 'Lonely', 'ending', 'friend', 'eradicating', 'meeting', 'push', 'response', 'did', 'suppose', 'someone', 'Spend', 'coordinators', 'turned', 'do', 'Club', 'be', 'none', 'edge', 'forming', 'seemed', 'send', 'Clubs', 'plantation'}\n",
      "Punctuation similarity: 0.8246173469387755\n",
      "Length similarity: 0.41243315508021394\n",
      "Active voice count: 0.8181818181818182\n",
      "Passive voice count: 0.18181818181818182\n",
      "Grammar errors count: 45\n",
      "Upper case count: 27\n",
      "Lower case count: 573\n",
      "Number of function words: 28\n",
      "Number of nodes in graph: 101\n",
      "Phrase patterns: [('-', 'an'), ('Co-ordinators', '-'), ('Not', 'that'), ('about', 'forming'), ('an', 'entire'), ('and', 'ending'), ('anything', 'more'), ('compassionate', 'thing'), ('did', \"n't\"), ('entire', 'network'), ('eradicating', 'loneliness'), ('good', 'response'), ('if', 'no-one'), ('leaflets', 'which'), ('loneliness', 'forever'), ('meeting', 'and'), ('more', 'tragic'), ('no-one', 'turned'), ('non-threatening', 'atmosphere'), ('organisers', 'over'), ('people', 'together'), ('push', 'some'), ('say', 'things'), ('someone', 'organising'), ('than', 'someone'), ('things', 'like'), ('time', 'on'), ('today', 'about'), ('together', 'in'), ('totally', 'alone'), ('tragic', 'than'), ('what', 'if'), ('which', 'say'), ('work', 'with'), ('you', 'know'), ('?', \"''\"), (\"''\", 'or'), (\"'m\", 'lonely'), ('And', 'then'), ('But', 'then'), ('Could', 'there'), ('To', 'get'), ('``', 'Spend'), ('be', 'anything'), ('but', 'it'), ('franchise', 'it'), ('it', 'seemed'), ('like', '``'), ('lonely', 'people'), ('lot', 'of'), (\"n't\", 'get'), ('network', 'of'), ('of', 'time'), ('on', 'the'), ('or', '``'), ('out', 'leaflets'), ('over', 'the'), ('send', 'out'), ('so', 'they'), ('suppose', 'they'), ('the', 'Playstation'), ('the', 'edge'), ('there', 'would'), ('they', 'did'), ('thing', 'to'), ('think', 'there'), ('thought', 'today'), ('to', 'do'), ('to', 'other'), ('would', 'be'), ('Lonely', 'Club'), ('?', 'Could'), ('Club', 'meeting'), ('Club', 'organisers'), ('Playstation', '?'), ('ending', 'up'), ('spring', 'up'), ('turned', 'up'), ('up', 'totally'), ('I', 'thought'), ('then', 'I'), ('get', 'lonely'), ('it', 'out'), ('there', 'be'), ('Clubs', 'could'), ('That', 'could'), ('could', 'franchise'), ('could', 'push'), ('could', 'send'), ('could', 'spring'), ('could', 'work'), (',', 'but'), (',', 'eradicating'), (',', 'so'), (',', 'what'), (',', 'you'), ('friend', ','), ('know', ','), ('I', \"'m\"), ('I', 'suppose'), ('I', 'think'), ('Lonely', 'Clubs'), ('Lonely', 'Co-ordinators'), ('Spend', 'a'), ('a', 'compassionate'), ('a', 'friend'), ('a', 'good'), ('a', 'lot'), ('a', 'non-threatening'), ('forming', 'a'), ('in', 'a'), ('organising', 'a'), ('other', 'Lonely'), ('seemed', 'a'), ('some', 'Lonely'), ('that', 'I'), ('with', 'a'), ('.', 'I'), ('.', 'And'), ('.', 'But'), ('.', 'Not'), ('.', 'That'), ('.', 'To'), ('alone', '.'), ('atmosphere', '.'), ('do', '.'), ('edge', '.'), ('forever', '.'), ('response', '.'), ('up', '?'), ('I', 'could'), ('they', 'could'), (',', 'to'), ('lonely', ','), ('out', ','), ('thought', ','), ('a', 'Lonely'), ('``', 'Lonely'), ('be', 'a'), ('get', 'Lonely'), ('of', 'Lonely'), (\"''\", '.'), ('up', ','), ('Lonely', '?'), ('Club', '.')]\n",
      "TF-IDF matrix shape: (1, 46)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4\n",
      "Total words: 51\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'posted', 'have', 'Digital', 'get', 'go', 'picture', 'need', 'know', None, 'Camera', 'pictures', 'got', 'figured', 'explore', 'post'}\n",
      "Punctuation similarity: 0.6423611111111112\n",
      "Length similarity: 0.4137323943661972\n",
      "Active voice count: 1.0\n",
      "Passive voice count: 0.0\n",
      "Grammar errors count: 15\n",
      "Upper case count: 30\n",
      "Lower case count: 150\n",
      "Number of function words: 8\n",
      "Number of nodes in graph: 40\n",
      "Phrase patterns: [('Digital', 'Camera'), ('a', 'picture'), ('all', 'figured'), ('figured', 'out'), ('get', 'pictures'), ('got', 'my'), ('here', 'now'), ('hopefully', 'the'), ('it', 'all'), ('just', 'need'), ('know', 'how'), ('my', 'Digital'), ('next', 'post'), ('on', 'here'), ('out', ','), ('pictures', 'posted'), ('post', 'will'), ('posted', 'up'), ('pretty', 'much'), ('the', 'next'), ('up', 'on'), ('.', 'WEEEEEEEEEEEEEEEEEEEEE'), ('Camera', 'and'), ('So', 'off'), ('and', 'hopefully'), ('explore', 'and'), ('have', 'a'), ('have', 'it'), ('much', 'have'), ('now', '.'), ('picture', '.'), ('will', 'have'), ('!', '!'), ('WEEEEEEEEEEEEEEEEEEEEE', '!'), ('go', 'to'), ('how', 'to'), ('need', 'to'), ('to', 'explore'), ('to', 'get'), ('to', 'know'), (',', 'I'), ('I', 'go'), ('I', 'got'), ('I', 'just'), ('I', 'pretty'), ('off', 'I'), ('.', 'So'), ('So', 'I'), ('and', 'I')]\n",
      "TF-IDF matrix shape: (1, 15)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 10\n",
      "Total words: 314\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'wanting', 'bunch', 'of', 'was', 'Knife', 'please', 'keeps', 'tonight', 'told', 'day', 'work', 'Yellow', 'Well', 'hair', 'been', 'songs', 'be', 'have', 'hour', 'Orgasmic', 'Least', 'feel', 'get', 'chirpy', 'bit', 'bonus', None, 'town', 'see', 'lack', 'nagana', 'warm', 'Did', 'everything', 'weekend', 'take', 'wont', 'done', 'huh', 'Digital', 'shower', 'ON', 'cousin', 'am', 'humid', 'mood', 'Boss', 'playing', 'today', 'Camera', 'is', 'getting', 'going', 'radio', 'are', 'brother', 'look', 'bed', 'Falls', 'drinks', 'DAY', 'money', 'lunch', 'TIME', 'road', 'do', 'school', 'talking', 'son', 'left', 'dunno', 'think', 'things', 'go', 'lot', 'want', 'time', 'dad', 'sleep', 'wake', 'alert', 'moved', 'does', 'timing', 'THIS', 'seemed', 'cause'}\n",
      "Punctuation similarity: 0.7970693618171985\n",
      "Length similarity: 0.25938461538461544\n",
      "Active voice count: 0.4\n",
      "Passive voice count: 0.6\n",
      "Grammar errors count: 87\n",
      "Upper case count: 51\n",
      "Lower case count: 995\n",
      "Number of function words: 67\n",
      "Number of nodes in graph: 172\n",
      "Phrase patterns: [(\"'s\", 'ok'), ('...', 'Well'), ('4:00', 'a.m.'), ('Digital', 'Camera'), ('LOL', 'Ok'), ('Niagara', 'Falls'), ('ON', 'TIME'), ('Prefect', 'timing'), ('THE', 'DAY'), ('THIS', 'second'), ('Yellow', 'Knife'), ('be', 'picture'), ('bed', 'even'), ('been', 'wanting'), ('chirpy', 'mood'), ('does', \"n't\"), ('drinks', 'tonight'), ('even', 'though'), ('ever', 'wake'), ('favorite', 'songs'), ('feel', 'guilty'), ('full', 'hour'), ('hair', 'curly'), ('hour', 'lunch'), ('look', 'decent'), ('mood', '...'), ('mostly', 'humid'), ('now', 'Niagara'), ('ok', 'cause'), ('practically', 'flawless'), ('rarely', 'see'), ('ready', 'ON'), ('see', 'her'), ('she', 'moved'), ('sleep', 'talking'), ('sunny', 'outside'), ('then', 'Yellow'), ('think', 'about'), ('timing', 'because'), ('up', 'one'), ('wake', 'up'), ('was', 'THE'), ('wont', 'feel'), ('And', 'last'), ('Boss', 'told'), ('Did', 'you'), ('about', 'all'), ('all', 'playing'), ('everything', 'just'), ('get', 'out'), ('getting', 'better'), ('getting', 'quite'), ('good', 'weekend'), ('guilty', 'this'), ('in', 'such'), ('in', 'town'), ('its', 'warm'), ('just', 'keeps'), ('just', 'seemed'), ('maybe', 'Least'), (\"n't\", 'take'), ('or', 'maybe'), ('picture', 'time'), ('really', 'been'), ('right', 'THIS'), ('songs', 'are'), ('take', 'much'), ('totally', 'right'), ('us', 'are'), ('well', 'back'), ('will', 'get'), ('you', 'ever'), ('you', 'look'), ('?', 'LOL'), ('?', 'You'), ('You', 'actually'), ('actually', 'want'), ('curly', 'today'), ('go', 'totally'), ('great', 'day'), ('huh', '?'), ('keeps', 'on'), ('left', 'my'), ('me', 'huh'), ('my', 'bonus'), ('my', 'dad'), ('my', 'hair'), ('on', 'Monday'), ('one', 'day'), ('playing', 'on'), ('please', 'me'), ('so', 'with'), ('that', 'Digital'), ('the', 'radio'), ('the', 'road'), ('the', 'things'), ('today', 'was'), ('told', 'me'), ('tonight', 'so'), ('wanting', 'that'), ('with', 'my'), ('work', 'done'), ('.', 'My'), ('DAY', 'for'), ('Its', 'not'), ('TIME', 'for'), ('bit', 'of'), ('bunch', 'of'), ('for', 'drinks'), ('for', 'school'), ('lack', 'of'), ('not', 'least'), ('not', 'sunny'), ('of', 'bed'), ('of', 'sleep'), ('of', 'us'), ('out', 'of'), ('My', 'Boss'), ('My', 'cousin'), ('are', 'all'), ('maybe', 'this'), ('this', 'weekend'), ('going', 'to'), ('(', 'Prefect'), ('(', 'mostly'), ('(', 'or'), ('(', 'she'), ('Camera', ')'), ('Falls', ')'), ('Least', 'but'), ('bonus', '('), ('done', '('), ('her', '('), ('humid', 'but'), ('it', \"'s\"), ('it', 'does'), ('least', '('), ('second', ')'), ('though', 'it'), ('warm', '('), ('your', 'Mocaccino'), ('your', 'favorite'), ('your', 'shower'), ('your', 'son'), ('I', 'am'), ('is', 'in'), ('but', 'not'), ('a', 'bit'), ('a', 'bunch'), ('a', 'chirpy'), ('a', 'full'), ('a', 'good'), ('a', 'great'), ('quite', 'a'), ('such', 'a'), ('actually', 'getting'), ('all', 'the'), ('am', 'actually'), ('am', 'going'), ('are', 'going'), ('do', 'on'), ('do', 'so'), ('its', 'going'), ('last', '?'), ('on', 'getting'), ('right', '?'), ('so', 'its'), ('to', 'do'), ('but', 'it'), ('.', 'And'), ('.', 'Its'), ('Monday', '.'), ('better', '.'), ('have', 'really'), ('have', 'time'), ('not', 'last'), ('not', 'right'), ('well', 'not'), (',', 'your'), ('Orgasmic', 'and'), ('and', 'everything'), ('and', 'think'), ('brother', 'and'), ('flawless', 'and'), ('lunch', 'and'), ('town', 'and'), ('me', 'that'), ('on', 'the'), ('that', 'day'), ('that', 'today'), (')', '.'), ('to', 'go'), ('to', 'work'), ('I', 'dunno'), ('I', 'left'), ('I', 'rarely'), ('I', 'will'), ('I', 'wont'), ('Mocaccino', 'is'), ('because', 'I'), ('cause', 'I'), ('is', '4:00'), ('is', 'Orgasmic'), ('is', 'awesome'), ('is', 'lack'), ('is', 'practically'), ('is', 'ready'), ('shower', 'is'), ('son', 'is'), ('things', 'I'), ('(', 'well'), ('but', 'its'), ('it', 'just'), ('it', 'really'), ('last', 'but'), ('your', 'Boss'), ('I', 'have'), ('back', 'to'), ('moved', 'to'), ('much', 'to'), ('road', 'to'), ('seemed', 'to'), ('to', 'Alberta'), ('to', 'be'), ('to', 'please'), ('want', 'to'), ('and', 'your'), ('take', 'a'), ('actually', 'have'), ('day', 'for'), ('for', 'me'), ('for', 'my'), ('go', 'for'), ('of', 'work'), ('cousin', '.'), ('time', '.'), ('weekend', '.'), (',', 'brother'), (',', 'now'), (',', 'then'), ('Alberta', ','), ('Knife', ','), ('Ok', ','), ('Well', ','), ('a.m.', ','), ('awesome', ','), ('dad', ','), ('decent', ','), ('dunno', ','), ('outside', ','), ('radio', ','), ('school', ','), ('talking', ','), (')', 'so'), ('?', ')'), ('today', ')'), ('and', 'cousin'), ('weekend', 'and'), ('.', 'I'), ('Boss', 'is'), ('cousin', 'is'), ('really', 'is'), ('this', 'is'), (',', 'but'), ('get', 'to'), ('time', 'to'), ('to', 'get'), ('to', 'take'), ('have', 'a'), ('and', 'I'), ('day', 'and'), ('me', 'and'), (',', 'maybe'), (',', 'well'), (',', 'you'), ('I', 'go'), ('is', 'that'), ('so', 'I'), ('today', 'is'), ('work', 'I'), ('work', 'is'), ('to', 'actually'), (',', 'the'), (',', 'today'), ('day', ','), ('go', ','), (')', 'and'), ('and', 'it'), ('have', 'to'), (')', 'I'), ('it', 'is'), ('and', 'a'), (',', 'have'), ('is', 'a')]\n",
      "TF-IDF matrix shape: (1, 90)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 18\n",
      "Total words: 429\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'enough', 'needs', 'was', 'wants', 'try', 'candy', 'shared', 'told', 'wanted', 'sense', 'guys', 'looking', 'type', 'claimed', 'Blah', 'did', 'bother', 'found', 'been', 'decision', 'Matt', 'be', 'have', 'depended', 'relationship', 'came', 'fault', 'guess', 'towards', 'make', 'hand', 'half', 'thing', 'guy', 'trust', None, 'see', 'depend', 'years', 'seem', 'sex', 'blame', 'makes', 'called', 'forbid', 'male', 'am', 'lets', 'aka', 'Bitch', 'demand', 'world', 'feeling', 'is', 'had', 'movies', 'anyone', 'being', 'life', 'woman', 'do', 'men', 'bitch', 'talking', 'think', 'trying', 'liked', 'fling', 'leaning', 'time', 'want', 'my', 'know', 'has', 'thought', 'fantasy', 'friend', 'hear', 'hook', 'watch', 'said', 'happened', 'fucking', 'squeeze', 'fine', 'bye', 'friendship', 'end', 'date', 'meet'}\n",
      "Punctuation similarity: 0.7549752861602498\n",
      "Length similarity: 0.36823626600578274\n",
      "Active voice count: 0.3888888888888889\n",
      "Passive voice count: 0.6111111111111112\n",
      "Grammar errors count: 90\n",
      "Upper case count: 46\n",
      "Lower case count: 1323\n",
      "Number of function words: 97\n",
      "Number of nodes in graph: 200\n",
      "Phrase patterns: [('Mr.', 'Arkansas'), ('My', 'fuck'), ('after', 'four'), ('aka', 'Mr.'), ('also', 'depend'), ('at', 'least'), ('being', 'alone'), ('bitch', 'because'), ('came', 'by'), ('casual', 'sex'), ('decent', 'even'), ('found', 'himself'), ('four', 'fucking'), ('god', 'forbid'), ('guys', 'over'), ('had', 'enough'), ('half', 'decent'), ('has', 'any'), ('hear', 'from'), ('life', 'depended'), ('makes', 'sense'), ('monogamous', 'type'), ('new', 'fling'), ('next', '5'), ('real', 'Bitch'), ('super', 'unlucky'), ('there', 'trying'), ('wants', 'some'), ('watch', 'movies'), ('whatever', 'decision'), ('...', 'He'), ('Cindy', 'has'), ('He', \"'s\"), ('all', 'BS'), ('all', 'you'), ('any', 'needs'), ('bad', '...'), ('but', 'myself'), ('even', 'if'), ('every', 'thought'), ('forbid', 'Cindy'), ('hand', '...'), ('hook', 'up'), ('is', 'pretty'), ('last', '2'), ('last', 'time'), ('leaning', 'more'), ('little', 'secret'), ('more', 'after'), ('more', 'towards'), ('movies', 'when'), ('my', 'fault'), ('my', 'life'), ('no', 'where'), ('out', 'there'), ('pretty', 'simple'), ('shared', 'every'), ('simple', 'friendship'), ('this', 'world'), ('too', 'bad'), ('up', 'front'), (\"'m\", 'so'), ('ca', \"n't\"), ('!', '!'), ('talking', 'about'), ('have', 'been'), ('Bitch', '('), ('Blah', '!'), ('This', 'other'), ('You', 'would'), ('anyone', 'half'), ('be', 'intimate'), ('be', 'monogamous'), ('blame', 'anyone'), ('can', 'be'), ('did', ')'), ('earlier', ')'), ('fine', ')'), ('for', '4'), ('from', 'him'), ('front', 'about'), ('in', 'general'), ('just', 'fine'), ('lets', 'be'), ('looking', 'for'), ('make', 'at'), ('male', 'one'), ('meet', 'anyone'), ('men', 'for'), ('one', 'thing'), ('only', 'human'), ('only', 'see'), ('other', 'guys'), ('other', 'hand'), ('really', 'know'), ('secret', '('), ('told', 'him'), ('was', 'single'), ('woman', 'would'), ('would', 'try'), ('you', 'want'), ('the', 'last'), ('when', 'he'), ('I', 'am'), ('2', 'years'), ('4', 'years'), ('5', 'years'), ('date', 'me'), ('depended', 'on'), ('end', 'of'), ('enough', 'with'), ('fuck', 'friend'), ('fucking', 'years'), ('intimate', 'with'), ('liked', 'me'), (\"n't\", 'blame'), (\"n't\", 'really'), (\"n't\", 'seem'), ('of', 'being'), ('of', 'relationship'), ('ok', 'with'), ('or', 'came'), ('or', 'hear'), ('see', 'or'), ('so', 'confused'), ('so', 'tired'), ('squeeze', 'me'), ('tired', 'of'), ('type', 'of'), ('unlucky', 'or'), ('with', 'casual'), ('with', 'men'), (\"'s\", 'all'), ('but', 'no'), ('bye', 'Cindy'), ('bye', 'bye'), ('every', 'little'), ('guess', 'what'), ('if', 'my'), ('is', 'my'), ('little', 'more'), ('needs', 'too'), ('simple', 'but'), ('this', 'guy'), ('do', \"n't\"), ('There', 'have'), ('general', 'not'), ('have', 'had'), ('have', 'shared'), ('we', 'have'), ('If', 'that'), ('by', 'to'), ('claimed', 'he'), ('he', 'found'), ('he', 'liked'), ('he', 'said'), ('he', 'wants'), ('it', 'happened'), ('once', 'he'), ('over', 'the'), ('said', 'he'), ('seem', 'to'), ('that', 'makes'), ('the', 'earlier'), ('the', 'end'), ('the', 'next'), ('time', 'it'), ('to', 'bother'), ('to', 'date'), ('to', 'hook'), ('to', 'meet'), ('to', 'smoke-up'), ('towards', 'the'), ('trying', 'to'), ('wanted', 'to'), (\"'s\", 'in'), ('anyone', 'but'), ('be', 'up'), ('been', 'other'), ('been', 'talking'), ('guy', 'for'), ('him', 'when'), ('in', 'this'), ('know', 'if'), ('know', 'what'), ('other', 'guy'), ('was', 'all'), ('would', 'think'), ('on', 'it'), ('about', 'a'), ('of', 'no'), ('out', 'of'), ('up', 'with'), ('called', 'him'), ('for', 'at'), ('just', 'be'), ('just', 'only'), ('know', 'one'), ('only', 'called'), ('a', 'bitch'), ('a', 'male'), ('a', 'new'), ('a', 'real'), ('a', 'serious'), ('a', 'woman'), ('actually', 'a'), ('am', 'leaning'), ('am', 'looking'), ('am', 'ok'), ('am', 'super'), ('demand', 'a'), ('himself', 'a'), ('have', 'needs'), ('not', 'out'), ('not', 'too'), ('.', 'I'), ('a', 'friend'), ('if', 'that'), ('it', 'is'), ('that', \"'s\"), ('that', 'is'), ('think', 'that'), ('to', 'this'), ('(', 'or'), ('called', 'friend'), ('called', 'me'), ('do', 'just'), ('do', 'know'), ('friend', '('), ('friend', 'only'), ('him', 'on'), ('me', 'in'), (\"n't\", 'know'), ('one', 'so'), ('or', 'at'), ('so', 'called'), ('with', 'anyone'), ('would', 'do'), ('am', 'not'), ('.', 'Blah'), ('.', 'Either'), ('.', 'If'), ('.', 'My'), ('.', 'There'), ('.', 'This'), ('.', 'You'), ('BS', '.'), ('alone', '.'), ('friendship', '.'), ('myself', '.'), ('trust', '.'), ('world', '.'), ('I', 'do'), ('years', '.'), ('and', 'also'), ('and', 'fantasy'), ('and', 'feeling'), ('and', 'squeeze'), ('and', 'trust'), ('and', 'wanted'), ('and', 'watch'), ('and', 'whatever'), ('bother', 'and'), ('depend', 'and'), ('fantasy', 'and'), ('fault', 'and'), ('feeling', 'and'), ('human', 'and'), ('smoke-up', 'and'), ('thought', 'and'), ('try', 'and'), ('him', 'not'), ('not', 'in'), ('not', 'talking'), ('I', \"'m\"), ('I', 'ca'), ('me', 'and'), ('friend', 'on'), ('me', 'or'), ('so', 'on'), ('about', 'it'), ('he', 'was'), ('he', 'would'), ('it', 'was'), ('least', 'it'), ('least', 'the'), ('talking', 'to'), ('that', 'was'), ('the', 'other'), ('was', 'the'), (',', 'I'), ('a', 'little'), ('a', 'simple'), (',', 'Matt'), (',', 'actually'), (',', 'aka'), (',', 'claimed'), (',', 'god'), (',', 'however'), (',', 'lets'), (',', 'once'), (',', 'we'), ('Arkansas', ','), ('Matt', ','), ('confused', ','), ('fling', ','), ('happened', ','), ('relationship', ','), ('sense', ','), ('serious', ','), ('sex', ','), ('single', ','), ('some', ','), ('thing', ','), ('I', 'want'), ('friend', 'that'), ('of', 'that'), ('on', 'the'), ('years', ','), ('am', 'a'), ('Cindy', '.'), ('needs', '.'), ('too', '.'), ('Either', 'I'), ('I', 'can'), ('I', 'demand'), ('I', 'did'), ('I', 'make'), ('I', 'told'), ('because', 'I'), ('decision', 'I'), ('however', 'I'), ('where', 'I'), ('and', 'every'), ('and', 'guess'), ('and', 'out'), ('(', 'a'), ('am', 'only'), ('am', 'talking'), ('for', 'a'), ('want', 'a'), ('not', 'to'), ('it', 'the'), (')', '.'), ('I', 'have'), (')', 'and'), ('want', 'and'), (',', 'bye'), ('guy', ','), ('no', ','), ('what', ','), ('...', 'I'), ('I', 'guess'), ('I', 'think'), ('guess', 'I'), ('think', 'I'), ('what', 'I'), ('and', 'so'), ('with', 'and'), (')', ','), (',', 'just'), (',', 'one'), ('anyone', ','), ('in', ','), ('one', ','), ('want', ','), ('and', 'have'), ('it', '.'), ('that', '.'), ('(', 'I'), ('I', 'called'), ('I', 'just'), ('least', 'I'), ('it', 'and'), ('on', ',')]\n",
      "TF-IDF matrix shape: (1, 101)\n",
      "LSA matrix shape: (1, 1)\n",
      "Number of sentences: 4\n",
      "Total words: 71\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'matter', 'get', 'go', 'Ciao', 'killer', 'mojo', 'ass', 'want', 'is', 'had', 'head', 'thing', 'shit', 'blamed', 'nothing', 'break', 'boss', 'home', 'lunch', 'tons', 'fact', 'days', 'ache', 'take', 'do', 'happens', 'have', 'eat', 'bath', 'goes'}\n",
      "Punctuation similarity: 0.71875\n",
      "Length similarity: 0.4965277777777778\n",
      "Active voice count: 0.75\n",
      "Passive voice count: 0.25\n",
      "Grammar errors count: 30\n",
      "Upper case count: 10\n",
      "Lower case count: 218\n",
      "Number of function words: 17\n",
      "Number of nodes in graph: 54\n",
      "Phrase patterns: [('Ciao', '!'), ('MOFO', 'bubble'), ('any', 'break'), ('ass', 'MOFO'), ('break', 'as'), ('bubble', 'bath'), ('days', 'where'), ('do', 'is'), ('go', 'home'), ('goes', 'right'), ('had', 'lunch'), ('head', 'ache'), ('is', 'go'), ('killer', 'days'), ('littlest', 'thing'), ('long', 'ass'), (\"n't\", 'had'), ('nothing', 'goes'), ('pounding', 'head'), ('right', 'for'), ('thing', 'happens'), ('those', 'killer'), ('to', 'do'), ('want', 'to'), ('where', 'nothing'), ('you', 'get'), ('I', 'have'), ('and', 'you'), ('blamed', 'or'), ('for', 'the'), ('get', 'blamed'), ('get', 'tons'), ('have', \"n't\"), ('lunch', 'or'), ('or', 'any'), ('the', 'boss'), ('the', 'littlest'), (',', 'eat'), ('.', 'All'), ('.', 'Ciao'), ('All', 'I'), ('I', 'want'), ('One', 'of'), ('a', 'long'), ('a', 'matter'), ('a', 'pounding'), ('ache', ','), ('and', 'take'), ('as', 'a'), ('bath', '.'), ('boss', ','), ('eat', 'and'), ('fact', '.'), ('happens', 'and'), ('home', ','), ('matter', 'of'), ('of', 'fact'), ('of', 'shit'), ('of', 'those'), ('shit', '.'), ('take', 'a'), ('tons', 'of'), ('or', 'the'), ('have', 'a'), (',', 'I'), (',', 'and'), ('.', 'I')]\n",
      "TF-IDF matrix shape: (1, 28)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 11\n",
      "Total words: 78\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'interest', 'am', 'people', 'waning', 'go', 'mind', 'couple', 'is', 'had', 'are', 'hand', 'day', 'thing', 'has', 'Is', 'arranged', 'turned', 'be', 'have', 'surprise', 'party'}\n",
      "Punctuation similarity: 0.5822704081632653\n",
      "Length similarity: 0.46868686868686865\n",
      "Active voice count: 0.6363636363636364\n",
      "Passive voice count: 0.36363636363636365\n",
      "Grammar errors count: 21\n",
      "Upper case count: 13\n",
      "Lower case count: 227\n",
      "Number of function words: 18\n",
      "Number of nodes in graph: 57\n",
      "Phrase patterns: [('!', 'Maybe'), (\"'re\", 'still'), ('2nd', 'anniversary'), ('?', 'It'), ('Is', 'my'), ('It', 'would'), ('You', \"'re\"), ('anniversary', 'party'), ('be', 'arranged'), ('day', '2'), ('go', 'with'), ('great', '!'), ('hand', ','), ('have', 'just'), ('is', 'day'), ('my', 'interest'), (\"n't\", 'surprise'), ('other', 'hand'), ('party', 'should'), ('should', 'be'), ('still', 'here'), ('surprise', 'me'), ('these', 'before'), ('thing', 'waning'), ('turned', 'out'), ('waning', '?'), ('whole', 'thing'), ('would', \"n't\"), ('Both', 'of'), ('On', 'the'), ('So', 'am'), ('They', 'turned'), ('couple', 'of'), ('has', 'turned'), ('in', 'mind'), ('interest', 'in'), ('just', 'the'), ('of', 'these'), ('out', 'great'), ('out', 'rubbish'), ('people', 'in'), ('the', 'other'), ('the', 'people'), (',', 'this'), ('Maybe', 'a'), ('a', '2nd'), ('a', 'couple'), ('a', 'go'), ('had', 'a'), ('this', 'has'), ('this', 'is'), ('this', 'whole'), ('with', 'a'), ('I', 'had'), ('I', 'have'), ('am', 'I'), ('.', 'So'), ('So', 'this'), ('in', 'this'), ('of', 'I'), ('.', 'Both'), ('.', 'On'), ('.', 'They'), ('.', 'You'), ('2', '.'), ('arranged', '.'), ('before', '.'), ('here', '.'), ('me', '.'), ('mind', '.'), ('rubbish', '.'), ('.', 'I'), ('I', '.')]\n",
      "TF-IDF matrix shape: (1, 18)\n",
      "LSA matrix shape: (1, 1)\n",
      "Number of sentences: 14\n",
      "Total words: 269\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'needs', 'things', 'go', 'ask', 'met', 'was', 'led', 'is', 'father', 'try', 'anything', 'effort', 'had', 'say', 'make', 'parents', 'daughter', 'told', 'telling', 'felt', 'thing', 'grandson', 'town', 'pick', 'know', 'has', 'mentioned', 'man', 'did', 'asked', 'hope', 'yesterday', 'visits', 'does', 'found', 'said', 'need', 'weekend', 'happened', 'lives', 'family', 'aside', 'do', 'patients', 'believe', 'upset', 'be', 'pulled', 'goes', 'week', 'raising', 'bring', 'left', 'kids', 'patient', 'floored', 'came', 'visit'}\n",
      "Punctuation similarity: 0.6536989795918368\n",
      "Length similarity: 0.4502304147465438\n",
      "Active voice count: 0.5\n",
      "Passive voice count: 0.5\n",
      "Grammar errors count: 59\n",
      "Upper case count: 21\n",
      "Lower case count: 884\n",
      "Number of function words: 69\n",
      "Number of nodes in graph: 127\n",
      "Phrase patterns: [('He', 'lives'), ('They', 'left'), ('always', 'ask'), ('am', 'raising'), ('an', 'effort'), ('any', 'other'), ('anything', 'special'), ('even', 'met'), ('ever', 'not'), ('found', 'out'), ('know', 'what'), ('make', 'an'), ('one', 'thing'), ('patients', 'how'), ('probably', 'very'), ('so', 'sad'), ('thing', 'led'), ('things', 'on'), ('very', 'lonely'), (\"'s\", 'your'), ('90', \"'s\"), ('It', \"'s\"), ('She', 'said'), ('The', 'patient'), ('be', 'upset'), ('believe', 'it'), ('do', 'things'), ('does', 'make'), ('effort', 'but'), ('had', 'any'), ('him', 'up'), ('is', 'probably'), ('man', 'who'), ('me', 'when'), ('new', 'patient'), ('other', 'family'), ('patient', 'yesterday'), ('pick', 'him'), ('raising', 'them'), ('really', 'know'), ('special', 'happened'), ('sweet', 'old'), ('telling', 'me'), ('them', 'right'), ('who', 'is'), ('you', 'do'), ('your', 'family'), ('in', 'town'), ('would', 'be'), ('could', \"n't\"), ('grand', 'father'), ('great', 'grand'), ('if', 'they'), ('ask', 'my'), ('felt', 'so'), ('grandson', 'never'), ('has', 'never'), ('his', '90'), ('his', 'grandson'), ('just', 'felt'), ('just', 'go'), ('met', 'his'), ('my', 'own'), ('my', 'patients'), ('never', 'even'), ('never', 'goes'), ('on', 'my'), ('did', \"n't\"), ('to', 'say'), ('a', 'new'), ('a', 'sweet'), ('bring', 'her'), ('happened', 'in'), ('he', 'has'), ('he', 'mentioned'), ('her', 'aside'), ('how', 'their'), ('lives', 'in'), ('mentioned', 'a'), ('out', 'that'), ('pulled', 'her'), ('their', 'week'), ('their', 'weekend'), ('told', 'her'), ('was', 'floored'), ('was', 'or'), ('was', 'telling'), ('weekend', 'was'), ('would', 'ever'), ('would', 'try'), ('asked', 'him'), ('it', 'is'), ('old', 'man'), ('really', 'hope'), ('say', 'anything'), ('her', 'kids'), ('that', 'he'), ('.', 'I'), ('When', 'she'), ('but', 'she'), ('grand', 'daughter'), ('grand', 'parents'), ('not', 'visit'), ('or', 'if'), ('she', 'came'), ('she', 'does'), ('she', 'needs'), ('upset', 'if'), ('their', 'grand'), ('they', 'would'), ('visit', 'their'), (',', 'you'), (\"n't\", 'believe'), (\"n't\", 'need'), ('right', ','), ('up', ','), ('yesterday', ','), ('asked', 'my'), ('be', 'so'), ('his', 'great'), ('is', 'so'), ('never', 'visits'), ('town', 'and'), (',', 'a'), ('came', 'to'), ('led', 'to'), ('need', 'to'), ('needs', 'to'), ('to', 'another'), ('to', 'bring'), ('to', 'pick'), ('what', 'to'), ('I', 'asked'), ('visits', '.'), (\"n't\", 'visit'), ('a', 'man'), ('family', 'in'), ('had', 'a'), ('he', 'had'), ('man', 'in'), ('me', 'that'), ('patient', 'was'), ('said', 'that'), ('was', 'old'), ('just', 'could'), ('him', 'if'), ('hope', 'she'), ('if', 'anything'), ('said', 'they'), ('she', 'did'), ('they', 'did'), ('they', 'said'), ('visit', 'me'), ('visit', 'them'), ('I', 'just'), ('and', 'one'), ('and', 'told'), ('another', 'and'), ('aside', 'and'), ('go', 'and'), ('goes', 'and'), ('left', 'and'), ('parents', 'and'), ('try', 'and'), ('week', 'and'), (\"'s\", ','), ('do', \"n't\"), ('family', ','), (\"n't\", 'do'), (\"n't\", 'really'), ('special', ','), ('he', 'never'), ('in', 'his'), ('my', 'kids'), ('sad', 'that'), ('that', 'his'), ('.', 'He'), ('.', 'It'), ('.', 'She'), ('.', 'The'), ('.', 'They'), ('.', 'When'), ('I', 'always'), ('I', 'am'), ('I', 'found'), ('I', 'pulled'), ('So', 'I'), ('daughter', '.'), ('floored', '.'), ('lonely', '.'), ('own', '.'), ('when', 'I'), ('say', 'to'), ('I', 'was'), ('kids', '.'), ('sad', 'if'), ('they', 'could'), ('in', 'their'), (',', 'just'), ('a', 'grand'), ('grand', 'kids'), ('her', 'she'), ('if', 'he'), ('kids', 'if'), ('she', 'would'), ('that', 'she'), (',', 'I'), ('and', 'great'), ('and', 'it'), ('and', 'make'), ('and', 'visits'), ('father', 'and'), ('old', 'and'), ('I', 'had'), ('I', 'hope'), ('I', 'really'), ('effort', '.'), ('father', '.'), ('hope', 'I'), ('it', '.'), ('them', '.'), ('kids', 'to'), ('to', 'her'), ('to', 'visit'), ('and', 'could'), ('sad', '.'), ('and', 'he'), ('and', 'I'), ('I', 'would')]\n",
      "TF-IDF matrix shape: (1, 64)\n",
      "LSA matrix shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6\n",
      "Total words: 86\n",
      "Structure similarity: 1.0\n",
      "Misspelled words: {'am', 'think', 'people', 'death', 'tuna', 'alive', 'island', 'anniversary', 'is', 'cheeseburgers', 'like', 'look', 'know', 'weird', 'chilling', 'Is', 'theory', 'elves', 'happened', 'do', 'believe', 'living', 'ate', 'mean', 'piggy', 'Today'}\n",
      "Punctuation similarity: 0.7371323529411764\n",
      "Length similarity: 0.3333333333333333\n",
      "Active voice count: 0.3333333333333333\n",
      "Passive voice count: 0.6666666666666666\n",
      "Grammar errors count: 26\n",
      "Upper case count: 12\n",
      "Lower case count: 271\n",
      "Number of function words: 19\n",
      "Number of nodes in graph: 58\n",
      "Phrase patterns: [(\"'\", 'death'), (\"'s\", 'really'), ('...', 'However'), ('Tupac', 'and'), ('a', 'theory'), ('am', 'one'), ('and', 'Biggy'), ('at', 'all'), ('ate', '...'), ('island', 'with'), ('just', 'chillin'), ('living', 'on'), ('look', 'at'), ('mean', 'look'), ('on', 'some'), ('people', 'who'), ('really', 'not'), ('remote', 'island'), ('some', 'remote'), ('still', 'alive'), ('those', 'people'), ('who', 'like'), ('with', 'Tupac'), ('?', 'Is'), ('Elvis', \"'\"), ('What', 'do'), ('You', 'know'), ('all', 'the'), ('do', \"n't\"), ('happened', 'to'), ('like', 'to'), (\"n't\", 'think'), ('the', 'anniversary'), ('the', 'cheeseburgers'), ('think', 'happened'), ('to', 'believe'), ('.', 'What'), ('.', 'You'), ('Biggy', '.'), ('I', 'am'), ('I', 'mean'), ('Today', 'is'), ('anniversary', 'of'), ('believe', 'that'), ('death', '.'), ('not', 'that'), ('of', 'a'), ('of', 'those'), ('one', 'of'), ('that', \"'s\"), ('that', 'weird'), ('theory', '.'), ('weird', 'of'), ('know', ','), ('he', 'is'), ('Is', 'he'), ('cheeseburgers', 'he'), ('he', 'ate'), ('Elvis', '?'), ('alive', '?'), ('do', 'you'), ('to', 'Elvis'), ('you', 'know'), ('you', 'think'), (',', 'I'), (',', 'just'), (',', 'living'), ('However', ','), ('chillin', ','), ('?', 'I'), ('I', 'do'), ('is', 'still'), ('is', 'the'), ('of', 'Elvis'), ('he', 'still'), ('think', 'he'), (',', 'you'), ('alive', ','), ('that', 'he'), (',', 'that'), ('is', ',')]\n",
      "TF-IDF matrix shape: (1, 26)\n",
      "LSA matrix shape: (1, 1)\n",
      "Accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from nltk.tree import Tree\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import re\n",
    "\n",
    "def extract_phrase_patterns(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Create a bigram collocation finder\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    \n",
    "    # Apply a statistical measure to identify significant collocations\n",
    "    scored_collocations = bigram_finder.score_ngrams(BigramAssocMeasures.chi_sq)\n",
    "    \n",
    "    # Extract collocations that meet a certain threshold\n",
    "    significant_collocations = [bigram for bigram, score in scored_collocations if score >  3.0]\n",
    "    \n",
    "    return significant_collocations\n",
    "\n",
    "def calculate_punctuation_similarity(text):\n",
    "    # Define the set of punctuation marks\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "\n",
    "    # Count punctuation marks in the text\n",
    "    punctuation_counts = {punctuation_mark: text.count(punctuation_mark) for punctuation_mark in punctuation_marks}\n",
    "\n",
    "    # Compute punctuation similarity as the sum of squared differences between punctuation frequencies\n",
    "    total_marks = sum(punctuation_counts.values())\n",
    "    punctuation_frequencies = {mark: count / total_marks for mark, count in punctuation_counts.items()}\n",
    "\n",
    "    # Compute similarity using squared Euclidean distance between punctuation distributions\n",
    "    punctuation_sim = 0.0\n",
    "    for mark in punctuation_marks:\n",
    "        punctuation_sim += (punctuation_frequencies.get(mark, 0.0) - 1.0 / len(punctuation_marks)) ** 2\n",
    "\n",
    "    punctuation_sim = 1.0 - punctuation_sim  # Normalize to [0, 1]\n",
    "    \n",
    "    return punctuation_sim\n",
    "\n",
    "def calculate_sentence_length_similarity(sentences):\n",
    "    if len(sentences) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_length = sum(len(sentence) for sentence in sentences)\n",
    "    average_length = total_length / len(sentences)\n",
    "\n",
    "    max_length = max(len(sentence) for sentence in sentences)\n",
    "    min_length = min(len(sentence) for sentence in sentences)\n",
    "\n",
    "    if max_length == min_length:\n",
    "        return 0.0\n",
    "\n",
    "    normalized_average_length = (average_length - min_length) / (max_length - min_length)\n",
    "\n",
    "    return normalized_average_length\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def get_function_words(text):\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def detect_voice(sentence):\n",
    "    \"\"\"\n",
    "    Detects the voice (active or passive) of a given sentence.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: The detected voice ('active' or 'passive').\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words and get part-of-speech tags\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Check for passive voice indicators\n",
    "    passive_indicators = ['is', 'am', 'are', 'was', 'were', 'been', 'being', 'be', 'by']\n",
    "\n",
    "    # Check if the sentence contains any passive voice indicators\n",
    "    if any(tagged_word[0].lower() in passive_indicators for tagged_word in tagged_words):\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_features(text):\n",
    "    try:\n",
    "        # Skip empty texts\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = len(word_tokenize(text))\n",
    "       \n",
    "        # Initialize structure similarity to a default value\n",
    "        structure_similarity = 0.0\n",
    "        \n",
    "        if sentences:  # Check if sentences list is not empty\n",
    "            # Calculate similarity based on sentence structure\n",
    "            structure_similarity = len(sentences) / len(sentences)\n",
    "\n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled = set([spell.correction(word) for sentence in words for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # Calculate punctuation similarity\n",
    "        punctuation_sim = calculate_punctuation_similarity(text)\n",
    "\n",
    "        # Calculate sentence length similarity\n",
    "        length_similarity = calculate_sentence_length_similarity(sentences)\n",
    "\n",
    "        # Detect active/passive voice\n",
    "        active_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'active') / len(sentences)\n",
    "        passive_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'passive') / len(sentences)\n",
    "\n",
    "        # Grammar errors\n",
    "        grammar_errors_count = len(misspelled)\n",
    "\n",
    "        # Case usage (upper/lower case)\n",
    "        upper_case_count = sum(1 for char in text if char.isupper())\n",
    "        lower_case_count = sum(1 for char in text if char.islower())\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph = generate_ngram_transition_graph(text, n_value)\n",
    "\n",
    "        phrase_patterns = extract_phrase_patterns(text)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_words = get_function_words(text)\n",
    "\n",
    "        # Debugging statements\n",
    "        print(\"Number of sentences:\", len(sentences))\n",
    "        print(\"Total words:\", total_words)\n",
    "        print(\"Structure similarity:\", structure_similarity)\n",
    "        print(\"Misspelled words:\", misspelled)\n",
    "        print(\"Punctuation similarity:\", punctuation_sim)\n",
    "        print(\"Length similarity:\", length_similarity)\n",
    "        print(\"Active voice count:\", active_voice_count)\n",
    "        print(\"Passive voice count:\", passive_voice_count)\n",
    "        print(\"Grammar errors count:\", grammar_errors_count)\n",
    "        print(\"Upper case count:\", upper_case_count)\n",
    "        print(\"Lower case count:\", lower_case_count)\n",
    "        print(\"Number of function words:\", len(function_words))\n",
    "        print(\"Number of nodes in graph:\", len(graph.nodes))\n",
    "        print(\"Phrase patterns:\", phrase_patterns)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=4)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Debugging statements\n",
    "        print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "        print(\"LSA matrix shape:\", lsa_matrix.shape)\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr = calculate_ttr(text)\n",
    "\n",
    "        # Collect features into a list\n",
    "        features = [structure_similarity, length_similarity, punctuation_sim, \n",
    "                    active_voice_count, passive_voice_count, \n",
    "                    grammar_errors_count, upper_case_count, \n",
    "                    lower_case_count, len(set(function_words)), len(graph.nodes), ttr] + list(lsa_matrix.flatten())\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        features_normalized = scaler.fit_transform([features])\n",
    "\n",
    "        return features_normalized[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_features: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(author_a_texts, not_author_a_texts):\n",
    "    # Label author_a texts as 1 and not_author_a texts as 0\n",
    "    X = author_a_texts + not_author_a_texts\n",
    "    y = [1] * len(author_a_texts) + [0] * len(not_author_a_texts)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def main(author_a_texts, not_author_a_texts):\n",
    "    # Load and preprocess data\n",
    "    X, y = load_data(author_a_texts, not_author_a_texts)\n",
    "\n",
    "    # Extract features from the texts\n",
    "    X_features = [extract_features(text) for text in X]\n",
    "\n",
    "    # Remove None values\n",
    "    X_features = [x for x in X_features if x is not None]\n",
    "\n",
    "    # Convert feature list to numpy array\n",
    "    X_features = np.array(X_features)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=40)\n",
    "\n",
    "    # Train a random forest classifier (new stuff)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=40)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    author_a_texts = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "    \"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "    \"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "    \"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "    \"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "    \"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "    \"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "    \"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "    \"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "    \"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "    \"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "    \"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "    \"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "    \"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "    \"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "    \"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "    not_author_a_texts = [\"\"\"\t\t\t\t\t\t\n",
    "        As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "    \"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\n",
    "    \"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "    I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\n",
    "    \"\"\" \n",
    "    ]  # Provide a list of texts not from author A\n",
    "    main(author_a_texts, not_author_a_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"gru_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 12)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 12), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 354\u001b[0m\n\u001b[0;32m    305\u001b[0m author_a_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know if I have it in me to make another one.   I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\"\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\"\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\"\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"\u001b[39;00m]  \u001b[38;5;66;03m# Provide a list of texts from author A\u001b[39;00m\n\u001b[0;32m    321\u001b[0m not_author_a_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\t\t\t\t\t\t\u001b[39m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124m    As promised, here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the next instalment of bus mongs.  I bet you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve been looking forward to this, haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Man\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and form a rock n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m roll band, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspirit of the blitz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m style dialect.   Eg: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOoh \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mello Frank, I \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mope you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll be putting yer foot down today, my Bert\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms expecting his dinner!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong way!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt think of anything else.  It really is just a small sample.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m    323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\t\t\t\t\t\t\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm gonna go ahead and assume that a majority of the people who read this don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSweet Home Alabama\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll wait....   Ok, now that you have seen it, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt you just love it?  It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwo Weeks Notice\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBridget Jones\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Diary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, how can you NOT love that movie?  You gotta love Bridget, she\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms awesome.  Hmm, what other movies do I like?  OH, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to Lose a Guy in Ten Days\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm out, have a great day!   \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    353\u001b[0m ]  \u001b[38;5;66;03m# Provide a list of texts not from author A\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_a_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_author_a_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 278\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(author_a_texts, not_author_a_texts)\u001b[0m\n\u001b[0;32m    272\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    273\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    274\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Print debug information\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_wlyxux5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"gru_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 12)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 12), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import traceback\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.util import bigrams  \n",
    "from scipy import spatial\n",
    "from nltk.tree import Tree\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import re\n",
    "# Import TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def extract_phrase_patterns(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Create a bigram collocation finder\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    \n",
    "    # Apply a statistical measure to identify significant collocations\n",
    "    scored_collocations = bigram_finder.score_ngrams(BigramAssocMeasures.chi_sq)\n",
    "    \n",
    "    # Extract collocations that meet a certain threshold\n",
    "    significant_collocations = [bigram for bigram, score in scored_collocations if score >  3.0]\n",
    "    \n",
    "    return significant_collocations\n",
    "\n",
    "def calculate_punctuation_similarity(text):\n",
    "    # Define the set of punctuation marks\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "\n",
    "    # Count punctuation marks in the text\n",
    "    punctuation_counts = {punctuation_mark: text.count(punctuation_mark) for punctuation_mark in punctuation_marks}\n",
    "\n",
    "    # Compute punctuation similarity as the sum of squared differences between punctuation frequencies\n",
    "    total_marks = sum(punctuation_counts.values())\n",
    "    punctuation_frequencies = {mark: count / total_marks for mark, count in punctuation_counts.items()}\n",
    "\n",
    "    # Compute similarity using squared Euclidean distance between punctuation distributions\n",
    "    punctuation_sim = 0.0\n",
    "    for mark in punctuation_marks:\n",
    "        punctuation_sim += (punctuation_frequencies.get(mark, 0.0) - 1.0 / len(punctuation_marks)) ** 2\n",
    "\n",
    "    punctuation_sim = 1.0 - punctuation_sim  # Normalize to [0, 1]\n",
    "    \n",
    "    return punctuation_sim\n",
    "\n",
    "def calculate_sentence_length_similarity(sentences):\n",
    "    if len(sentences) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_length = sum(len(sentence) for sentence in sentences)\n",
    "    average_length = total_length / len(sentences)\n",
    "\n",
    "    max_length = max(len(sentence) for sentence in sentences)\n",
    "    min_length = min(len(sentence) for sentence in sentences)\n",
    "\n",
    "    if max_length == min_length:\n",
    "        return 0.0\n",
    "\n",
    "    normalized_average_length = (average_length - min_length) / (max_length - min_length)\n",
    "\n",
    "    return normalized_average_length\n",
    "\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "def get_function_words(text):\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def detect_voice(sentence):\n",
    "    \"\"\"\n",
    "    Detects the voice (active or passive) of a given sentence.\n",
    "\n",
    "    Parameters:\n",
    "        sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: The detected voice ('active' or 'passive').\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words and get part-of-speech tags\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Check for passive voice indicators\n",
    "    passive_indicators = ['is', 'am', 'are', 'was', 'were', 'been', 'being', 'be', 'by']\n",
    "\n",
    "    # Check if the sentence contains any passive voice indicators\n",
    "    if any(tagged_word[0].lower() in passive_indicators for tagged_word in tagged_words):\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_features(text):\n",
    "    try:\n",
    "        # Skip empty texts\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = len(word_tokenize(text))\n",
    "       \n",
    "        # Initialize structure similarity to a default value\n",
    "        structure_similarity = 0.0\n",
    "        \n",
    "        if sentences:  # Check if sentences list is not empty\n",
    "            # Calculate similarity based on sentence structure\n",
    "            structure_similarity = len(sentences) / len(sentences)\n",
    "\n",
    "        # Tokenize words and get part-of-speech tags\n",
    "        words = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        # Check spelling using pyspellchecker\n",
    "        spell = SpellChecker()\n",
    "        misspelled = set([spell.correction(word) for sentence in words for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        # Calculate punctuation similarity\n",
    "        punctuation_sim = calculate_punctuation_similarity(text)\n",
    "\n",
    "        # Calculate sentence length similarity\n",
    "        length_similarity = calculate_sentence_length_similarity(sentences)\n",
    "\n",
    "        # Detect active/passive voice\n",
    "        active_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'active') / len(sentences)\n",
    "        passive_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'passive') / len(sentences)\n",
    "\n",
    "        # Grammar errors\n",
    "        grammar_errors_count = len(misspelled)\n",
    "\n",
    "        # Case usage (upper/lower case)\n",
    "        upper_case_count = sum(1 for char in text if char.isupper())\n",
    "        lower_case_count = sum(1 for char in text if char.islower())\n",
    "\n",
    "        # Generate n-gram transition graphs\n",
    "        n_value = 2  # You can adjust the n-gram size\n",
    "        graph = generate_ngram_transition_graph(text, n_value)\n",
    "\n",
    "        phrase_patterns = extract_phrase_patterns(text)\n",
    "\n",
    "        # Additional features: Function word counts or presence/absence\n",
    "        function_words = get_function_words(text)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=4)  # You can adjust the number of components\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Calculate TTR for each text\n",
    "        ttr = calculate_ttr(text)\n",
    "\n",
    "        # Collect features into a list\n",
    "        features = [structure_similarity, length_similarity, punctuation_sim, \n",
    "                    active_voice_count, passive_voice_count, \n",
    "                    grammar_errors_count, upper_case_count, \n",
    "                    lower_case_count, len(set(function_words)), len(graph.nodes), ttr] + list(lsa_matrix.flatten())\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        features_normalized = scaler.fit_transform([features])\n",
    "\n",
    "        return features_normalized[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_features: {e}\")\n",
    "        traceback.print_exc()  # Print the full traceback for detailed error information\n",
    "        return None\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(author_a_texts, not_author_a_texts):\n",
    "    # Label author_a texts as 1 and not_author_a texts as 0\n",
    "    X = author_a_texts + not_author_a_texts\n",
    "    y = [1] * len(author_a_texts) + [0] * len(not_author_a_texts)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def main(author_a_texts, not_author_a_texts):\n",
    "    # Load and preprocess data\n",
    "    X, y = load_data(author_a_texts, not_author_a_texts)\n",
    "\n",
    "    # Extract features from the texts\n",
    "    X_features = [extract_features(text) for text in X]\n",
    "\n",
    "    # Remove None values\n",
    "    X_features = [x for x in X_features if x is not None]\n",
    "\n",
    "    # Convert feature list to numpy array\n",
    "    X_features = np.array(X_features)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=40)\n",
    "\n",
    "    # Define the GRU model\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "    # Print debug information\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "    # Check model architecture\n",
    "    print(\"Model summary:\")\n",
    "    print(model.summary())\n",
    "\n",
    "    # Check for NaNs or infinite values\n",
    "    print(\"NaNs in X_train:\", np.isnan(X_train).any())\n",
    "    print(\"Infinite values in X_train:\", not np.isfinite(X_train).all())\n",
    "\n",
    "    # Evaluate the model\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    author_a_texts = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "    \"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "    \"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "    \"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "    \"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "    \"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "    \"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "    \"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "    \"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "    \"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "    \"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "    \"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "    \"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "    \"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "    \"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "    \"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "    not_author_a_texts = [\"\"\"\t\t\t\t\t\t\n",
    "        As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "    \"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\n",
    "    \"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "    I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "    ]  # Provide a list of texts not from author A\n",
    "    main(author_a_texts, not_author_a_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"gru_4\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, 1, 12)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 1, 1, 12), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 313\u001b[0m\n\u001b[0;32m    264\u001b[0m author_a_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know if I have it in me to make another one.   I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\"\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\"\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\"\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"\u001b[39;00m]  \u001b[38;5;66;03m# Provide a list of texts from author A\u001b[39;00m\n\u001b[0;32m    280\u001b[0m not_author_a_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\t\t\t\t\t\t\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124m    As promised, here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the next instalment of bus mongs.  I bet you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve been looking forward to this, haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Man\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and form a rock n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m roll band, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspirit of the blitz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m style dialect.   Eg: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOoh \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mello Frank, I \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mope you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll be putting yer foot down today, my Bert\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms expecting his dinner!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong way!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt think of anything else.  It really is just a small sample.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\t\t\t\t\t\t\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm gonna go ahead and assume that a majority of the people who read this don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSweet Home Alabama\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll wait....   Ok, now that you have seen it, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt you just love it?  It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwo Weeks Notice\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBridget Jones\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Diary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, how can you NOT love that movie?  You gotta love Bridget, she\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms awesome.  Hmm, what other movies do I like?  OH, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to Lose a Guy in Ten Days\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm out, have a great day!   \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    312\u001b[0m ]  \u001b[38;5;66;03m# Provide a list of texts not from author A\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_a_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_author_a_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 242\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(author_a_texts, not_author_a_texts)\u001b[0m\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m    233\u001b[0m     GRU(\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    234\u001b[0m     GlobalAveragePooling1D(),\n\u001b[0;32m    235\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    236\u001b[0m ])\n\u001b[0;32m    238\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    239\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    240\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 242\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filesvjz2wi1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"gru_4\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, 1, 12)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 1, 1, 12), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract significant phrase patterns from text\n",
    "def extract_phrase_patterns(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Create a bigram collocation finder\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    \n",
    "    # Apply a statistical measure to identify significant collocations\n",
    "    scored_collocations = bigram_finder.score_ngrams(BigramAssocMeasures.chi_sq)\n",
    "    \n",
    "    # Extract collocations that meet a certain threshold\n",
    "    significant_collocations = [bigram for bigram, score in scored_collocations if score > 3.0]\n",
    "    \n",
    "    return significant_collocations\n",
    "\n",
    "# Function to calculate punctuation similarity\n",
    "def calculate_punctuation_similarity(text):\n",
    "    # Define the set of punctuation marks\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "\n",
    "    # Count punctuation marks in the text\n",
    "    punctuation_counts = {punctuation_mark: text.count(punctuation_mark) for punctuation_mark in punctuation_marks}\n",
    "\n",
    "    # Compute punctuation similarity as the sum of squared differences between punctuation frequencies\n",
    "    total_marks = sum(punctuation_counts.values())\n",
    "    punctuation_frequencies = {mark: count / total_marks for mark, count in punctuation_counts.items()}\n",
    "\n",
    "    # Compute similarity using squared Euclidean distance between punctuation distributions\n",
    "    punctuation_sim = 0.0\n",
    "    for mark in punctuation_marks:\n",
    "        punctuation_sim += (punctuation_frequencies.get(mark, 0.0) - 1.0 / len(punctuation_marks)) ** 2\n",
    "\n",
    "    punctuation_sim = 1.0 - punctuation_sim  # Normalize to [0, 1]\n",
    "    \n",
    "    return punctuation_sim\n",
    "\n",
    "# Function to calculate sentence length similarity\n",
    "def calculate_sentence_length_similarity(sentences):\n",
    "    if len(sentences) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_length = sum(len(sentence) for sentence in sentences)\n",
    "    average_length = total_length / len(sentences)\n",
    "\n",
    "    max_length = max(len(sentence) for sentence in sentences)\n",
    "    min_length = min(len(sentence) for sentence in sentences)\n",
    "    if max_length == min_length:\n",
    "        return 0.0\n",
    "\n",
    "    normalized_average_length = (average_length - min_length) / (max_length - min_length)\n",
    "\n",
    "    return normalized_average_length\n",
    "\n",
    "# Function to calculate part-of-speech tag similarity\n",
    "def pos_tag_similarity(words1, words2):\n",
    "    pos_tags1 = [tag for sentence in words1 for (word, tag) in sentence]\n",
    "    pos_tags2 = [tag for sentence in words2 for (word, tag) in sentence]\n",
    "\n",
    "    common_tags = set(pos_tags1).intersection(pos_tags2)\n",
    "    total_tags = set(pos_tags1).union(pos_tags2)\n",
    "\n",
    "    return len(common_tags) / len(total_tags)\n",
    "\n",
    "# Function to extract function words from text\n",
    "def get_function_words(text):\n",
    "    function_words = set([\"a\", \"an\", \"the\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
    "                          \"in\", \"on\", \"under\", \"over\", \"between\", \"among\",\n",
    "                          \"and\", \"but\", \"or\", \"if\", \"because\",\n",
    "                          \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \n",
    "                          \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"])\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    function_words_in_text = [word for word in words if word in function_words]\n",
    "\n",
    "    return function_words_in_text\n",
    "\n",
    "# Function to generate n-gram transition graph\n",
    "def generate_ngram_transition_graph(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = list(bigrams(words)) if n == 2 else list(nltk.ngrams(words, n))\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for gram in ngrams:\n",
    "        G.add_edge(gram[0], gram[1])\n",
    "\n",
    "    return G\n",
    "\n",
    "# Function to compute Jaccard similarity between graphs\n",
    "def compute_jaccard_similarity(graph1, graph2):\n",
    "    nodes_set1 = set(graph1.nodes)\n",
    "    nodes_set2 = set(graph2.nodes)\n",
    "\n",
    "    intersection = nodes_set1.intersection(nodes_set2)\n",
    "    union = nodes_set1.union(nodes_set2)\n",
    "\n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Function to calculate type-token ratio (TTR)\n",
    "def calculate_ttr(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    unique_words = set(words)\n",
    "    num_tokens = len(words)\n",
    "\n",
    "    if num_tokens > 0:\n",
    "        ttr = len(unique_words) / num_tokens\n",
    "    else:\n",
    "        ttr = 0.0\n",
    "\n",
    "    return ttr\n",
    "\n",
    "# Function to detect voice in a sentence (active or passive)\n",
    "def detect_voice(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    passive_indicators = ['is', 'am', 'are', 'was', 'were', 'been', 'being', 'be', 'by']\n",
    "\n",
    "    if any(tagged_word[0].lower() in passive_indicators for tagged_word in tagged_words):\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "# Function to extract features from text\n",
    "def extract_features(text):\n",
    "    try:\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        total_words = len(word_tokenize(text))\n",
    "       \n",
    "        structure_similarity = 0.0\n",
    "        \n",
    "        if sentences:\n",
    "            structure_similarity = len(sentences) / len(sentences)\n",
    "\n",
    "        words = [nltk.pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "        spell = SpellChecker()\n",
    "        misspelled = set([spell.correction(word) for sentence in words for (word, tag) in sentence if tag.startswith('N') or tag.startswith('V')])\n",
    "\n",
    "        punctuation_sim = calculate_punctuation_similarity(text)\n",
    "\n",
    "        length_similarity = calculate_sentence_length_similarity(sentences)\n",
    "\n",
    "        active_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'active') / len(sentences)\n",
    "        passive_voice_count = sum(1 for sentence in sentences if detect_voice(sentence) == 'passive') / len(sentences)\n",
    "\n",
    "        grammar_errors_count = len(misspelled)\n",
    "\n",
    "        upper_case_count = sum(1 for char in text if char.isupper())\n",
    "        lower_case_count = sum(1 for char in text if char.islower())\n",
    "\n",
    "        n_value = 2\n",
    "        graph = generate_ngram_transition_graph(text, n_value)\n",
    "\n",
    "        phrase_patterns = extract_phrase_patterns(text)\n",
    "\n",
    "        function_words = get_function_words(text)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "        svd = TruncatedSVD(n_components=4)\n",
    "        lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        ttr = calculate_ttr(text)\n",
    "\n",
    "        # Collect features into a list\n",
    "        features = [structure_similarity, length_similarity, punctuation_sim, \n",
    "                    active_voice_count, passive_voice_count, \n",
    "                    grammar_errors_count, upper_case_count, \n",
    "                    lower_case_count, len(set(function_words)), len(graph.nodes), ttr] + list(lsa_matrix.flatten())\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        features_normalized = scaler.fit_transform([features])\n",
    "\n",
    "        # Reshape the features to match the expected input shape of the GRU model\n",
    "        # Assuming sequence_length = 1 (one feature vector per text)\n",
    "        features_reshaped = np.expand_dims(features_normalized, axis=1)\n",
    "\n",
    "        return features_reshaped\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_features: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(author_a_texts, not_author_a_texts):\n",
    "    X = author_a_texts + not_author_a_texts\n",
    "    y = [1] * len(author_a_texts) + [0] * len(not_author_a_texts)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def main(author_a_texts, not_author_a_texts):\n",
    "    X, y = load_data(author_a_texts, not_author_a_texts)\n",
    "\n",
    "    X_features = [extract_features(text) for text in X]\n",
    "\n",
    "    X_features = [x for x in X_features if x is not None]\n",
    "\n",
    "    X_features = np.array(X_features)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=40)\n",
    "\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"NaNs in X_train:\", np.isnan(X_train).any())\n",
    "    print(\"Infinite values in X_train:\", not np.isfinite(X_train).all())\n",
    "\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    author_a_texts = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "    \"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "    \"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "    \"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "    \"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "    \"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "    \"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "    \"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "    \"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "    \"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "    \"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "    \"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "    \"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "    \"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "    \"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "    \"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "    not_author_a_texts = [\"\"\"\t\t\t\t\t\t\n",
    "        As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "    \"\"\",\n",
    "    \"\"\"\t\t\t\t\t\t\n",
    "        They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\n",
    "    \"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "    I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\n",
    "    \"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "    I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "    ]  # Provide a list of texts not from author A\n",
    "    main(author_a_texts, not_author_a_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.6910 - accuracy: 0.5500 - val_loss: 0.6894 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6964 - accuracy: 0.5000 - val_loss: 0.6856 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.6875 - accuracy: 0.5500 - val_loss: 0.6835 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6902 - accuracy: 0.6000 - val_loss: 0.6823 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6826 - accuracy: 0.6000 - val_loss: 0.6827 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6909 - accuracy: 0.5500 - val_loss: 0.6805 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6902 - accuracy: 0.6000 - val_loss: 0.6773 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6886 - accuracy: 0.5500 - val_loss: 0.6744 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6782 - accuracy: 0.6000 - val_loss: 0.6720 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6805 - accuracy: 0.6000 - val_loss: 0.6708 - val_accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 0.6960 - accuracy: 0.5000\n",
      "Test Accuracy: 0.5\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Probability of belonging to the author: 0.53770155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "texts_positive = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "\"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "\"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "\"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "\"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "\"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "\"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "\"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "\"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "\"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "\"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "\"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "\"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "\"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "\"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "\"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "texts_negative = [\"\"\"\t\t\t\t\t\t\n",
    "As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "] \n",
    "\n",
    "# Concatenate positive and negative examples\n",
    "texts = texts_positive + texts_negative\n",
    "labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Function to predict if text belongs to the author\n",
    "def predict_authorship(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n",
    "    probability = model.predict(padded_seq)[0][0]\n",
    "    return probability\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"A new text by Author A.\"\n",
    "probability = predict_authorship(test_text)\n",
    "print(\"Probability of belonging to the author:\", probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (29,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m voice_detection_features \u001b[38;5;241m=\u001b[39m [voice_detection(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Concatenate textual features\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m textual_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((padded_sequences, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase_patterns_features\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(punctuation_similarity_features)[:, \u001b[38;5;28;01mNone\u001b[39;00m], np\u001b[38;5;241m.\u001b[39marray(sentence_length_similarity_features)[:, \u001b[38;5;28;01mNone\u001b[39;00m], np\u001b[38;5;241m.\u001b[39marray(pos_tag_similarity_features)[:, \u001b[38;5;28;01mNone\u001b[39;00m], np\u001b[38;5;241m.\u001b[39marray(function_words_features), np\u001b[38;5;241m.\u001b[39marray(ngram_transition_graph_similarity_features)[:, \u001b[38;5;28;01mNone\u001b[39;00m], np\u001b[38;5;241m.\u001b[39marray(type_token_ratio_features)[:, \u001b[38;5;28;01mNone\u001b[39;00m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[0;32m    154\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(textual_features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (29,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "\n",
    "# Define functions for additional features\n",
    "\n",
    "def phrase_patterns(text):\n",
    "    tokens = text.split()\n",
    "    bigram_counts = Counter(bigrams(tokens))\n",
    "    significant_collocations = [bigram for bigram, count in bigram_counts.items() if count > 1] # Example threshold for significance\n",
    "    return significant_collocations\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "    punctuation_count_text1 = sum(text1.count(char) for char in punctuation_marks)\n",
    "    punctuation_count_text2 = sum(text2.count(char) for char in punctuation_marks)\n",
    "    return min(punctuation_count_text1, punctuation_count_text2) / max(punctuation_count_text1, punctuation_count_text2)\n",
    "\n",
    "def sentence_length_similarity(text1, text2):\n",
    "    sentences_text1 = nltk.sent_tokenize(text1)\n",
    "    sentences_text2 = nltk.sent_tokenize(text2)\n",
    "    avg_length_text1 = sum(len(sent.split()) for sent in sentences_text1) / len(sentences_text1)\n",
    "    avg_length_text2 = sum(len(sent.split()) for sent in sentences_text2) / len(sentences_text2)\n",
    "    return min(avg_length_text1, avg_length_text2) / max(avg_length_text1, avg_length_text2)\n",
    "\n",
    "def pos_tag_similarity(text1, text2):\n",
    "    pos_tags_text1 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text1))]\n",
    "    pos_tags_text2 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text2))]\n",
    "    pos_tag_set1 = set(pos_tags_text1)\n",
    "    pos_tag_set2 = set(pos_tags_text2)\n",
    "    return len(pos_tag_set1.intersection(pos_tag_set2)) / len(pos_tag_set1.union(pos_tag_set2))\n",
    "\n",
    "def function_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
    "    return function_words_text\n",
    "\n",
    "def ngram_transition_graph(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "    return transition_graph\n",
    "\n",
    "def ngram_transition_graph_similarity(graph1, graph2):\n",
    "    nodes_graph1 = set(graph1.nodes)\n",
    "    nodes_graph2 = set(graph2.nodes)\n",
    "    intersection = nodes_graph1.intersection(nodes_graph2)\n",
    "    union = nodes_graph1.union(nodes_graph2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "def voice_detection(sentence):\n",
    "    # Example implementation using simple keyword matching\n",
    "    if 'is' in sentence.split() or 'are' in sentence.split():\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "# Your original code starts here\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "texts_positive = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "\"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "\"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "\"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "\"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "\"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "\"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "\"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "\"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "\"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "\"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "\"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "\"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "\"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "\"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "\"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "texts_negative = [\"\"\"\t\t\t\t\t\t\n",
    "As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "] \n",
    "\n",
    "# Concatenate positive and negative examples\n",
    "texts = texts_positive + texts_negative\n",
    "labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Extract additional features\n",
    "phrase_patterns_features = [phrase_patterns(text) for text in texts]\n",
    "punctuation_similarity_features = [punctuation_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "sentence_length_similarity_features = [sentence_length_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "pos_tag_similarity_features = [pos_tag_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "function_words_features = [function_words(text) for text in texts]\n",
    "ngram_transition_graphs = [ngram_transition_graph(text) for text in texts]\n",
    "ngram_transition_graph_similarity_features = [ngram_transition_graph_similarity(ngram_transition_graphs[i], ngram_transition_graphs[i+1]) for i in range(len(texts)-1)]\n",
    "type_token_ratio_features = [type_token_ratio(text) for text in texts]\n",
    "voice_detection_features = [voice_detection(text) for text in texts]\n",
    "\n",
    "# Concatenate textual features\n",
    "textual_features = np.concatenate((padded_sequences, np.array(phrase_patterns_features), np.array(punctuation_similarity_features)[:, None], np.array(sentence_length_similarity_features)[:, None], np.array(pos_tag_similarity_features)[:, None], np.array(function_words_features), np.array(ngram_transition_graph_similarity_features)[:, None], np.array(type_token_ratio_features)[:, None]), axis=1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(textual_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Concatenate(),\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Function to predict if text belongs to the author\n",
    "def predict_authorship(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n",
    "    # Extract additional features\n",
    "    phrase_patterns_features = phrase_patterns(text)\n",
    "    punctuation_similarity_features = punctuation_similarity(text, reference_text)\n",
    "    sentence_length_similarity_features = sentence_length_similarity(text, reference_text)\n",
    "    pos_tag_similarity_features = pos_tag_similarity(text, reference_text)\n",
    "    function_words_features = function_words(text)\n",
    "    ngram_transition_graph = ngram_transition_graph(text)\n",
    "    type_token_ratio_features = type_token_ratio(text)\n",
    "    voice_detection_features = voice_detection(text)\n",
    "    # Concatenate textual features\n",
    "    textual_features = np.concatenate((padded_seq, np.array(phrase_patterns_features), np.array(punctuation_similarity_features)[:, None], np.array(sentence_length_similarity_features)[:, None], np.array(pos_tag_similarity_features)[:, None], np.array(function_words_features), np.array(ngram_transition_graph_similarity_features)[:, None], np.array(type_token_ratio_features)[:, None]), axis=1)\n",
    "    probability = model.predict(textual_features)[0][0]\n",
    "    return probability\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"A new text by Author A.\"\n",
    "probability = predict_authorship(test_text)\n",
    "print(\"Probability of belonging to the author:\", probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'The'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 225\u001b[0m\n\u001b[0;32m    223\u001b[0m ngram_transition_graph_similarity_features \u001b[38;5;241m=\u001b[39m ngram_transition_graph_similarity_features[:max_features_length]\n\u001b[0;32m    224\u001b[0m type_token_ratio_features \u001b[38;5;241m=\u001b[39m type_token_ratio_features[:max_features_length]\n\u001b[1;32m--> 225\u001b[0m function_words_array \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_words_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_function_words_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[:max_features_length]\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Convert all features to numpy arrays\u001b[39;00m\n\u001b[0;32m    228\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(padded_sequences)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:1132\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m-> 1132\u001b[0m trunc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(trunc, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1138\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'The'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Add this import\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "\n",
    "# Define functions for additional features\n",
    "\n",
    "def phrase_patterns(text):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < 2:\n",
    "        return []  # Return empty list for texts with less than two words\n",
    "    bigram_counts = Counter(bigrams(tokens))\n",
    "    significant_collocations = [bigram for bigram, count in bigram_counts.items() if count > 1]  # Example threshold for significance\n",
    "    return significant_collocations\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "    punctuation_count_text1 = sum(text1.count(char) for char in punctuation_marks)\n",
    "    punctuation_count_text2 = sum(text2.count(char) for char in punctuation_marks)\n",
    "    return min(punctuation_count_text1, punctuation_count_text2) / max(punctuation_count_text1, punctuation_count_text2)\n",
    "\n",
    "def sentence_length_similarity(text1, text2):\n",
    "    sentences_text1 = nltk.sent_tokenize(text1)\n",
    "    sentences_text2 = nltk.sent_tokenize(text2)\n",
    "    avg_length_text1 = sum(len(sent.split()) for sent in sentences_text1) / len(sentences_text1)\n",
    "    avg_length_text2 = sum(len(sent.split()) for sent in sentences_text2) / len(sentences_text2)\n",
    "    return min(avg_length_text1, avg_length_text2) / max(avg_length_text1, avg_length_text2)\n",
    "\n",
    "def pos_tag_similarity(text1, text2):\n",
    "    pos_tags_text1 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text1))]\n",
    "    pos_tags_text2 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text2))]\n",
    "    pos_tag_set1 = set(pos_tags_text1)\n",
    "    pos_tag_set2 = set(pos_tags_text2)\n",
    "    return len(pos_tag_set1.intersection(pos_tag_set2)) / len(pos_tag_set1.union(pos_tag_set2))\n",
    "\n",
    "def function_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
    "    return function_words_text\n",
    "\n",
    "def ngram_transition_graph(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    transition_graph.add_nodes_from(ngrams)\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "    return transition_graph\n",
    "\n",
    "def ngram_transition_graph_similarity(graph1, graph2):\n",
    "    nodes_graph1 = set(graph1.nodes)\n",
    "    nodes_graph2 = set(graph2.nodes)\n",
    "    intersection = nodes_graph1.intersection(nodes_graph2)\n",
    "    union = nodes_graph1.union(nodes_graph2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "def voice_detection(sentence):\n",
    "    # Example implementation using simple keyword matching\n",
    "    if 'is' in sentence.split() or 'are' in sentence.split():\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "# Your original code starts here\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "texts_positive = [\"\"\"The 4 tile mural I worked a week on went into the kiln,along with everything else, and thankfully everything was ok except the mural.  The underglaze was too thick, and the glaze was too thick (I decided to float glaze the tile for fear of smearing all the black.)  The glaze actually picked up the black, moved it over, and then fired in a big blob in several spots.  Live and learn.  I just don't know if I have it in me to make another one.   I'll have to dig deep for this one.  I guess I should be thinking of the kiln as half full instead of half empty!\"\"\",\n",
    "\"\"\"I'm so tired today because I was up all night worrying about the kiln firing.  It smelled something fierce, and I was worried we were all going to die of carbon monoxide poisoning in our sleep.  Plus, I kept hearing banging, which I hope wasn't anything exploding in the kiln, but I haven't found out yet because it's still 600 degrees the next day.\"\"\",\n",
    "\"\"\"I happily called the lady about the tile mural that was just set to see how great it looked.  To my suprise, she was very shocked at how warped the tiles look when set.  I was so upset I had nightmares all night and obsessed about it all day. Luckily, she called me back the next day to apologize because she was in a bad mood.  P.s.  Their check was returned the next day.\"\"\",\n",
    "\"\"\"Today I must get 100 bisque white tiles today.  My supplier of 7 years has my order of 4 weeks ago delayed in Mexico in customs.  Note to self:  Remember Murphy.  Never assume anything.\"\"\",\n",
    "\"\"\"MUST PAY SALES TAXES TODAY OR ELSE!  Something about a deadline is a sure cure to get one off one's butt.  Yesterday placed a free ad in the local paper. Cost for free, and for free stuff-This should be VERY interesting to see the response. This week I should test some cone 5 glazes just for the heck of it.\"\"\",\n",
    "\"\"\"fired the last mural.  now I'm depressed.  it's kind of like planning a wedding all year and then the day after you have nothing to do.  Not that I have nothing to do, just no one telling me what I have to do. (work orders) I could have been better prepared with a summer camp to fall into; I'm certainly getting phone calls.  But I was too sick to plan a couple of months ago.\"If you fail to plan, you plan to fail!\"  On another note, I am regretting not buying the small test kiln.  Twice , and now three times I have lost a customer or an opportunity because I didn't have a smaller kiln to do test tiles in or cone 10, or whatever.  I will be getting one soon.\"\"\",\n",
    "\"\"\"make hump molds make slab shapes:  babies, women,large star windchimes, small stars make multi-level vase  garden tiles or initial tiles peacock tray baby stuff for daniel-frame w letters, ornament, train plaque,send tiles for footprints(also to Liz) mix colored dipping glazes in quart containers from Smart and Final marketing tools needed: scout flyers party flyers new maps or general flyers summer camp schedule flyers baby footprint postcards fix website babyfootprint gift certificates\"\"\",\n",
    "\"\"\"well, the Robinson mural worked out.  4 tiles cracked or broken, all re-painted.  Hope they like them.  The Cordillera mural is getting bigger everyday, now 75 more tiles than anticipated.  They look beautiful going into the kiln.  Double stilting them for less warping.  Had a chance to teach a self-portrait class for children;one of my favorites.  This week had my second student for handmade tiles who wants to go into business as such.  After a little trepidation, I gave into the fact that I am a teacher, and so I teach.  She however wants to learn clay crafting, versus painting, so this is fun.  Got to break open my plaster, and discovered I have a love-hate relationship with plaster carving. Thought it might be easier if I colored the plaster in three after mixing the batch and pour it in layers, so you could see what you are doing. I do love pouring molds.\"\"\",\n",
    "\"\"\"I have a school painting on Monday.  For some reason I have a total mental block about the glazes.  Couldn't get the bottles I want,don't want to use the old ones.  Debating which glazes to use, the yucky cheap ones, or the nice expensive ones.  Usually, I have no choice.  I just don't want to spend any more money on half-used glazes. I have a hundred different colors, but not enough of any one to fill 12 bottles.\"\"\",\n",
    "\"\"\"Well, I got a call from the mural organizer who asked if the tiles would be ready to view tomorrow.  Of course, I hadn't even re-fired the tiles yet, or done the two \"dog paw\" accents, or the tile that no one wanted to paint, or the stupid 4 tile center.  So I had to fess up, and beg for more time.  Of course, now that my rear is on the line, I managed to become amazingly inspired and finish the two accent tiles while my kids got ready for school.  I cancelled a doctor's appointment, and painted the stupid center.  Not perfect and beautiful like the last one, but it's there at least.  Now the dilemma of how lazy do I want to be?  Should I leave it black and white, or color it in, and if I color it in, how much color should I bother with?  Or is it passable as it is?  Mostly, I just fear total failure like the last time.\"\"\",\n",
    "\"\"\"The great news is the mural I started working on last year, I was asking $6/tile. At that rate I wasn't going to get anywhere, so I asked for $8/tile. I've waited a year, and the lady felt so bad, she said they would give me $10/tile..Yippee!\"\"\",\n",
    "\"\"\"Today I need to start mixing glazes for the last tile painting for the school year. The question is can I get 200 of the new nozzle bottles I tried out this weekend by Friday shipped and filled? Or should I go with what I have , which now seems like crap compared. They of course will never know the difference. Re-painting 2 tiles that broke, refiring two broken tiles, and refiring 6\" tiles that the glaze didn't flatten out all the way. Must be done asap because they are setting this weekend, and has already been delayed once. Guess I better start re-painting those stupid 4 part mural tiles. Ugh!\"\"\",\n",
    "\"\"\"Today I had a glass artist over for a firing.  It was a good excuse to do some research on fused glass.  My past attempt at painted,fused, and slumped glass turned out so-so.  I have some material already, so it would be nice to learn how to use it properly.  She is an older woman, and I had a nice time talking about glass with her; how she sells her stuff (in Venezuela), and just about life as an artist.  I custom programmed my kiln (she usually does it manually).  It was interesting to know that you can open a red hot flaming kiln with glass inside with no breakage.  I can't wait to see what is inside.  Learned about cutting glass, slumping in bisque, applying enamels to gum arabic through a sifter, using elmers glue to stick shards of glass together, using a metallic sharpie to write with on glass, and firing inclusions and dichroic glass.  Like to test frit on clay and glass.  baking soda makes bubbles between glass (use sparingly).Use of fiber paper vs. kiln wash.  slump at a higher temperature and fire paint at a lower temp to keep intensity of color.Use ceramic frames for drop molds (dishes)  Cracked bisque works fine as a glass saggar! \"\"\",\n",
    "\"\"\"Attending NCECA in San Diego in 2003 was a turning point for me in many ways. Little did I know when several people asked if I was going, that it was more than I could have imagined. I went reluctantly, tired from work, but curious. When I got there I was lost, and wandered aimlessly, not knowing what I had walked into. I paid my $65, and set off to see what it was that everyone thought was so great. I wandered in and out of lectures and demonstrations. I was most interested in the business lectures, only really wanting to find a way to make a living doing what I love. I wandered through exhibits, spying the mug sale, the cone box contest, and the k-12 children's entries. Was my stuff up to par with the \"real\" teachers who had a degree? I vowed I would enter next year, just to be competitive.(I didn't , but that's another story). I was really excited to go to the basement area where everyone was selling everything. Tools I didn't know existed, schools beckoning (asking myself, how would my life had been different if I had majored in ceramics, and not married and had children), companies throwing samples my way by the caseloads. Paper, paper, and more paper. The next day was better, knowing that I was there to learn as much as possible in a short time period. I sat through lectures and demos. I absorbed conversations and watched people look and watch. When I got back home to my studio, I wasn't the same. When I left, I was a housewife that had more than a passing interest in a hobby. I was an entrepeneur, trying to find the holy grail that would catapault me from sometimes breaking even to supporting myself. When I came back, I felt like an artist. I realized I knew much more than I thought. I realized that the real world experience I had jumped into blindly had given me more opportunity than most people get in a lifetime of study. I saw my life 20 years from now, and 40 years from now, planning what I would like to do when the kids are grown and this season of my life had passed. I saw myself, 70 years old, touching the clay and asking the questions...... First , when I got back to work, I was engulfed by production and exploring new avenues of business. I taught with a new confidence, that yes, I knew what I was doing with what I did, and everything else would come later. I experimented more, and slowly the studio became a studio, not a storefront. I had an apprentice, and a muse. I would spend hours with the music on, in the silent of my space, pondering the next projects, or working with ferocity. I realized the sacrifices I had made as an artist, in my ventures as a businesswoman. I had no extra time or energy to \"create\" for the sake of creating, going into the unknown with no \"agenda\". I did not know what that felt like. I closed the studio. It felt like death. Where was my purpose without a store to support? I hated being just a mother. I almost couldn't do it, and didn't have to. I had renegotiated my lease for pennies. But I knew I had to cut off my arm for another one to grow literally. I moved the studio to my home, like a lot of potters do. I am lucky that I have patient people who live with me that accept the studio taking over the whole of the house. The driveway, the garage, the courtyard, the livingroom, the office, even in the bedroom. They know my sanity lies in it.\"\"\",\n",
    "\"\"\"ceramic doorhangers with addons fused in themes: horse,flower ect-blank for dry erase ceramic lightswitch faceplates with addons fused in themes also pour lightswich plates then handbuild over them and around them gifts to do:scriffito doorhangers for stefani,emily,and natalie daniel and also ceramic babybottle bank for daniel and babyblocks frame family tree large tile with handbuilt additions and a \"wall\" or fence around it GO GET 200 BOTTLES FROM C +C WHEREHOUSE AND PICKUP AND RANDIS\"\"\",\n",
    "\"\"\"recently tried a new dipping clear that unfortunately was discontinued due to lead leeching. Won't use it on dinnerware, but, oh my god, it is beautiful. Good thing I didn't return it to the factory like they wanted. Wonder if they'll still sell it with a different label warning. They should! I will write them because they took an uneccessary beating because of the mistake.\"\"\"]  # Provide a list of texts from author A\n",
    "texts_negative = [\"\"\"\t\t\t\t\t\t\n",
    "As promised, here's the next instalment of bus mongs.  I bet you've been looking forward to this, haven't you...   2. Bus Monitors  Now, in every walk of life, in every profession, in every place where humans exist there are heirachies.  I accept these heirachies with varying degrees of grace.  But, if there is one thing that makes me want to stick two fingers up to \"The Man\" and form a rock n' roll band, it's people who assume importance and status without any requirement for them to exist.  I have to be careful here to convey exactly what I mean.  I want you to understand.  Two elderly women on my bus service have elected themselves bus monitors.  As far as I know, there was never any formal nomination.  Let's be clear; these people have assumed the position of bus lords.  This basically involves:  a)  Sitting right behind the driver and shouting conversations at him in a \"spirit of the blitz\" style dialect.   Eg: \"Ooh 'ello Frank, I 'ope you'll be putting yer foot down today, my Bert's expecting his dinner!\".   Essentially, mindless, insiduous prattle.  The volume at which these conversations take place cow everyone around them into aural submission.  No-one can read, listening to music is impossible, and quiet chats with friends are verboten.  Essentially, this is an exercise in illustrating that they are friends with the driver, and so assume some of the importance they crave by association.  They rarely look around or even notice other bus people, the bus people they nominally claim to represent.  b) Getting on the bus first.  This is truly the raison d'etre of the bus monitor.  They force themselves, elbows and handbags flailing, onto the buses first for three reasons.  Firstly, this (again) gives them the air of importance and status that they crave.  Secondly, getting on the bus first gives them first choice of seats - they can then position themselves in prime bus real estate for loud driver conversations.  Thirdly, this allows them to have protracted chats with the driver, and fumble for their tickets whilst a large queue stretches back outside getting drenched in the rain.   c)  On the rare occasions where a new driver has been in place (I always feel great sympathy for these hapless footsoldiers, thrust naively onto the battlefield), bus monitors enter a state of heightened awareness.  Not content with shouting often unnecessary directions into the side of the driver's head, they will also offer information on who normally gets on at those stops, whether to wait for them if they aren't there and other classified, bus-monitor-priveleged information.  MI5 themselves would have dossiers less detailed on members of the Taliban.  d) On the rarer still occasions where the bus makes a wrong turning, the bus monitors become a flurry of activity.  \"Wrong way!\" they shout, whilst looking around incredulously at fellow passengers, as if the driver had defaced a war memorial.  e) Bus monitors are the guardians of bus protocol.  Although they can blatantly disregard other passengers, any kind of ignorance on the part of other passengers is met with disapproving looks.  Any breach of accepted protocol, whether or not you have ever been in this country before, been on a bus before, have the use of your arms and legs etc is met with their clear disgust.    Wedged into their seats with their old-woman paraphanelia, these are actually quite sad individuals.  I can only imagine the voids in their lives must have become slightly less yawning when they found solace in bossing people about on buses.  In two years of bus usage, I have yet to see them justify their self-appointed positions, and on top of it all, they clearly enjoy this.  They act like they are doing me a favour.  If getting on my nerves and stinking of Parma Violets is somehow helping me, I can only marvel at what my shortcomings must have been to start with.  Perhaps I was too relaxed and the bus didn't smell of Parma Violets enough.  We can but wonder.  This is just a small sample of the irritations that these people cause, and for once, I am not just saying that because I can't think of anything else.  It really is just a small sample.\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "In case any of you people care, I am one of the hardy souls of this world who commute to work.  Yes, I get the bus.  And I like it.  In fact, a 30-45 minute journey in the morning is an unbelievably relaxing way to get to work.  In a carefully temperature controlled cocoon, you can pop a bit of music on and watch the scenery, leaving all the actual \"doing\" to someone else.  Namely the driver.  It's difficult to convey the benefits of merely sitting, doing nothing, on an adequately comfortable seat, and not having to worry about anything for half an hour.   This is, in theory, superb.  However, my idyll in this metal tube with wheels is frequently tested by putrid invaders.  Invaders of the worst kind.  Space invaders, if you like.  I have attempted to categorise them in a new series, starting below.  All users of public transport will identify them.  And though they have many names, their presence is unmistakable.    1.  The feckless youngster.  Yesterday a regular user of my bus service, a feckless young girl, brought into sharp focus why I hate other bus people so much.  Perhaps I should avoid the term \"bus people\", as this either suggests a gypsy-like existence in an abandoned bus, or people who actually resemble buses.  Either way, it's not what I am trying to say.  Basically, I shall now define \"bus people\" as people who get the bus, in order to avoid confusion.  Anyway, her crimes against me are myriad and serious.  In an international court of bus law (ICBL) she would probably be tried and sentenced to death.   We have a distinct history.  It all started when, about 18 months ago, this individual started to wait at my stop.  Looking little different from the usual slack-jawed windowlickers of my home town, I paid little heed, instead assuming my favourite bus-waiting position of roughly perpendicular to the shelter in order to look up the hill, legs heroically akimbo like the Collossus of Rhodes.  I was somewhat surprised when she boarded the private vehicle which takes me to work.  Perhaps I had misjudged her, despite her appearance and demeanour.   A few weeks passed with respectful silence between us.  All was well, and I felt we had formed an invisible bond of ignoring eachother.  But then, a terrible thing happened. One day, she approached the bus stop, and I was unfortunate enough to momentarily lock eyes with her.  This, as most people would doubtless know, is a pre-cursor to some kind of conversation.  To my alarm, I had discovered that my mouth was open as well.  Snapping it shut, I did my best to rescue the situation.  I noticed that something was different about her... something was amiss.  My mind raced to pin it down.  Of course! Her hair.  She had dyed her hair.    \"I like your hair\" I said, before the full disastrous impact of what I had done hit me.   I had sparked up a conversation with a bus person!  No more louche days reading in the window seat, listening to the latest grooves.  No more beautiful days watching the speeding countryside.  I would be sucked in, engulfed in this desperate harlot's whirlygig of hair chat.  Maybe the whole situation would escalate to shopping, or worse, work.  Oh cruelest of all fates!!! Why?  Why did my tongue forsake me, when I most needed it to stop it's diabolical dance!  \"Oh, thanks, I only di....\"  By this time I had run onto the bus.  I couldn't risk more contact or possible friendship with this woman.  She would doubtless destroy what little peace I could wrestle from my day.  More would come of this, I was sure, and indeed it did.  An insidious campaign of irritation followed.  Once, the bus arrived ridiculously early, and we both missed it.  An uncomfortably long period of waiting ensued, before it was clear that no bus would be coming.  I was forced by the situation to offer a non-commital \"I think we've missed it\".  She rudely turned her back and stormed off, frantically jabbing at her mobile phone.  As we were both bound for the same destination, and we had both missed the same bus, a nice gesture would have been to offer a place in the lift she was undoubtedly arranging (although I would have turned her down on principle).  Instead she glared at me as if I had somehow Karmically arranged the absence of the bus in order to ruin her day.    This week alone, of the 5 days which are busable, she has neglected to have a ticket on 3 days.  This is not only gyppo behaviour, but is also an embarrassing social situation, which I seek to avoid at all times.  All 3 times, she has been \"let off\" the fare, which has only exponentially increased my contempt for her.  Then there's the running.  I get on the bus first, due to clever kerbside positioning.  She gets on immediately afterwards, and I swear she runs directly behind me, hurrying me along.  I feel obliged to hurl everything into the seat and dive out of her way.  Why she feels the need to hurtle up the bus is a mystery to all except me.  To me, it is but more evidence of her idiocy.    It's clear she thinks she is the J-Lo of the bus community.  Well she got her commupance today alright.  As the bus drew near, some schoolchildren passed us.  Their cries of \"She's got a £2 handbag!\" were delight to my ears as they systematically humiliated my self-important co-busee, who dresses like someone doing an impression of a character from Sex in the City down on their luck.  Other times the bus has pulled away, as she frantically runs behind it, and I have merely sat, smiling smugly.  Oh, good will have it's days.  But such are the cosmic forces of yin and yan that my victories are only part of a timeless struggle.  One which must be won at all costs. \n",
    "\"\"\",\n",
    "\"\"\"\t\t\t\t\t\t\n",
    "They're Good, but Let's Not Start Any Wars Over Them   Well, in a new section of the page, I look at music and decide whether it's any good, for the benefit of you, the reader.  I will call it \"My Opinion on Music\".  Or \"Reviews\".  Yeah, that one.       Well, Franz Fedinand (or \"The 'Nand\" as I haven't christened them) are a Scottish indie type outfit.  That doesn't do them justice - \"Indie\" is used far too loosely nowadays to have any real meaning.  In this instance, let's take it to mean that they are progressive and slightly non-conformist. What's their sound like? I'll tell you.  They owe a big debt to Tom Verlaine and Television.  That kind of skewed funkiness cut through with some melodious guitar work and bass lines.  Then, in other instances, lead singer Alex Kapranos sounds like a more coquetteish Ian Curtis.  Either way, the mix spells funky and the music spells good. There's flashes of Iggy Pop's The Idiot in the density of some of the tracks, flashes of The Pixies in the pop-artful approach to lyrics.  Bizzarely, some parts of the album also recall Blondie at their Parallel-Lines zenith.  You work it out.  I can't be bothered. I've read and heard comparisons to \"The 'Werk\" (Kraftwerk).  This is pretty crass on the surface - there's snatches of German on some tracks, which is probably the main reason for the comparisons.  However, having said that, there is an undercurrent of a peculiarly teutonic baroque.  Difficult to pin down, but themes like darkened cinemas and dancing with men called Michael conjure a particularly Weimar atmosphere, in my mind at least. So we've established that their influences are a smorgasbord of left-field  artists.  But what is the driver that make The Nand stand out? Well there are moments of adreneline pumping brilliance.  The type that makes you want to go out and have a fight or run really fast, like all the best music does.  The opener \"Jacqueline\" is a multi-layered romp which displays a joy for words and sound which is refreshing.  It's slightly self-consciously skewed - it's not full-on absurdity, but has kind of taken a toffee hammer and tapped the norm hard enough to make it less normal.  Rhyming \"spectacles\" with \"erecticles\" is one such example.  The barnstorming chorus, which extols the virtues of holidaying is another.  A well rounded debut, all in all, but as a friend said to me after the Stroke's first album - \"Where do they go from here?\".  They might have just painted themselves into a corner by releasing something so polished so soon. Time will tell, but until that time tells, don't go assassinating any Archdukes.  \n",
    "\"\"\"    ,    \"\"\"\t\t\t\t\t\t\n",
    "I can't think of anything to write today, so this is going to go one of two ways.  Either I will turn this into an entertaining missive on not being able to write anything, or it will just grind to a halt, teetering precariously on the keep/delete axis.  Nearly ground to a halt after that sentence.  I suppose this hinges now on how long I have to continue for to make this a missive.  I don't know if there is a central agency which sets the length of missives, tracts and statements.  If not, there should be.  It would at least prevent confusion at times like this. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I thought today about forming a Lonely Club.  Not that I'm lonely, but it seemed a compassionate thing to do.  To get lonely people together in a non-threatening atmosphere.  I could send out leaflets which say things like \"Spend a lot of time on the Playstation?\" or \"Lonely?\".  I think there would be a good response.  And then I could franchise it out, to other Lonely Co-ordinators - an entire network of Lonely Clubs could spring up, eradicating loneliness forever.  But then I thought, what if no-one turned up?  Could there be anything more tragic than someone organising a Lonely Club meeting and ending up totally alone.  That could push some Lonely Club organisers over the edge.  I suppose they could work with a friend, you know, so they didn't get Lonely. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I got my Digital Camera and I pretty much have it all figured out, I just need to know how to get pictures posted up on here now.  So off I go to explore and hopefully the next post will have a picture.  WEEEEEEEEEEEEEEEEEEEEE!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Did you ever wake up one day and everything just seemed to go totally right?    You actually want to get out of bed even though it is 4:00 a.m., your shower is awesome, your son is ready ON TIME for school, you look decent, the road to work is practically flawless and your favorite songs are all playing on the radio, your Mocaccino is Orgasmic and your Boss is in such a chirpy mood...    Well, today is that day for me and it just keeps on getting better.  My Boss told me that today was THE DAY for my bonus (Prefect timing because I have really been wanting that Digital Camera).  My cousin is in town and I rarely see her (she moved to Alberta, then Yellow Knife, now Niagara Falls) and a bunch of us are going to go for drinks tonight so its going to be picture time.  Its not sunny outside, but its warm (mostly humid but it's ok cause I left my hair curly today).  I am going to actually have time to take a full hour lunch and I will get to do so with my dad, brother and cousin.  And last but not least (or maybe Least but not last?) I am actually getting quite a bit of work done (well not right THIS second) so I wont feel guilty this weekend and think about all the things I have to do on Monday.  I dunno, maybe this is lack of sleep talking, but it really is a great day, it doesn't take much to please me huh? LOL  Ok, well back to work I go, have a good weekend.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I think I have had enough with men for at least the next 5 years.  Either I am super unlucky or I am a real Bitch (I am leaning more towards the earlier).  I can't seem to meet anyone half decent even if my life depended on it.  I am not talking about a serious, lets be monogamous type of relationship, I am talking about a simple friendship.  I have been talking to this guy for 4 years, Matt, aka Mr. Arkansas, we have shared every little secret (or at least I did) and every thought and fantasy and feeling and out of no where I am a bitch because I demand a little more after four fucking years.    My fuck friend on the other hand... He's in general not too bad... I just only see or hear from him when he wants some, god forbid Cindy has any needs.  There have been other guys over the last 2 years, one so called friend only called me or came by to smoke-up and watch movies when he was single, once he found himself a new fling, bye bye Cindy.  I called him on it the last time it happened, he said he would try and squeeze me in, I told him not to bother and guess what, that was the end of that.  This other guy, claimed he liked me and wanted to date me and so on, it was all BS.  I am ok with casual sex, I am only human and have needs too.  If that's all you want, just be up front about it and whatever decision I make at least it is my fault and I can't blame anyone but myself.  You would think that is pretty simple but no, not in this world.  I don't really know what I want and I am not out there trying to hook up with anyone, however I do know one thing, I want a friend (a male one, actually a woman would do just fine), I guess I am looking for a friend that I can be intimate with and also depend and trust.  I don't know if that makes sense, I'm so confused, I'm so tired of being alone.  Blah!!!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "One of those killer days where nothing goes right for the boss, and you get blamed or the littlest thing happens and you get tons of shit.  I have a pounding head ache, I haven't had lunch or any break as a matter of fact.  All I want to do is go home, eat and take a long ass MOFO bubble bath.  Ciao!\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Is my interest in this whole thing waning?  It wouldn't surprise me.  I had a go with a couple of these before.  They turned out rubbish. On the other hand, this is day 2.  You're still here.  So am I.  Both of I.  So this has turned out great!  Maybe a 2nd anniversary party should be arranged.  I have just the people in mind.\n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "So I had a new patient yesterday, a man in his 90's, a sweet old man who is probably very lonely.  I always ask my patients how their weekend was or if anything special happened in their week and one thing led to another and I found out that his grandson never visits.  He lives in town and it is so sad that he never goes and visits.  The patient was telling me that he has never even met his great grand kids.  I just couldn't believe it.  I asked him if he had any other family in town and he mentioned a grand daughter.  When she came to pick him up, I pulled her aside and told her she needs to bring her kids to visit their grand father and great grand father.  She said that she would try and make an effort but she didn't really know what to say to her kids.  I was floored.  It's your family, you don't need to say anything special, just go and visit.  They left and I just felt so sad.  I really hope she does make an effort.  I asked my kids if they would ever not visit their grand parents and they said they would be upset if they couldn't visit them.  I hope I am raising them right, I would be so sad if they didn't visit me when I was old and couldn't do things on my own. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "Today is the anniversary of Elvis' death.  What do you think happened to Elvis?  Is he still alive?  I don't think he is, I mean look at all the cheeseburgers he ate... However, I am one of those people who like to believe that he is still alive, you know, just chillin, living on some remote island with Tupac and Biggy.  You know, that's really not that weird of a theory. \n",
    "\"\"\" , \"\"\"\t\t\t\t\t\t\n",
    "I'm gonna go ahead and assume that a majority of the people who read this don't watch much t.v. or if you do, its most likely Discovery, History, National Geographic or some other channel that requires you to think a little bit (come on, if you watch the learning channel, you at least have to think a LITTLE).  I too, watch those channels, but every now and then, I like to shut off my brain and watch some mindless crap.  So last night, I watched one of my favorite movies (mainly cause it makes me laugh) \"Sweet Home Alabama\".  Love it.  If you have never seen it, shame on you!  You need to go and rent it right now, go ahead, I'll wait....   Ok, now that you have seen it, don't you just love it?  It's so cheesy and so predictable but you know what, I love those types of movies.  Another movie I love, \"Two Weeks Notice\", have you seen it?  Its another good one.  I have to say, Sandra Bullock and Hugh Grant make a great pair.  Oh, and lets not forget \"Bridget Jones' Diary\", how can you NOT love that movie?  You gotta love Bridget, she's awesome.  Hmm, what other movies do I like?  OH, \"How to Lose a Guy in Ten Days\", Love that one too.  Kate Hudson and Matthew McConaughey are awesome together.  Ok, I think that's enough for now, I could go on and on.  You should write to me and let me know what movies you like to watch, I am always on the look out for a good chick flick.  Ok, well, I'm out, have a great day!   \"\"\"\n",
    "] \n",
    "\n",
    "\n",
    "# Concatenate positive and negative examples\n",
    "texts = texts_positive + texts_negative\n",
    "labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')  # Padding sequences using pad_sequences\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert function words to numerical indices\n",
    "word_to_index = {}  # Create a dictionary to map words to indices\n",
    "index = 1\n",
    "for words_list in function_words_features:\n",
    "    for word in words_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = index\n",
    "            index += 1\n",
    "\n",
    "# Remove non-numeric elements from function_words_array\n",
    "function_words_array_numeric = [[word for word in words if word.isdigit()] for words in function_words_array]\n",
    "\n",
    "# Pad each list of numeric function words to the maximum length\n",
    "padded_function_words_features = [words + ['0'] * (max_function_words_length - len(words)) for words in function_words_array_numeric]\n",
    "\n",
    "# Convert padded function words features into a 2D NumPy array\n",
    "function_words_array_padded = np.array(padded_function_words_features)\n",
    "\n",
    "# Pad sequences to ensure consistent length\n",
    "function_words_array_padded = pad_sequences(function_words_array_padded, maxlen=max_function_words_length, padding='post')\n",
    "\n",
    "# Convert each list of function words to numerical indices\n",
    "numerical_function_words = [[word_to_index[word] for word in words_list] for words_list in function_words_features]\n",
    "\n",
    "# Pad the sequences to the maximum length\n",
    "padded_function_words = pad_sequences(numerical_function_words, maxlen=max_function_words_length, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "function_words_array = np.array(padded_function_words)\n",
    "\n",
    "\n",
    "# Extract additional features\n",
    "phrase_patterns_features = [phrase_patterns(text) for text in texts]\n",
    "punctuation_similarity_features = [punctuation_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "sentence_length_similarity_features = [sentence_length_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "pos_tag_similarity_features = [pos_tag_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "function_words_features = [function_words(text) for text in texts]\n",
    "ngram_transition_graphs = [ngram_transition_graph(text) for text in texts]\n",
    "ngram_transition_graph_similarity_features = [ngram_transition_graph_similarity(ngram_transition_graphs[i], ngram_transition_graphs[i+1]) for i in range(len(texts)-1)]\n",
    "type_token_ratio_features = [type_token_ratio(text) for text in texts]\n",
    "voice_detection_features = [voice_detection(text) for text in texts]\n",
    "\n",
    "# Create a vocabulary of all unique bigrams\n",
    "vocab = set()\n",
    "for bigrams_list in phrase_patterns_features:\n",
    "    for bigram in bigrams_list:\n",
    "        vocab.add(bigram)\n",
    "\n",
    "# Assign a unique index to each bigram\n",
    "bigram_to_index = {bigram: i + 1 for i, bigram in enumerate(vocab)}\n",
    "\n",
    "# Replace each bigram in phrase_patterns_features with its corresponding index\n",
    "indexed_phrase_patterns_features = [[bigram_to_index[bigram] for bigram in bigrams_list] for bigrams_list in phrase_patterns_features]\n",
    "\n",
    "# Compute the maximum length of the indexed phrase patterns\n",
    "max_phrase_patterns_length = max(len(seq) for seq in indexed_phrase_patterns_features)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_phrase_patterns_features = pad_sequences(indexed_phrase_patterns_features, maxlen=max_phrase_patterns_length, padding='post')\n",
    "\n",
    "# Find the maximum length of function words\n",
    "max_function_words_length = max(len(words) for words in function_words_features)\n",
    "\n",
    "# Pad each list of function words to the maximum length\n",
    "padded_function_words_features = [words + [''] * (max_function_words_length - len(words)) for words in function_words_features]\n",
    "\n",
    "# Convert padded function words features into a 2D NumPy array\n",
    "function_words_array = np.array(padded_function_words_features)\n",
    "\n",
    "# Calculate max_features_length\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(function_words_array), len(type_token_ratio_features))\n",
    "\n",
    "# Ensure all features have consistent shapes\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(function_words_array), len(type_token_ratio_features))\n",
    "\n",
    "# Truncate or pad the other features to match max_features_length\n",
    "punctuation_similarity_features = punctuation_similarity_features[:max_features_length]\n",
    "sentence_length_similarity_features = sentence_length_similarity_features[:max_features_length]\n",
    "pos_tag_similarity_features = pos_tag_similarity_features[:max_features_length]\n",
    "ngram_transition_graph_similarity_features = ngram_transition_graph_similarity_features[:max_features_length]\n",
    "type_token_ratio_features = type_token_ratio_features[:max_features_length]\n",
    "function_words_array = pad_sequences(function_words_array, maxlen=max_function_words_length, padding='post')[:max_features_length]\n",
    "\n",
    "# Convert all features to numpy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "padded_phrase_patterns_features = np.array(padded_phrase_patterns_features)\n",
    "punctuation_similarity_features = np.array(punctuation_similarity_features)\n",
    "sentence_length_similarity_features = np.array(sentence_length_similarity_features)\n",
    "pos_tag_similarity_features = np.array(pos_tag_similarity_features)\n",
    "function_words_array = np.array(function_words_array)\n",
    "ngram_transition_graph_similarity_features = np.array(ngram_transition_graph_similarity_features)\n",
    "type_token_ratio_features = np.array(type_token_ratio_features)\n",
    "\n",
    "# Concatenate textual features\n",
    "textual_features = np.concatenate((padded_sequences, \n",
    "                                   padded_phrase_patterns_features, \n",
    "                                   punctuation_similarity_features, \n",
    "                                   sentence_length_similarity_features, \n",
    "                                   pos_tag_similarity_features, \n",
    "                                   function_words_array, \n",
    "                                   ngram_transition_graph_similarity_features, \n",
    "                                   type_token_ratio_features), \n",
    "                                  axis=1)\n",
    "\n",
    "\n",
    "# Similar transformations for other features if needed\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(textual_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Function to predict if text belongs to the author\n",
    "def predict_authorship(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n",
    "    # Extract additional features\n",
    "    phrase_patterns_features = phrase_patterns(text)\n",
    "    punctuation_similarity_features = punctuation_similarity(text, reference_text)\n",
    "    sentence_length_similarity_features = sentence_length_similarity(text, reference_text)\n",
    "    pos_tag_similarity_features = pos_tag_similarity(text, reference_text)\n",
    "    function_words_features = function_words(text)\n",
    "    ngram_transition_graph = ngram_transition_graph(text)\n",
    "    type_token_ratio_features = type_token_ratio(text)\n",
    "    voice_detection_features = voice_detection(text)\n",
    "    # Convert features to compatible shapes\n",
    "    padded_phrase_patterns_features = pad_sequences([phrase_patterns_features], maxlen=max_phrase_patterns_length, padding='post')\n",
    "    # Similar transformations for other features if needed\n",
    "    # Concatenate textual features\n",
    "    textual_features = np.concatenate((padded_seq, padded_phrase_patterns_features, np.array(punctuation_similarity_features)[:, None], np.array(sentence_length_similarity_features)[:, None], np.array(pos_tag_similarity_features)[:, None], np.array(function_words_features), np.array(ngram_transition_graph_similarity_features)[:, None], np.array(type_token_ratio_features)[:, None]), axis=1)\n",
    "    probability = model.predict(textual_features)[0][0]\n",
    "    return probability\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"A new text by Author A.\"\n",
    "probability = predict_authorship(test_text)\n",
    "print(\"Probability of belonging to the author:\", probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'The'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 178\u001b[0m\n\u001b[0;32m    176\u001b[0m ngram_transition_graph_similarity_features \u001b[38;5;241m=\u001b[39m ngram_transition_graph_similarity_features[:max_features_length]\n\u001b[0;32m    177\u001b[0m type_token_ratio_features \u001b[38;5;241m=\u001b[39m type_token_ratio_features[:max_features_length]\n\u001b[1;32m--> 178\u001b[0m function_words_array \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_words_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_function_words_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[:max_features_length]\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Convert all features to numpy arrays\u001b[39;00m\n\u001b[0;32m    181\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(padded_sequences)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\data_utils.py:1132\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m-> 1132\u001b[0m trunc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(trunc, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1138\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'The'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define functions for additional features\n",
    "\n",
    "def phrase_patterns(text):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < 2:\n",
    "        return []  # Return empty list for texts with less than two words\n",
    "    bigram_counts = Counter(bigrams(tokens))\n",
    "    significant_collocations = [bigram for bigram, count in bigram_counts.items() if count > 1]  # Example threshold for significance\n",
    "    return significant_collocations\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "    punctuation_count_text1 = sum(text1.count(char) for char in punctuation_marks)\n",
    "    punctuation_count_text2 = sum(text2.count(char) for char in punctuation_marks)\n",
    "    return min(punctuation_count_text1, punctuation_count_text2) / max(punctuation_count_text1, punctuation_count_text2)\n",
    "\n",
    "def sentence_length_similarity(text1, text2):\n",
    "    sentences_text1 = nltk.sent_tokenize(text1)\n",
    "    sentences_text2 = nltk.sent_tokenize(text2)\n",
    "    avg_length_text1 = sum(len(sent.split()) for sent in sentences_text1) / len(sentences_text1)\n",
    "    avg_length_text2 = sum(len(sent.split()) for sent in sentences_text2) / len(sentences_text2)\n",
    "    return min(avg_length_text1, avg_length_text2) / max(avg_length_text1, avg_length_text2)\n",
    "\n",
    "def pos_tag_similarity(text1, text2):\n",
    "    pos_tags_text1 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text1))]\n",
    "    pos_tags_text2 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text2))]\n",
    "    pos_tag_set1 = set(pos_tags_text1)\n",
    "    pos_tag_set2 = set(pos_tags_text2)\n",
    "    return len(pos_tag_set1.intersection(pos_tag_set2)) / len(pos_tag_set1.union(pos_tag_set2))\n",
    "\n",
    "def function_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    function_words_text = [word for word in tokens if word.lower() in stop_words]\n",
    "    return function_words_text\n",
    "\n",
    "def ngram_transition_graph(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    transition_graph.add_nodes_from(ngrams)\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "    return transition_graph\n",
    "\n",
    "def ngram_transition_graph_similarity(graph1, graph2):\n",
    "    nodes_graph1 = set(graph1.nodes)\n",
    "    nodes_graph2 = set(graph2.nodes)\n",
    "    intersection = nodes_graph1.intersection(nodes_graph2)\n",
    "    union = nodes_graph1.union(nodes_graph2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "def voice_detection(sentence):\n",
    "    # Example implementation using simple keyword matching\n",
    "    if 'is' in sentence.split() or 'are' in sentence.split():\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "# Concatenate positive and negative examples\n",
    "texts = texts_positive + texts_negative\n",
    "labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')  # Padding sequences using pad_sequences\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert function words to numerical indices\n",
    "word_to_index = {}  # Create a dictionary to map words to indices\n",
    "index = 1\n",
    "for words_list in function_words_features:\n",
    "    for word in words_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = index\n",
    "            index += 1\n",
    "\n",
    "# Remove non-numeric elements from function_words_array\n",
    "function_words_array_numeric = [[word for word in words if word.isdigit()] for words in function_words_array]\n",
    "\n",
    "# Pad each list of numeric function words to the maximum length\n",
    "padded_function_words_features = [words + ['0'] * (max_function_words_length - len(words)) for words in function_words_array_numeric]\n",
    "\n",
    "# Convert padded function words features into a 2D NumPy array\n",
    "function_words_array_padded = np.array(padded_function_words_features)\n",
    "\n",
    "# Pad sequences to ensure consistent length\n",
    "function_words_array_padded = pad_sequences(function_words_array_padded, maxlen=max_function_words_length, padding='post')\n",
    "\n",
    "# Truncate or pad the other features to match max_features_length\n",
    "function_words_array_padded = function_words_array_padded[:max_features_length]\n",
    "\n",
    "# Convert each list of function words to numerical indices\n",
    "numerical_function_words = [[word_to_index[word] for word in words_list] for words_list in function_words_features]\n",
    "\n",
    "# Pad the sequences to the maximum length\n",
    "padded_function_words = pad_sequences(numerical_function_words, maxlen=max_function_words_length, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "function_words_array = np.array(padded_function_words)\n",
    "\n",
    "\n",
    "# Extract additional features\n",
    "phrase_patterns_features = [phrase_patterns(text) for text in texts]\n",
    "punctuation_similarity_features = [punctuation_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "sentence_length_similarity_features = [sentence_length_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "pos_tag_similarity_features = [pos_tag_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "function_words_features = [function_words(text) for text in texts]\n",
    "ngram_transition_graphs = [ngram_transition_graph(text) for text in texts]\n",
    "ngram_transition_graph_similarity_features = [ngram_transition_graph_similarity(ngram_transition_graphs[i], ngram_transition_graphs[i+1]) for i in range(len(texts)-1)]\n",
    "type_token_ratio_features = [type_token_ratio(text) for text in texts]\n",
    "voice_detection_features = [voice_detection(text) for text in texts]\n",
    "\n",
    "# Create a vocabulary of all unique bigrams\n",
    "vocab = set()\n",
    "for bigrams_list in phrase_patterns_features:\n",
    "    for bigram in bigrams_list:\n",
    "        vocab.add(bigram)\n",
    "\n",
    "# Assign a unique index to each bigram\n",
    "bigram_to_index = {bigram: i + 1 for i, bigram in enumerate(vocab)}\n",
    "\n",
    "# Replace each bigram in phrase_patterns_features with its corresponding index\n",
    "indexed_phrase_patterns_features = [[bigram_to_index[bigram] for bigram in bigrams_list] for bigrams_list in phrase_patterns_features]\n",
    "\n",
    "# Compute the maximum length of the indexed phrase patterns\n",
    "max_phrase_patterns_length = max(len(seq) for seq in indexed_phrase_patterns_features)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_phrase_patterns_features = pad_sequences(indexed_phrase_patterns_features, maxlen=max_phrase_patterns_length, padding='post')\n",
    "\n",
    "# Find the maximum length of function words\n",
    "max_function_words_length = max(len(words) for words in function_words_features)\n",
    "\n",
    "# Pad each list of function words to the maximum length\n",
    "padded_function_words_features = [words + [''] * (max_function_words_length - len(words)) for words in function_words_features]\n",
    "\n",
    "# Convert padded function words features into a 2D NumPy array\n",
    "function_words_array = np.array(padded_function_words_features)\n",
    "\n",
    "# Calculate max_features_length\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(function_words_array), len(type_token_ratio_features))\n",
    "\n",
    "# Ensure all features have consistent shapes\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(function_words_array), len(type_token_ratio_features))\n",
    "\n",
    "# Truncate or pad the other features to match max_features_length\n",
    "punctuation_similarity_features = punctuation_similarity_features[:max_features_length]\n",
    "sentence_length_similarity_features = sentence_length_similarity_features[:max_features_length]\n",
    "pos_tag_similarity_features = pos_tag_similarity_features[:max_features_length]\n",
    "ngram_transition_graph_similarity_features = ngram_transition_graph_similarity_features[:max_features_length]\n",
    "type_token_ratio_features = type_token_ratio_features[:max_features_length]\n",
    "function_words_array = pad_sequences(function_words_array, maxlen=max_function_words_length, padding='post')[:max_features_length]\n",
    "\n",
    "# Convert all features to numpy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "padded_phrase_patterns_features = np.array(padded_phrase_patterns_features)\n",
    "punctuation_similarity_features = np.array(punctuation_similarity_features)\n",
    "sentence_length_similarity_features = np.array(sentence_length_similarity_features)\n",
    "pos_tag_similarity_features = np.array(pos_tag_similarity_features)\n",
    "function_words_array = np.array(function_words_array)\n",
    "ngram_transition_graph_similarity_features = np.array(ngram_transition_graph_similarity_features)\n",
    "type_token_ratio_features = np.array(type_token_ratio_features)\n",
    "\n",
    "# Concatenate textual features\n",
    "textual_features = np.concatenate((padded_sequences, \n",
    "                                   padded_phrase_patterns_features, \n",
    "                                   punctuation_similarity_features, \n",
    "                                   sentence_length_similarity_features, \n",
    "                                   pos_tag_similarity_features, \n",
    "                                   function_words_array, \n",
    "                                   ngram_transition_graph_similarity_features, \n",
    "                                   type_token_ratio_features), \n",
    "                                  axis=1)\n",
    "\n",
    "\n",
    "# Similar transformations for other features if needed\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(textual_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Function to predict if text belongs to the author\n",
    "def predict_authorship(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n",
    "    # Extract additional features\n",
    "    phrase_patterns_features = phrase_patterns(text)\n",
    "    punctuation_similarity_features = punctuation_similarity(text, reference_text)\n",
    "    sentence_length_similarity_features = sentence_length_similarity(text, reference_text)\n",
    "    pos_tag_similarity_features = pos_tag_similarity(text, reference_text)\n",
    "    function_words_features = function_words(text)\n",
    "    ngram_transition_graph = ngram_transition_graph(text)\n",
    "    type_token_ratio_features = type_token_ratio(text)\n",
    "    voice_detection_features = voice_detection(text)\n",
    "    # Convert features to compatible shapes\n",
    "    padded_phrase_patterns_features = pad_sequences([phrase_patterns_features], maxlen=max_phrase_patterns_length, padding='post')\n",
    "    # Similar transformations for other features if needed\n",
    "    # Concatenate textual features\n",
    "    textual_features = np.concatenate((padded_seq, padded_phrase_patterns_features, np.array(punctuation_similarity_features)[:, None], np.array(sentence_length_similarity_features)[:, None], np.array(pos_tag_similarity_features)[:, None], np.array(function_words_features), np.array(ngram_transition_graph_similarity_features)[:, None], np.array(type_token_ratio_features)[:, None]), axis=1)\n",
    "    probability = model.predict(textual_features)[0][0]\n",
    "    return probability\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"A new text by Author A.\"\n",
    "probability = predict_authorship(test_text)\n",
    "print(\"Probability of belonging to the author:\", probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m type_token_ratio_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(type_token_ratio_features)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Concatenate textual features\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m textual_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mpadded_phrase_patterns_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mpunctuation_similarity_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43msentence_length_similarity_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mpos_tag_similarity_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mngram_transition_graph_similarity_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtype_token_ratio_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Similar transformations for other features if needed\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[0;32m    149\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(textual_features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define functions for additional features\n",
    "\n",
    "def phrase_patterns(text):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < 2:\n",
    "        return []  # Return empty list for texts with less than two words\n",
    "    bigram_counts = Counter(bigrams(tokens))\n",
    "    significant_collocations = [bigram for bigram, count in bigram_counts.items() if count > 1]  # Example threshold for significance\n",
    "    return significant_collocations\n",
    "\n",
    "def punctuation_similarity(text1, text2):\n",
    "    punctuation_marks = set(string.punctuation)\n",
    "    punctuation_count_text1 = sum(text1.count(char) for char in punctuation_marks)\n",
    "    punctuation_count_text2 = sum(text2.count(char) for char in punctuation_marks)\n",
    "    return min(punctuation_count_text1, punctuation_count_text2) / max(punctuation_count_text1, punctuation_count_text2)\n",
    "\n",
    "def sentence_length_similarity(text1, text2):\n",
    "    sentences_text1 = nltk.sent_tokenize(text1)\n",
    "    sentences_text2 = nltk.sent_tokenize(text2)\n",
    "    avg_length_text1 = sum(len(sent.split()) for sent in sentences_text1) / len(sentences_text1)\n",
    "    avg_length_text2 = sum(len(sent.split()) for sent in sentences_text2) / len(sentences_text2)\n",
    "    return min(avg_length_text1, avg_length_text2) / max(avg_length_text1, avg_length_text2)\n",
    "\n",
    "def pos_tag_similarity(text1, text2):\n",
    "    pos_tags_text1 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text1))]\n",
    "    pos_tags_text2 = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(text2))]\n",
    "    pos_tag_set1 = set(pos_tags_text1)\n",
    "    pos_tag_set2 = set(pos_tags_text2)\n",
    "    return len(pos_tag_set1.intersection(pos_tag_set2)) / len(pos_tag_set1.union(pos_tag_set2))\n",
    "\n",
    "def ngram_transition_graph(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    transition_graph = nx.DiGraph()\n",
    "    transition_graph.add_nodes_from(ngrams)\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        transition_graph.add_edge(ngrams[i], ngrams[i + 1])\n",
    "    return transition_graph\n",
    "\n",
    "def ngram_transition_graph_similarity(graph1, graph2):\n",
    "    nodes_graph1 = set(graph1.nodes)\n",
    "    nodes_graph2 = set(graph2.nodes)\n",
    "    intersection = nodes_graph1.intersection(nodes_graph2)\n",
    "    union = nodes_graph1.union(nodes_graph2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def type_token_ratio(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens)\n",
    "\n",
    "def voice_detection(sentence):\n",
    "    # Example implementation using simple keyword matching\n",
    "    if 'is' in sentence.split() or 'are' in sentence.split():\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return 'active'\n",
    "\n",
    "# Concatenate positive and negative examples\n",
    "texts = texts_positive + texts_negative\n",
    "labels = [1] * len(texts_positive) + [0] * len(texts_negative)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')  # Padding sequences using pad_sequences\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Extract additional features\n",
    "phrase_patterns_features = [phrase_patterns(text) for text in texts]\n",
    "punctuation_similarity_features = [punctuation_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "sentence_length_similarity_features = [sentence_length_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "pos_tag_similarity_features = [pos_tag_similarity(texts[i], texts[i+1]) for i in range(len(texts)-1)]\n",
    "ngram_transition_graphs = [ngram_transition_graph(text) for text in texts]\n",
    "ngram_transition_graph_similarity_features = [ngram_transition_graph_similarity(ngram_transition_graphs[i], ngram_transition_graphs[i+1]) for i in range(len(texts)-1)]\n",
    "type_token_ratio_features = [type_token_ratio(text) for text in texts]\n",
    "voice_detection_features = [voice_detection(text) for text in texts]\n",
    "\n",
    "# Create a vocabulary of all unique bigrams\n",
    "vocab = set()\n",
    "for bigrams_list in phrase_patterns_features:\n",
    "    for bigram in bigrams_list:\n",
    "        vocab.add(bigram)\n",
    "\n",
    "# Assign a unique index to each bigram\n",
    "bigram_to_index = {bigram: i + 1 for i, bigram in enumerate(vocab)}\n",
    "\n",
    "# Replace each bigram in phrase_patterns_features with its corresponding index\n",
    "indexed_phrase_patterns_features = [[bigram_to_index[bigram] for bigram in bigrams_list] for bigrams_list in phrase_patterns_features]\n",
    "\n",
    "# Compute the maximum length of the indexed phrase patterns\n",
    "max_phrase_patterns_length = max(len(seq) for seq in indexed_phrase_patterns_features)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_phrase_patterns_features = pad_sequences(indexed_phrase_patterns_features, maxlen=max_phrase_patterns_length, padding='post')\n",
    "\n",
    "# Calculate max_features_length\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(type_token_ratio_features))\n",
    "\n",
    "# Ensure all features have consistent shapes\n",
    "max_features_length = max(len(padded_sequences), len(padded_phrase_patterns_features), len(type_token_ratio_features))\n",
    "\n",
    "# Truncate or pad the other features to match max_features_length\n",
    "punctuation_similarity_features = punctuation_similarity_features[:max_features_length]\n",
    "sentence_length_similarity_features = sentence_length_similarity_features[:max_features_length]\n",
    "pos_tag_similarity_features = pos_tag_similarity_features[:max_features_length]\n",
    "ngram_transition_graph_similarity_features = ngram_transition_graph_similarity_features[:max_features_length]\n",
    "type_token_ratio_features = type_token_ratio_features[:max_features_length]\n",
    "\n",
    "# Convert all features to numpy arrays\n",
    "padded_sequences = np.array(padded_sequences)\n",
    "padded_phrase_patterns_features = np.array(padded_phrase_patterns_features)\n",
    "punctuation_similarity_features = np.array(punctuation_similarity_features)\n",
    "sentence_length_similarity_features = np.array(sentence_length_similarity_features)\n",
    "pos_tag_similarity_features = np.array(pos_tag_similarity_features)\n",
    "ngram_transition_graph_similarity_features = np.array(ngram_transition_graph_similarity_features)\n",
    "type_token_ratio_features = np.array(type_token_ratio_features)\n",
    "\n",
    "# Concatenate textual features\n",
    "textual_features = np.concatenate((padded_sequences, \n",
    "                                   padded_phrase_patterns_features, \n",
    "                                   punctuation_similarity_features, \n",
    "                                   sentence_length_similarity_features, \n",
    "                                   pos_tag_similarity_features, \n",
    "                                   ngram_transition_graph_similarity_features, \n",
    "                                   type_token_ratio_features), \n",
    "                                  axis=1)\n",
    "\n",
    "# Similar transformations for other features if needed\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(textual_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Function to predict if text belongs to the author\n",
    "def predict_authorship(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n",
    "    # Extract additional features\n",
    "    phrase_patterns_features = phrase_patterns(text)\n",
    "    punctuation_similarity_features = punctuation_similarity(text, reference_text)\n",
    "    sentence_length_similarity_features = sentence_length_similarity(text, reference_text)\n",
    "    pos_tag_similarity_features = pos_tag_similarity(text, reference_text)\n",
    "    ngram_transition_graph = ngram_transition_graph(text)\n",
    "    type_token_ratio_features = type_token_ratio(text)\n",
    "    # Convert features to compatible shapes\n",
    "    padded_phrase_patterns_features = pad_sequences([phrase_patterns_features], maxlen=max_phrase_patterns_length, padding='post')\n",
    "    # Similar transformations for other features if needed\n",
    "    # Concatenate textual features\n",
    "    textual_features = np.concatenate((padded_seq, padded_phrase_patterns_features, np.array(punctuation_similarity_features)[:, None], np.array(sentence_length_similarity_features)[:, None], np.array(pos_tag_similarity_features)[:, None], np.array(ngram_transition_graph_similarity_features)[:, None], np.array(type_token_ratio_features)[:, None]), axis=1)\n",
    "    probability = model.predict(textual_features)[0][0]\n",
    "    return probability\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"A new text by Author A.\"\n",
    "probability = predict_authorship(test_text)\n",
    "print(\"Probability of belonging to the author:\", probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded_sequences: (29, 1013)\n",
      "Shape of padded_phrase_patterns_features: (29, 44)\n",
      "Shape of punctuation_similarity_features: (28, 1)\n",
      "Shape of sentence_length_similarity_features: (28, 1)\n",
      "Shape of pos_tag_similarity_features: (28, 1)\n",
      "Shape of function_words_array: (29, 507)\n",
      "Shape of ngram_transition_graph_similarity_features: (28, 1)\n",
      "Shape of type_token_ratio_features: (29, 1)\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum length of function words\n",
    "max_function_words_length = max(len(words) for words in function_words_features)\n",
    "\n",
    "# Pad each list of function words to the maximum length\n",
    "padded_function_words_features = [words + [''] * (max_function_words_length - len(words)) for words in function_words_features]\n",
    "\n",
    "# Convert padded function words features into a 2D NumPy array\n",
    "function_words_array = np.array(padded_function_words_features)\n",
    "\n",
    "print(\"Shape of padded_sequences:\", padded_sequences.shape)\n",
    "print(\"Shape of padded_phrase_patterns_features:\", padded_phrase_patterns_features.shape)\n",
    "print(\"Shape of punctuation_similarity_features:\", np.array(punctuation_similarity_features)[:, None].shape)\n",
    "print(\"Shape of sentence_length_similarity_features:\", np.array(sentence_length_similarity_features)[:, None].shape)\n",
    "print(\"Shape of pos_tag_similarity_features:\", np.array(pos_tag_similarity_features)[:, None].shape)\n",
    "print(\"Shape of function_words_array:\", function_words_array.shape)\n",
    "print(\"Shape of ngram_transition_graph_similarity_features:\", np.array(ngram_transition_graph_similarity_features)[:, None].shape)\n",
    "print(\"Shape of type_token_ratio_features:\", np.array(type_token_ratio_features)[:, None].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
